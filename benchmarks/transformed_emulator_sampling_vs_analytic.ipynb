{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# TransformedEmulator: Sampling vs Analytical (Delta)\n",
    "\n",
    "This notebook benchmarks speed and accuracy of two prediction modes in `TransformedEmulator`:\n",
    "- `output_from_samples=False` (analytical/approx via inverse + delta method, diagonal variance)\n",
    "- `output_from_samples=True` (sampling-based inversion, diagonal variance)\n",
    "\n",
    "We compare timings for `.fit()`, `.predict_mean()`, and `.predict_mean_and_variance()` and report accuracy (RMSE and mean NLL) across different target dimensionalities.\n",
    "\n",
    "Settings:\n",
    "- Model: GaussianProcess\n",
    "- full_covariance=False (always)\n",
    "- x transforms: Standardize\n",
    "- y transforms benchmarked: Standardize, Standardize+PCA, Standardize+VAE\n",
    "- n_samples (sampling path): 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from autoemulate.data.utils import set_random_seed\n",
    "from autoemulate.emulators import GaussianProcess\n",
    "from autoemulate.emulators.transformed.base import TransformedEmulator\n",
    "from autoemulate.transforms import StandardizeTransform, PCATransform, VAETransform\n",
    "\n",
    "device = None  # use default CPU\n",
    "set_random_seed(123)\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "def make_synthetic(n_train=600, n_test=200, x_dim=10, y_dim=32, noise=0.05):\n",
    "    \"\"\"\n",
    "    Create a simple nonlinear multioutput regression dataset.\n",
    "    y = A * sin(B @ x) + C * x + noise\n",
    "    \"\"\"\n",
    "    set_random_seed(123 + y_dim)\n",
    "    X_train = torch.randn(n_train, x_dim)\n",
    "    X_test = torch.randn(n_test, x_dim)\n",
    "    B = torch.randn(x_dim, y_dim) / math.sqrt(x_dim)\n",
    "    A = torch.randn(y_dim)\n",
    "    C = torch.randn(x_dim, y_dim) / math.sqrt(x_dim)\n",
    "    def f(X):\n",
    "        lin = X @ C\n",
    "        s = torch.sin(X @ B) * A\n",
    "        return lin + s\n",
    "    Y_train = f(X_train) + noise * torch.randn(n_train, y_dim)\n",
    "    Y_test = f(X_test) + noise * torch.randn(n_test, y_dim)\n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "\n",
    "@dataclass\n",
    "class BenchResult:\n",
    "    y_dim: int\n",
    "    y_transform: str\n",
    "    output_from_samples: bool\n",
    "    fit_time_s: float\n",
    "    pred_mean_time_s: float\n",
    "    pred_mv_time_s: float\n",
    "    rmse: float\n",
    "    nll: float\n",
    "\n",
    "def rmse(y_true: torch.Tensor, y_pred: torch.Tensor) -> float:\n",
    "    return float(torch.sqrt(torch.mean((y_true - y_pred) ** 2)))\n",
    "\n",
    "def mean_diag_nll(y_true: torch.Tensor, mean: torch.Tensor, var: torch.Tensor) -> float:\n",
    "    min_var = 1e-6\n",
    "    var = torch.clamp(var, min=min_var)\n",
    "    nll = 0.5 * (\n",
    "        torch.log(2 * torch.pi * var) + (y_true - mean) ** 2 / var\n",
    "    )\n",
    "    return float(nll.mean())\n",
    "\n",
    "def y_transform_configs(y_dim: int):\n",
    "    return [\n",
    "        (\"standardize\", [StandardizeTransform()]),\n",
    "        (\"standardize+pca\", [StandardizeTransform(), PCATransform(n_components=min( max(1, y_dim//4), max(1, y_dim-1) ))]),\n",
    "        (\"standardize+vae\", [StandardizeTransform(), VAETransform(latent_dim=max(1, y_dim//4))]),\n",
    "    ]\n",
    "\n",
    "def benchmark_one(y_dim: int, y_t_name: str, y_t_list, output_from_samples: bool, n_samples: int = 256) -> BenchResult:\n",
    "    Xtr, Ytr, Xte, Yte = make_synthetic(y_dim=y_dim)\n",
    "    em = TransformedEmulator(\n",
    "        Xtr,\n",
    "        Ytr,\n",
    "        x_transforms=[StandardizeTransform()],\n",
    "        y_transforms=y_t_list,\n",
    "        model=GaussianProcess,\n",
    "        output_from_samples=output_from_samples,\n",
    "        n_samples=n_samples,\n",
    "        full_covariance=False,\n",
    "        device=device,\n",
    "    )\n",
    "    t0 = time.perf_counter()\n",
    "    em.fit(Xtr, Ytr)\n",
    "    fit_t = time.perf_counter() - t0\n",
    "\n",
    "    # Warm-up (exclude first-call overhead from timings)\n",
    "    Xw = Xte[: min(64, Xte.shape[0])]\n",
    "    try:\n",
    "        _ = em.predict_mean(Xw)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        _ = em.predict_mean_and_variance(Xw)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    mu = em.predict_mean(Xte)\n",
    "    pm_t = time.perf_counter() - t0\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    mu2, var = em.predict_mean_and_variance(Xte)\n",
    "    pmv_t = time.perf_counter() - t0\n",
    "\n",
    "    # Sanity: mu and mu2 should be close\n",
    "    _ = float(torch.mean((mu - mu2).abs()))\n",
    "\n",
    "    r = BenchResult(\n",
    "        y_dim=y_dim,\n",
    "        y_transform=y_t_name,\n",
    "        output_from_samples=output_from_samples,\n",
    "        fit_time_s=fit_t,\n",
    "        pred_mean_time_s=pm_t,\n",
    "        pred_mv_time_s=pmv_t,\n",
    "        rmse=rmse(Yte, mu),\n",
    "        nll=mean_diag_nll(Yte, mu2, var),\n",
    "    )\n",
    "    return r\n",
    "\n",
    "def run_benchmarks(y_dims=(4, 32, 128), n_samples=256) -> pd.DataFrame:\n",
    "    results = []\n",
    "    for yd in y_dims:\n",
    "        for y_t_name, y_t_list in y_transform_configs(yd):\n",
    "            for ofs in (False, True):\n",
    "                r = benchmark_one(yd, y_t_name, y_t_list, ofs, n_samples=n_samples)\n",
    "                results.append(r.__dict__)\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = run_benchmarks(y_dims=(4, 32, 128), n_samples=256)\n",
    "# df = run_benchmarks(y_dims=(4, 32), n_samples=256)\n",
    "df.sort_values(['y_dim', 'y_transform', 'output_from_samples'], inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot timings\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 12), sharey=False)\n",
    "for i, metric in enumerate(['fit_time_s', 'pred_mean_time_s', 'pred_mv_time_s']):\n",
    "    for j, y_t in enumerate(sorted(df['y_transform'].unique())):\n",
    "        ax = axes[i, j]\n",
    "        sub = df[df['y_transform'] == y_t]\n",
    "        sns.barplot(data=sub, x='y_dim', y=metric, hue='output_from_samples', ax=ax)\n",
    "        ax.set_title(f\"{metric} — {y_t}\")\n",
    "        ax.set_xlabel('y_dim')\n",
    "        if i == 0:\n",
    "            ax.legend(title='from_samples')\n",
    "        else:\n",
    "            ax.get_legend().remove()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy metrics\n",
    "fig, axes = plt.subplots(2, len(df['y_transform'].unique()), figsize=(16, 6), sharey=False)\n",
    "for j, y_t in enumerate(sorted(df['y_transform'].unique())):\n",
    "    sub = df[df['y_transform'] == y_t]\n",
    "    sns.barplot(data=sub, x='y_dim', y='rmse', hue='output_from_samples', ax=axes[0, j])\n",
    "    axes[0, j].set_title(f'RMSE — {y_t}')\n",
    "    axes[0, j].set_xlabel('y_dim')\n",
    "    sns.barplot(data=sub, x='y_dim', y='nll', hue='output_from_samples', ax=axes[1, j])\n",
    "    axes[1, j].set_title(f'Mean NLL — {y_t}')\n",
    "    axes[1, j].set_xlabel('y_dim')\n",
    "for i in range(2):\n",
    "    for j in range(len(df['y_transform'].unique())):\n",
    "        if i == 0:\n",
    "            axes[i, j].legend(title='from_samples')\n",
    "        else:\n",
    "            axes[i, j].get_legend().remove()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Notes\n",
    "- Both modes share the same underlying fitted model; differences are in inversion to original space and variance handling.\n",
    "- Sampling variance estimates can be noisy for small `n_samples`; increase it to trade speed for stability.\n",
    "- Analytical (delta) path assumes approximate linearization; may bias variance under strong nonlinearity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
