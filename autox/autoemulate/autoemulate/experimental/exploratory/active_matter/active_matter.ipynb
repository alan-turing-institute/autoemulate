{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Exploring [\"The Well\" Active matter simulations](https://polymathic-ai.org/the_well/datasets/active_matter/#active-matter-simulations) dataset\n",
    "\n",
    "This notebook explores the dataset and roughly follows the [dataset tutorial](https://polymathic-ai.org/the_well/tutorials/dataset/) found on The Well's documentation, but looks specifically at the active matter simulations dataset, with clarifying notes. This is a prelude to using the dataset as an example for The Alan Turing Institute's [AutoEmulate](https://github.com/alan-turing-institute/autoemulate) project.\n",
    "\n",
    "### Dataset notes (see [paper](https://arxiv.org/abs/2308.06675))\n",
    "\n",
    "Active matter systems consist of collections of agents, such as particles or macromolecules, that convert chemical energy into mechanical work; A few prominent experimental realizations include suspensions of swimming bacteria and in vitro mixtures of cytoskeletal filaments and motor proteins.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from einops import rearrange\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# from neuralop.models import FNO\n",
    "\n",
    "from the_well.benchmark.metrics import VRMSE\n",
    "from the_well.data import WellDataset\n",
    "from the_well.utils.download import well_download\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "Notebook TODOs:\n",
    "- [ ] Update description of fields below\n",
    "- [ ] Train FNO model from example\n",
    "- [ ] Download the validation set\n",
    "- [ ] Carry out evaluation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Download the dataset or stream from Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the training dataset\n",
    "base_path = \"./datasets/datasets\"  # path/to/storage\n",
    "# This took over 80 min to download\n",
    "if os.path.exists(os.path.join(base_path, \"active_matter\")):\n",
    "    print(f\"Dataset already downloaded.\")\n",
    "else:\n",
    "    well_download(base_path=base_path, dataset=\"active_matter\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the base path to huggingface hub if you want to stream instead of downloading\n",
    "# base_path = \"hf://datasets/polymathic-ai/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "trainset = WellDataset(\n",
    "    well_base_path=base_path,  # access from HF hub\n",
    "    well_dataset_name=\"active_matter\",\n",
    "    well_split_name=\"train\",\n",
    "    n_steps_input=81,  # 81 time steps in the active matter dataset\n",
    "    n_steps_output=0,\n",
    "    use_normalization=True,  # standardize the fields with respect to their mean and standard deviation over a subset of the training set\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "On the web page for the dataset, the parameters are described as:\n",
    "\n",
    "Set of coefficients or non-dimensional parameters evaluated:\n",
    "\n",
    "```\n",
    "    α= {-1,-2,-3,-4,-5}\n",
    "\n",
    "    β= {0.8}\n",
    "\n",
    "    ζ= {1,3,5,7,9,11,13,15,17}\n",
    "```\n",
    "\n",
    "This means the total number of parameter combinations is 5 x 1 x 9 = 45.\n",
    "\n",
    "There are 5 trajectories (simulations) for each parameter combination, so the total number of trajectories is 5 x 45 = 225."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The length of of the training dataset should be < 225\n",
    "len(trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "Fields available in the data: concentration (scalar field), velocity (vector field), orientation tensor (tensor field), strain-rate tensor (tensor field).\n",
    "\n",
    "TODO: what does this mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D_xx is 2nd order derivative of the x component of the velocity field\n",
    "# E_xx\n",
    "trainset.metadata.field_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the available fields\n",
    "field_names = [\n",
    "    name for group in trainset.metadata.field_names.values() for name in group\n",
    "]\n",
    "field_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we have set the n_steps_input to 81, the shape of the input fields should be (81, 256, 256, 11)\n",
    "# So we have 81 time steps and for each of 11 fields a 256 x 256 grid which could be viewed as a 2D image\n",
    "trainset[174]['input_fields'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "The most important elements are `input_fields` and `output_fields`.\n",
    "\n",
    "They represent the time-varying physical fields of the dynamical system and are generally the input and target of our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As well as the input_fields, we have:    (note: will only include output_fields if n_steps_output > 0)\n",
    "trainset[174].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Visualise the data\n",
    "\n",
    "Let's create an animation of the concentration field for a single simulation across all timesteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to animate simulation images\n",
    "\n",
    "def animate_simulation(simulation, start_frame = 0, interval=200):\n",
    "    \"\"\"\n",
    "    Function to animate a 3D simulation using matplotlib.\n",
    "    Args:\n",
    "        simulation (numpy.ndarray): A 3D numpy array of shape (num_frames, height, width).\n",
    "                                    The first dimension represents the time steps.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the minimum and maximum values for color scaling\n",
    "    vmin = np.nanmin(simulation)\n",
    "    vmax = np.nanmax(simulation)\n",
    "\n",
    "    # Create a figure and axis for the animation\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "    # Add the first frame\n",
    "    num_frames = simulation.shape[0]\n",
    "    img = ax.imshow(\n",
    "        simulation[start_frame],\n",
    "        cmap='RdBu_r',\n",
    "        origin=\"upper\",\n",
    "        interpolation=\"none\", # no smoothing or blending between pixels\n",
    "        vmin=vmin,\n",
    "        vmax=vmax\n",
    "    )\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    # Function to update the frame\n",
    "    def update(frame):\n",
    "        img.set_data(simulation[frame])\n",
    "        ax.set_title(f\"Timestep: {frame + 1} of {num_frames}\")\n",
    "        return img,\n",
    "\n",
    "    # Create the animation\n",
    "    ani = animation.FuncAnimation(\n",
    "        fig,\n",
    "        update,\n",
    "        frames=range(start_frame, num_frames),\n",
    "        interval=interval\n",
    "    )\n",
    "\n",
    "    # Convert animation to HTML\n",
    "    html_anim = HTML(ani.to_jshtml())\n",
    "    plt.close(fig)  # Prevents static image display\n",
    "    return html_anim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a random index to get a simulation/trajectory to visualise\n",
    "simulation_4d = trainset[40]\n",
    "# Create a 3D array for the simulation run to pass to the visualisation\n",
    "conc_index = field_names.index(\"concentration\")\n",
    "simulation = simulation_4d[\"input_fields\"][:, :, :, conc_index].numpy()\n",
    "# Animate the simulation\n",
    "animate_simulation(simulation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "Now let's take a look at all the fields in the dataset, not just concentration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the simulation we chose in the previous cell and rearrange it to be (Field, Timestep, Lx, Ly)\n",
    "x = rearrange(simulation_4d[\"input_fields\"], \"Timestep Lx Ly Field -> Field Timestep Lx Ly\")\n",
    "\n",
    "# Get a list of 4 evenly spaced timesteps to visualise\n",
    "n_timesteps = len(simulation_4d[\"input_fields\"])\n",
    "timesteps = np.linspace(0, n_timesteps - 1, 4).astype(int)\n",
    "\n",
    "# Create a figure to show every field for 4 timesteps of the simulation\n",
    "F = trainset.metadata.n_fields\n",
    "fig, axs = plt.subplots(F, 4, figsize=(4 * 2.4, F * 1.2))\n",
    "\n",
    "for field in range(F):\n",
    "    vmin = np.nanmin(x[field])\n",
    "    vmax = np.nanmax(x[field])\n",
    "\n",
    "    axs[field, 0].set_ylabel(f\"{field_names[field]}\")\n",
    "\n",
    "    for t in range(4):\n",
    "        ts = timesteps[t]\n",
    "        axs[field, t].imshow(\n",
    "            x[field, ts], cmap=\"RdBu_r\", interpolation=\"none\", vmin=vmin, vmax=vmax\n",
    "        )\n",
    "        axs[field, t].set_xticks([])\n",
    "        axs[field, t].set_yticks([])\n",
    "\n",
    "        axs[0, t].set_title(f\"x_{ts}\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Visualise other fields\n",
    "\n",
    "Clearly some of the fields other that concentration would be good to visualise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a random index to get a simulation/trajectory to visualise\n",
    "simulation_4d = trainset[60]\n",
    "# Choose a different field to visualise\n",
    "field = \"D_xx\"\n",
    "# Create a 3D array for the simulation run to pass to the visualisation\n",
    "field_index = field_names.index(field)\n",
    "simulation = simulation_4d[\"input_fields\"][:, :, :, field_index].numpy()\n",
    "# Animate the simulation\n",
    "animate_simulation(simulation, interval=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thewellenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
