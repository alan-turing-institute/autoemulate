{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702bb87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from autoemulate.experimental.emulators import GaussianProcessExact\n",
    "from autoemulate.experimental.emulators.random_forest import RandomForest\n",
    "from autoemulate.experimental.emulators.transformed.base import TransformedEmulator\n",
    "from autoemulate.experimental.transforms import PCATransform, VAETransform, StandardizeTransform\n",
    "from sklearn.datasets import make_regression\n",
    "import torchmetrics\n",
    "from autoemulate.experimental.model_selection import evaluate\n",
    "from autoemulate.experimental.types import TensorLike\n",
    "\n",
    "# Uncomment to enable logging for GPs\n",
    "import logging\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def make_data(\n",
    "    random_state: int = 42,\n",
    "    n_samples: int = 200,\n",
    "    n_informative:int = 2,\n",
    "    n_features: int = 5,\n",
    "    noise: float = 0.2,\n",
    "    n_targets: int = 500\n",
    "):\n",
    "    x, y, _ = make_regression(\n",
    "        n_samples=n_samples,\n",
    "        n_features=n_features,\n",
    "        noise=noise,\n",
    "        random_state=random_state,\n",
    "        n_informative=n_informative,\n",
    "        n_targets=n_targets,\n",
    "        coef=True,\n",
    "    )\n",
    "    x = torch.tensor(x, dtype=torch.float32)\n",
    "    y = torch.tensor(y, dtype=torch.float32)\n",
    "    return x, y\n",
    "\n",
    "# Train data\n",
    "x, y = make_data(random_state=42, n_targets=10)\n",
    "\n",
    "# Test data\n",
    "x2, y2 = make_data(random_state=43, n_targets=10)\n",
    "\n",
    "# Make transform\n",
    "pca = PCATransform(n_components=2)\n",
    "pca.fit(x)\n",
    "\n",
    "# Transform data\n",
    "print(pca(x).shape)\n",
    "\n",
    "# Invert PCA on tensor\n",
    "print(pca.inv(pca(x)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b547ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example y transform\n",
    "pca_y = PCATransform(n_components=1)\n",
    "pca_y.fit(y)\n",
    "print(pca_y(y)[:10].shape)\n",
    "print(pca_y.inv(pca_y(y))[:10].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660ad383",
   "metadata": {},
   "outputs": [],
   "source": [
    "emulator = TransformedEmulator(\n",
    "    x=x,\n",
    "    y=y,\n",
    "    x_transforms=[PCATransform(n_components=4), VAETransform(latent_dim=2)],\n",
    "    y_transforms=[StandardizeTransform(), PCATransform(n_components=1)],\n",
    "    model=GaussianProcessExact,\n",
    "    epochs=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbf0765",
   "metadata": {},
   "outputs": [],
   "source": [
    "emulator.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ba4909",
   "metadata": {},
   "outputs": [],
   "source": [
    "emulator.full_covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b82540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit\n",
    "emulator.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245b28bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on training data\n",
    "pred = emulator.predict(x[:30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdc3408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example using transformed emulator with GP and Random Forest\n",
    "from autoemulate.experimental.model_selection import r2_metric\n",
    "\n",
    "\n",
    "for model in [GaussianProcessExact, RandomForest]:\n",
    "    # Create transformed emulator with GP\n",
    "    emulator = TransformedEmulator(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        x_transforms=[PCATransform(n_components=4), VAETransform(latent_dim=2)],\n",
    "        y_transforms=[StandardizeTransform(), PCATransform(n_components=1)],\n",
    "        model=model,\n",
    "        epochs=100,\n",
    "    )\n",
    "\n",
    "    # Fit\n",
    "    emulator.fit(x, y)\n",
    "\n",
    "    # Predict on training data\n",
    "    pred = emulator.predict(x[:30])\n",
    "    pred = pred if isinstance(pred, TensorLike) else pred.mean\n",
    "    print(f\"Train R2: {evaluate(pred, y[:30], r2_metric()):.3f}\")\n",
    "\n",
    "    # Predict on test data\n",
    "    pred = emulator.predict(x2)\n",
    "    pred = pred if isinstance(pred, TensorLike) else pred.mean\n",
    "    print(f\"Test  R2: {evaluate(pred, y2, r2_metric()):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0a0e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.distributions import MultitaskMultivariateNormal, MultivariateNormal\n",
    "\n",
    "# Mean zero as just test covariance\n",
    "mean = torch.zeros(1, 10)\n",
    "cov = torch.eye(10)\n",
    "mvn = MultitaskMultivariateNormal(mean=mean, covariance_matrix=cov)\n",
    "\n",
    "# Generate some samples\n",
    "torch.manual_seed(42)\n",
    "scales = torch.randint(100, size=(1, 10), dtype=torch.float32)\n",
    "samples= MultivariateNormal(mean=mean, covariance_matrix=cov * scales).sample(torch.Size([100])).squeeze()\n",
    "\n",
    "# Fit standardize transform\n",
    "t = StandardizeTransform()\n",
    "t.fit(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b082e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare inverse from sampling and analytical solution\n",
    "sampled = t._inverse_sample(mvn, n_samples=10000).covariance_matrix\n",
    "analytical = t._inverse_gaussian(mvn).covariance_matrix\n",
    "diff = sampled - analytical\n",
    "diff_normed = diff.abs() / sampled\n",
    "torch.allclose(diff_normed.diagonal(), torch.zeros_like(diff_normed).diagonal(), atol=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e571bdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare analytical and sampling solutions for inverse transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from autoemulate.experimental.types import GaussianLike\n",
    "\n",
    "for t in [VAETransform(latent_dim=2), PCATransform(n_components=2)]:\n",
    "    x, y = make_data(n_targets=5, n_samples=200)\n",
    "    t.fit(y)\n",
    "    z = t(y)\n",
    "    gp = GaussianProcessExact(x, z, standardize_x=True, standardize_y=True)\n",
    "    gp.fit(x, z)\n",
    "    z_pred = gp.predict(x[: x.shape[0] // 2])\n",
    "    assert isinstance(z_pred, GaussianLike)\n",
    "\n",
    "    print(str(t))\n",
    "    for n_samples in [10, 100, 1000, 10000]:\n",
    "        inv_sample = t._inverse_sample(z_pred, n_samples=n_samples)\n",
    "        inv_gaussian = t._inverse_gaussian(z_pred)\n",
    "        diff = inv_sample.covariance_matrix - inv_gaussian.covariance_matrix\n",
    "        plt.title(f\"n_samples={n_samples}\")\n",
    "        plt.hist(inv_sample.covariance_matrix.flatten().detach().numpy(), bins=np.arange(-1, 1, 0.01), alpha=0.5, label='Sampled')\n",
    "        plt.hist(inv_gaussian.covariance_matrix.flatten().detach().numpy(), bins=np.arange(-1, 1, 0.01), alpha=0.5, label=\"Analytical\")\n",
    "        plt.hist(diff.flatten().detach().numpy(), bins=np.arange(-1, 1, 0.01), alpha=0.5, label='Difference')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199b1610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the covariance matrices and difference (flatten non-batch dimension)\n",
    "cov_inv_gaussian = inv_gaussian.covariance_matrix.flatten(1)\n",
    "cov_inv_sample = inv_sample.covariance_matrix.flatten(1)\n",
    "diff = (cov_inv_gaussian - cov_inv_sample)\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "for idx, x_ in enumerate([cov_inv_gaussian, cov_inv_sample, diff]):\n",
    "    x = x_.detach().numpy()\n",
    "    if idx > 1:\n",
    "        hmap = axs[idx].pcolormesh(x, vmin=-np.abs(x).max(), vmax=np.abs(x).max(), cmap=\"RdBu\")\n",
    "    else:\n",
    "        hmap = axs[idx].pcolormesh(x)\n",
    "    plt.colorbar(hmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9db041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare analytical and sampling solutions for transformed emulators without standardization\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from autoemulate.experimental.types import GaussianLike\n",
    "\n",
    "for t in [VAETransform(latent_dim=2), PCATransform(n_components=2)]:\n",
    "    x, y = make_data(n_targets=3)\n",
    "    for n_samples in [10, 100, 1000, 10000]:\n",
    "        gp = TransformedEmulator(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            y_transforms=[t],\n",
    "            x_transforms=[],\n",
    "            model=GaussianProcessExact,\n",
    "            epochs=50,\n",
    "            n_samples=n_samples,\n",
    "            full_covariance=True,\n",
    "            output_from_samples=False\n",
    "        )\n",
    "        gp.fit(x, y)\n",
    "        # z_pred = gp.model.predict(gp.transforms[0](x[: x.shape[0] // 2]))\n",
    "        z_pred = gp.model.predict(x[: x.shape[0] // 2])\n",
    "        assert isinstance(z_pred, GaussianLike)\n",
    "        inv_gaussian = gp._inv_transform_y_gaussian(z_pred)\n",
    "        inv_sample = gp._inv_transform_y_gaussian_sample(z_pred)\n",
    "        print(str(t))\n",
    "        diff = inv_sample.covariance_matrix - inv_gaussian.covariance_matrix\n",
    "        plt.title(f\"n_samples={n_samples}\")\n",
    "        plt.hist(inv_sample.covariance_matrix.flatten().detach().numpy(), bins=list(np.arange(-1, 1, 0.01)), alpha=0.5, label='Sampled')\n",
    "        plt.hist(inv_gaussian.covariance_matrix.flatten().detach().numpy(), bins=list(np.arange(-1, 1, 0.01)), alpha=0.5, label='Analytical')\n",
    "        plt.hist(diff.flatten().detach().numpy(), bins=list(np.arange(-1, 1, 0.01)), alpha=0.5, label='Difference')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807be5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare analytical and sampling solutions for transformed emulators with standardization\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from autoemulate.experimental.types import GaussianLike\n",
    "\n",
    "for t in [VAETransform(latent_dim=2), PCATransform(n_components=2)]:\n",
    "    x, y = make_data(n_targets=3)\n",
    "    for n_samples in [10, 100, 1000, 10000]:\n",
    "        gp = TransformedEmulator(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            x_transforms=[StandardizeTransform()],\n",
    "            y_transforms=[StandardizeTransform(), t],\n",
    "            model=GaussianProcessExact,\n",
    "            epochs=50,\n",
    "            n_samples=n_samples,\n",
    "            full_covariance=True,\n",
    "            output_from_samples=False\n",
    "        )\n",
    "        gp.fit(x, y)\n",
    "        # z_pred = gp.model.predict(gp.transforms[0](x[: x.shape[0] // 2]))\n",
    "        z_pred = gp.model.predict(x[: x.shape[0] // 2])\n",
    "        assert isinstance(z_pred, GaussianLike)\n",
    "        inv_gaussian = gp._inv_transform_y_gaussian(z_pred)\n",
    "        inv_sample = gp._inv_transform_y_gaussian_sample(z_pred)\n",
    "        print(str(t))\n",
    "        diff = inv_sample.covariance_matrix - inv_gaussian.covariance_matrix\n",
    "        plt.title(f\"n_samples={n_samples}\")\n",
    "        plt.hist(inv_sample.covariance_matrix.flatten().detach().numpy(), bins=list(np.arange(-1, 1, 0.01)), alpha=0.5, label='Sampled')\n",
    "        plt.hist(inv_gaussian.covariance_matrix.flatten().detach().numpy(), bins=list(np.arange(-1, 1, 0.01)), alpha=0.5, label='Analytical')\n",
    "        plt.hist(diff.flatten().detach().numpy(), bins=list(np.arange(-1, 1, 0.01)), alpha=0.5, label='Difference')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
