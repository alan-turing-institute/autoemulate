{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Multivariate Gaussian Distributions in AutoEmulate\n",
    "\n",
    "When building emulator models for expensive simulators, we often need to predict **multivariate outputs**. \n",
    "For an input $x$, let the simulator return a $d$-dimensional vector $f(x)\\in\\mathbb R^d$. \n",
    "In practice, we emulate a batch of $n$ inputs simultaneously, collecting:\n",
    "\n",
    "* **Predictive means**:\n",
    "  $\\mu \\;=\\; [\\,\\mu(x_1),\\dots,\\mu(x_n)\\,]^\\top \\;\\in\\; \\mathbb R^{n d}$\n",
    "\n",
    "* **Predictive covariance**:\n",
    "\n",
    "  $$\n",
    "    \\Sigma\n",
    "    \\;=\\;\n",
    "    \\begin{bmatrix}\n",
    "      \\Sigma_{11} & \\Sigma_{12} & \\cdots & \\Sigma_{1n} \\\\\n",
    "      \\Sigma_{21} & \\Sigma_{22} & \\cdots & \\Sigma_{2n} \\\\\n",
    "      \\vdots      & \\vdots      & \\ddots & \\vdots      \\\\\n",
    "      \\Sigma_{n1} & \\Sigma_{n2} & \\cdots & \\Sigma_{nn}\n",
    "    \\end{bmatrix}\n",
    "    \\;\\in\\; \\mathbb R^{nd\\times nd},\n",
    "  $$\n",
    "\n",
    "  where each block $\\Sigma_{ij}\\in\\mathbb R^{d\\times d}$ encodes the *cross-covariance* between outputs $f(x_i)$ and $f(x_j)$, and the diagonal $\\Sigma_{ii}$ is the *marginal* covariance for input $x_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Covariance Structures\n",
    "\n",
    "AutoEmulate provides abstractions to represent and manipulate these structured covariance matrices efficiently, combining flexibility, extensibility, and high performance.\n",
    "In active learning, three useful covariance-based metrics are provided as instance methods of the classes in the table below: \n",
    "`.logdet()` log determinant of covariance, `.trace()` trace of the covariance matrix, `max_eig()` maximum eigenvalue of the covariance matrix (the spectral norm).\n",
    "All of the covariance matricies of the following classes can be represented as a dense matrix, but, as reported below, we can take advantage of the matrix sparsity to speed up computations.\n",
    "\n",
    "<center>\n",
    "\n",
    "| Class           | Description | Representation Shape      | Logdet Complexity | Max Eigen Complexity | Trace Complexity |\n",
    "| -------------- | ----------- | ------------------------- | ----------------- | -------------------- | ---------------- |\n",
    "| `Dense`           | Covariance between dimensions and samples. | $(nd, nd)$              | $O((nd)^3)$     | $O((nd)^3)$        | $O((nd)^2)$    |\n",
    "| `Block_Diagonal` | Only covariance between dimensions, i.e. $\\Sigma_{ij} = 0$ for all $i \\neq j$. | $(n, d, d)$             | $O(nd^3)$       | $O(nd^3)$          | $O(nd^2)$      |\n",
    "| `Diagonal`       | Only variance, i.e. $\\Sigma_{ij} = 0$ and $\\Sigma_{ii}$ is diagonal for all $i\\neq j$. | $(n, d)$                | $O(nd)$         | $O(nd)$            | $O(nd)$        |\n",
    "| `Separable`      | Covariance between samples and dimensions modelled seperately, i.e. $\\Sigma = \\Sigma_n \\otimes \\Sigma_d$ | $(n, n)$ and $(d, d)$ | $O(n^3 + d^3)$  | $O(n^3 + d^3)$     | $O(n^2 + d^2)$ |\n",
    "| `Dirac`          | Represents deterministic point, i.e. $\\Sigma = 0$. | N/A                       | N/A               | N/A                  | N/A              |\n",
    "\n",
    "</center>\n",
    "\n",
    "These structures satisfy the containment relationships in terms of expressivity:\n",
    "\n",
    "$$\n",
    "  \\text{Full} \\supseteq \\text{Block-Diagonal} \\supseteq \\text{Diagonal},\n",
    "  \\quad\n",
    "  \\text{Full} \\supseteq \\text{Separable}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Ensembles, Empirical Gaussians, and conversions\n",
    "\n",
    "We also provide an `Ensemble` class, which constructs a `Dense` representation from a set of the above classes. This is useful, for instance, in query-by-committee in active learning, where we query the simulator at inputs $x$ where a measure of disagreement is high. The ensemble mean and covariance are given for $K$ number of Gaussian (of any of the classes above) as:\n",
    "\n",
    "$$\n",
    "\\mu\n",
    "= \\frac{1}{K}\n",
    "  \\sum_{i=1}^{K}\n",
    "    \\mu_i,\n",
    "$$\n",
    "$$\n",
    "\\Sigma\n",
    "= \\frac{1}{K}\n",
    "  \\sum_{i=1}^{K}\n",
    "    \\Sigma_i\n",
    "+\n",
    "  \\frac{1}{K - 1}\n",
    "  \\sum_{i=1}^{K}\n",
    "    \\bigl(\\mu_i - \\mu\\bigr)\n",
    "    \\bigl(\\mu_i - \\mu\\bigr)^\\top.\n",
    "$$\n",
    "\n",
    "We can also construct a `Dense` representation from a `(n, d)` batch of samples using the `Empirical` class.\n",
    "\n",
    "Note that both `Ensemble` and `Empirical` are subclasses of `Dense`. We provide a `.from_dense()` method for each of the non-dense classes in the table above, as well as a `to_dense()` method.\n",
    "Thus we may do the following for examples:\n",
    "\n",
    "```python\n",
    "# Anisotropic empirical distribution from k samples at n sampling locations, each with d dimensions\n",
    "k, n, d = 1000, 50, 3\n",
    "samples = torch.rand(k, n, d)\n",
    "dist0 = Empirical(samples)\n",
    "\n",
    "# Isotropic empirical distribution (set off-diagonal elements to zero)\n",
    "samples = torch.rand(k, n, d)\n",
    "dist1 = Diagonal.from_dense(Empirical(samples))\n",
    "\n",
    "# Just to demonstrate to the .to_dense() method\n",
    "dist1 = Diagonal.from_dense(dist1.to_dense())\n",
    "\n",
    "# We can combine them into an ensemble\n",
    "dist2 = Ensemble([dist0, dist1])\n",
    "print(dist2.logdet(), dist2.trace(), dist2.max_eig())\n",
    "```\n",
    "\n",
    "which yields\n",
    "\n",
    "```python\n",
    "tensor(-375.8140) tensor(12.4797) tensor(0.1201)\n",
    "```\n",
    "\n",
    "Below we benchmark the models that can be easily constructed with `.from_dense()`. Note that `Seperable` does not have this method (it only has `.to_dense()`), as the conversion is not bijective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, time\n",
    "import matplotlib.pyplot as plt\n",
    "from autoemulate.experimental.data.gaussian import *\n",
    "from typing import List\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "k, n, d = 1000, 50, 3\n",
    "samples = torch.rand(k, n, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(classes: List[Structured], n_runs: int, n_list: List[int], d_list: List[int], k: int) -> pd.DataFrame:\n",
    "\n",
    "    # For timing and comparison\n",
    "    def compare(f_struct, f_dense):\n",
    "        t0 = time.perf_counter()\n",
    "        r0 = float(f_struct())\n",
    "        t1 = time.perf_counter()\n",
    "        r1 = float(f_dense())\n",
    "        t2 = time.perf_counter()\n",
    "        T_struct, T_dense = t1 - t0, t2 - t1\n",
    "        return T_struct, T_dense, r0, r1\n",
    "    \n",
    "    # Results\n",
    "    results = []\n",
    "    for cls in classes:\n",
    "        for n in n_list:\n",
    "            for d in d_list:\n",
    "                for _ in range(n_runs):\n",
    "                    # Generate random samples\n",
    "                    samples = torch.rand(k, n, d)\n",
    "                    dist_struct = cls.from_dense(Empirical(samples))\n",
    "                    dist_dense = dist_struct.to_dense()\n",
    "                    for f_struct, f_dense in zip(\n",
    "                        [dist_struct.logdet, dist_struct.trace, dist_struct.max_eig],\n",
    "                        [dist_dense.logdet, dist_dense.trace, dist_dense.max_eig]\n",
    "                    ):\n",
    "                        T_struct, T_dense, r0, r1 = compare(f_struct, f_dense)\n",
    "                        results.append({\n",
    "                            \"Type\": cls.__name__,\n",
    "                            \"Function\": f_struct.__name__,\n",
    "                            \"Dense time\": T_dense * 1e6,\n",
    "                            \"Structured time\": T_struct * 1e6,\n",
    "                            \"Dense result\": r0,\n",
    "                            \"Structured result\": r1,\n",
    "                            \"Size\": n * d\n",
    "                        })\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(results, typ, function, metric, ax=None):\n",
    "    sub = (\n",
    "        results[(results[\"Type\"] == typ) & (results[\"Function\"] == function)]\n",
    "        .groupby(\"Size\")[metric]\n",
    "        .agg(mean=\"mean\", std=\"std\")\n",
    "        .reset_index()\n",
    "    )\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    # draw onto the explicit Axes object\n",
    "    ax.plot(sub[\"Size\"], sub[\"mean\"],\n",
    "            label=metric, marker=\"o\", linestyle=\"-\")\n",
    "    ax.fill_between(\n",
    "        sub[\"Size\"],\n",
    "        sub[\"mean\"] - sub[\"std\"],\n",
    "        sub[\"mean\"] + sub[\"std\"],\n",
    "        alpha=0.2,\n",
    "    )\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = compare(\n",
    "    classes=[\n",
    "        Block_Diagonal,\n",
    "        Diagonal,\n",
    "        Dirac,\n",
    "    ],\n",
    "    n_runs=100,\n",
    "    n_list=[10, 20, 50, 100],\n",
    "    d_list=[2, 4, 8, 16],\n",
    "    k=100\n",
    ")\n",
    "for t in [\"Block_Diagonal\", \"Diagonal\", \"Dirac\"]:\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    fig.suptitle(f\"Comparison of {t} structure\")\n",
    "    for i, f in enumerate([\"logdet\", \"trace\", \"max_eig\"]):\n",
    "        plt.sca(ax[i])  # ← make ax[i] current\n",
    "        plot_results(results, t, f, \"Dense time\", ax=ax[i])\n",
    "        plot_results(results, t, f, \"Structured time\", ax=ax[i])\n",
    "        ax[i].set_title(f\"{t} {f}\")\n",
    "        ax[i].set_xlabel(\"Size\")\n",
    "        ax[i].set_ylabel(\"Time (ms)\")\n",
    "        ax[i].legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
