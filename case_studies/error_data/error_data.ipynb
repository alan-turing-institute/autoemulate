{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Simulate some data and fit an emulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from autoemulate.experimental.emulators import GaussianProcessExact\n",
    "from autoemulate.experimental.emulators.transformed.base import TransformedEmulator\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_x = pd.read_csv(\"./error_data/3d/3_parameters.csv\")\n",
    "# df_y = pd.read_csv(\"./error_data/3d/output_3_params.csv\")\n",
    "df_x = pd.read_csv(\"./data/2d/2_parameters.csv\")\n",
    "df_y = pd.read_csv(\"./data/2d/output_2_params.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor(df_x.to_numpy()).float()\n",
    "y = torch.Tensor(df_y.to_numpy()).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.manual_seed(0)\n",
    "idx = torch.randperm(x.shape[0])\n",
    "# idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x[idx]\n",
    "y = y[idx, 1:]\n",
    "# y = y[idx, :1]\n",
    "x.shape, y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train, y_train, x_test, y_test = x[:800], y[:800], x[800:], y[800:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(df_y.iloc[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from autoemulate.experimental.compare import AutoEmulate\n",
    "from autoemulate.experimental.emulators.gaussian_process.kernel import rbf, rbf_plus_constant\n",
    "from autoemulate.experimental.emulators.transformed.base import TransformedEmulator\n",
    "from autoemulate.experimental.transforms import StandardizeTransform, PCATransform\n",
    "\n",
    "em = TransformedEmulator(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    model=GaussianProcessExact,\n",
    "    x_transforms=[StandardizeTransform()],\n",
    "    y_transforms=[StandardizeTransform()],\n",
    "    covar_module_fn=rbf_plus_constant,\n",
    ")\n",
    "\n",
    "em.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import R2Score\n",
    "from autoemulate.experimental.model_selection import evaluate\n",
    "\n",
    "evaluate(em.predict(x_train), y_train, metric=R2Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(em.predict(x_test), y_test, metric=R2Score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoemulate.experimental.compare import AutoEmulate\n",
    "\n",
    "ae = AutoEmulate(x, y, models=[GaussianProcessExact], log_level=\"debug\", n_iter=2, n_splits=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(df_x.iloc[:, 0], df_y.iloc[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ae.plot(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from autoemulate.experimental.sensitivity_analysis import SensitivityAnalysis\n",
    "\n",
    "problem = {\n",
    "    \"num_vars\": 2,\n",
    "    \"names\": [\"target_snr_db\", \"phase_offset\"],\n",
    "    \"bounds\": [(-2.0, 30.0), (-1.0, 1.0)],\n",
    "}\n",
    "\n",
    "sa = SensitivityAnalysis(emulator=ae.best_result().model, problem=problem)\n",
    "\n",
    "df_sa = sa.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa.plot_sobol(df_sa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simple HMC example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoemulate.experimental.calibration.bayes import BayesianCalibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with an \"observation\" that the GP has been trained on. \n",
    "\n",
    "Specifically, we will pretend we have N noisy experimental measurements. We should be able to recover the input parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = -1 # which simulated value to pick\n",
    "n_obs = 10\n",
    "noise_scale = 0.05 # set noise as some ratio of the observed value\n",
    "\n",
    "observations = {\"SER\": torch.Tensor([0.8])}\n",
    "observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parameters_range = {\n",
    "    \"target_snr_db\": (-2.0, 30.0),\n",
    "    \"phase_offset\": (-1.0, 1.0)\n",
    "}\n",
    "# use the simulator parameter_range \n",
    "bc = BayesianCalibration(em, parameters_range, observations, 10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run MCMC (note that below we have set the number of MCMC steps to a very low number, don't expect convergence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc = bc.run_mcmc(\n",
    "    warmup_steps=100, \n",
    "    num_samples=1000,\n",
    "    sampler='nuts',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned Pyro MCMC object has methods for accessing the generated samples (`mcmc.get_samples()`) or, as shown below, to get their summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Plotting with Arviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have an option to turn the MCMC object into an Arviz object, which can be passed to any of their plotting function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az_data = bc.to_arviz(mcmc, posterior_predictive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(az_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_pair(az_data, kind='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_ppc(az_data, kind='scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_autocorr(az_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Use sensitivity analysis and history matching to refine problem before running HMC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `MCMC_calibration` object has an option to provide a list of parameters to calibrate. A common approach is to select these based on results of `SensitivityAnalysis`.\n",
    "\n",
    "Similarly, the user provides parameter ranges from withing which to sample parameter values. This can be simply the range of the simulator. Alternatively, one can use `HistoryMatching` to reduce the parameter range and pass that to the `MCMC_calibration` instead. \n",
    "\n",
    "Below we demonstrate how to do both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoemulate.experimental.sensitivity_analysis import SensitivityAnalysis\n",
    "from autoemulate.experimental.calibration.history_matching import HistoryMatching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Run sensitivity analysis and get top N parameters (here we just get the top 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = {\n",
    "        \"num_vars\": 2,\n",
    "        \"names\": sim.param_names,\n",
    "        \"bounds\": sim.param_bounds,\n",
    "    }\n",
    "\n",
    "sa = SensitivityAnalysis(gp, problem=problem)\n",
    "df = sa.run(\"sobol\")\n",
    "\n",
    "top_param = sa.top_n_sobol_params(df, 1)\n",
    "\n",
    "# the output is just a list of strings, this could be set by hand\n",
    "top_param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Run history matching and generate new parameter bounds from NROY samples (if get any)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with some GP predictions\n",
    "x_new = sim.sample_inputs(20)\n",
    "output = gp.predict(torch.tensor(x_new, dtype=torch.float32))\n",
    "pred_means, pred_vars = (\n",
    "    output.mean.float().detach(),\n",
    "    output.variance.float().detach(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate NROY samples\n",
    "hm = HistoryMatching(\n",
    "    # take mean of observations and add noise\n",
    "    observations={k: [v.mean(), 10.0] for k,v in observations.items()},\n",
    "    threshold=5.0,\n",
    "    rank=2\n",
    ")\n",
    "implausability = hm.calculate_implausibility(pred_means, pred_vars)\n",
    "nroy_samples = hm.get_nroy(implausability, x_new)\n",
    "nroy_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The newly generated range is slightly narrower than the range of the simulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get new param bounds\n",
    "nroy_param_range = hm.generate_param_bounds(nroy_samples, param_names = sim.param_names)\n",
    "nroy_param_range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Pass results to the `MCMC_calibration` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_nroy = BayesianCalibration(\n",
    "    gp, \n",
    "    nroy_param_range if nroy_param_range is not None else sim.parameters_range, \n",
    "    observations, \n",
    "    10.0,\n",
    "    top_param\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc_nroy = bc_nroy.run_mcmc(warmup_steps=10, num_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc_nroy.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
