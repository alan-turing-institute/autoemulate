{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-end AutoEmulate workflow \n",
    "\n",
    "## Overview\n",
    "\n",
    "<!-- <b>In this workflow we demonstrate the integration of a Cardiovascular simulator, Naghavi Model from ModularCirc in an end-to-end AutoEmulate workflow.</b>  -->\n",
    "\n",
    "Naghavi model is a 0D (zero-dimensional) computational model of the cardiovascular system, which is used to simulate blood flow and pressure dynamics in the heart and blood vessels.\n",
    "\n",
    "This demo includes:\n",
    "- Setting up parameter ranges \n",
    "- Creating samples\n",
    "- Running the simulator to generate training data for the emulator \n",
    "- Using AutoEmulate to find the best pre-processing technique and model tailored to the simulation data \n",
    "- Applying history matching to refine the model and enhance parameter ranges \n",
    "- Sensitivity Analysis \n",
    "- Bayesian calibration\n",
    "\n",
    "TODO: update figure?\n",
    "<!-- <img src=\"https://raw.githubusercontent.com/alan-turing-institute/autoemulate/refs/heads/main/misc/workflow.png\" alt=\"Work Flow\" style=\"width:100%;\"/> -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Set up simulator and generate data\n",
    "\n",
    "TODO: add link here to how subclass the `Simulator` class to integrate their own simulator into an `AutoEmulate` workflow (see `docs/experimental/tutorials/simulator/01_custom_simulations.ipynb`)\n",
    "\n",
    "Below we import all that we need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `NaghaviSimulator` models a range of parameters. Below we choose to only track a subset of those. Note that the simulator is set up to output summary statistics for each of the tracked variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cardiac_simulator import NaghaviSimulator\n",
    "\n",
    "simulator = NaghaviSimulator(\n",
    "    output_variables=['lv.P_i', 'lv.P_o'],  # Only the ones you're interested in\n",
    "    n_cycles=300, \n",
    "    dt=0.001,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simulator comes with predefined input parameters ranges. We can sample from those using Latin Hypercube Sampling to generate data to train the emulator with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ao.r': (120.0, 360.0),\n",
       " 'ao.c': (0.15, 0.44999999999999996),\n",
       " 'art.r': (562.5, 1687.5),\n",
       " 'art.c': (1.5, 4.5),\n",
       " 'ven.r': (4.5, 13.5),\n",
       " 'ven.c': (66.65, 199.95000000000002),\n",
       " 'av.r': (3.0, 9.0),\n",
       " 'mv.r': (2.05, 6.1499999999999995),\n",
       " 'la.E_pas': (0.22, 0.66),\n",
       " 'la.E_act': (0.225, 0.675),\n",
       " 'la.v_ref': (5.0, 15.0),\n",
       " 'la.k_pas': (0.01665, 0.07500000000000001),\n",
       " 'lv.E_pas': (0.5, 1.5),\n",
       " 'lv.E_act': (1.5, 4.5),\n",
       " 'lv.v_ref': (5.0, 15.0),\n",
       " 'lv.k_pas': (0.00999, 0.045)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulator.parameters_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_samples = 100\n",
    "x = simulator.sample_inputs(N_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the simulator to generate predictions for the sampled parameters. Alternatively, for convenience. we can load already simulated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = False\n",
    "save = True\n",
    "read = True\n",
    "\n",
    "if run:\n",
    "    # Run batch simulations with the samples generated in Cell 1\n",
    "    y = simulator.forward_batch(x)\n",
    "\n",
    "    # Convert results to DataFrame for analysis\n",
    "    results_df = pd.DataFrame(y)\n",
    "    \n",
    "    if save:\n",
    "        # Save the results to a CSV file\n",
    "        results_df.to_csv('simulator_results.csv', index=False)\n",
    "\n",
    "if read:\n",
    "    # Read the results from the CSV file\n",
    "    results_df = pd.read_csv('simulator_results.csv')\n",
    "    y = torch.tensor(results_df.to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the output variables we've simulated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lv.P_i_min',\n",
       " 'lv.P_i_max',\n",
       " 'lv.P_i_mean',\n",
       " 'lv.P_i_range',\n",
       " 'lv.P_o_min',\n",
       " 'lv.P_o_max',\n",
       " 'lv.P_o_mean',\n",
       " 'lv.P_o_range']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulator.output_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Train emulator with AutoEmulate\n",
    " \n",
    "User should choose from the available `models` the `models` they would like to investigate. Here we restrict to just Gaussian Processes as we need an uncertainty aware emulator.\n",
    "\n",
    "TODO: below we also use PCA to reduce dimensionality of the outputs but we should not do this if we're working with summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Comparing models: 100%|██████████| 1.00/1.00 [00:12<00:00, 12.9s/model]\n"
     ]
    }
   ],
   "source": [
    "from autoemulate.experimental.compare import AutoEmulate\n",
    "from autoemulate.experimental.emulators.gaussian_process.exact import GaussianProcessExact\n",
    "from autoemulate.experimental.transforms.pca import PCATransform\n",
    "\n",
    "ae = AutoEmulate(\n",
    "    x, \n",
    "    y, \n",
    "    models=[GaussianProcessExact], \n",
    "    y_transforms_list=[[PCATransform(n_components=2)]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: the GPs are not doing great here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>x_transforms</th>\n",
       "      <th>y_transforms</th>\n",
       "      <th>config</th>\n",
       "      <th>rmse_test</th>\n",
       "      <th>r2_test</th>\n",
       "      <th>r2_test_std</th>\n",
       "      <th>r2_train</th>\n",
       "      <th>r2_train_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GaussianProcessExact</td>\n",
       "      <td>[StandardizeTransform()]</td>\n",
       "      <td>[PCATransform()]</td>\n",
       "      <td>{'mean_module_fn': &lt;function linear_mean at 0x...</td>\n",
       "      <td>3.798054</td>\n",
       "      <td>-0.246474</td>\n",
       "      <td>0.253701</td>\n",
       "      <td>0.517621</td>\n",
       "      <td>0.025239</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             model_name              x_transforms      y_transforms  \\\n",
       "0  GaussianProcessExact  [StandardizeTransform()]  [PCATransform()]   \n",
       "\n",
       "                                              config  rmse_test   r2_test  \\\n",
       "0  {'mean_module_fn': <function linear_mean at 0x...   3.798054 -0.246474   \n",
       "\n",
       "   r2_test_std  r2_train  r2_train_std  \n",
       "0     0.253701  0.517621      0.025239  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ae.summarise()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the best performing emulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ae.best_result().model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Sensitivity Analysis \n",
    "Use AutoEmulate to perform sensitivity analysis. This will help identify the parameters that have higher impact on the outputs to narrow down the search space for performing model calibration. \n",
    "\n",
    "TODO: could link to sensitivity analysis overview notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoemulate.experimental.sensitivity_analysis import SensitivityAnalysis\n",
    "\n",
    "# Define the problem dictionary for Sobol sensitivity analysis\n",
    "problem = {\n",
    "    'num_vars': simulator.in_dim,\n",
    "    'names': simulator.param_names,\n",
    "    'bounds': simulator.param_bounds\n",
    "}\n",
    "\n",
    "si = SensitivityAnalysis(model, problem=problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: currently the below runs super slow, have no idea why. It crashed when I tried running it on 8 output variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG   2025-07-17 15:48:50,981 - autoemulate - Running sensitivity analysis with method=sobol, n_samples=1024, conf_level=0.95\n"
     ]
    }
   ],
   "source": [
    "# si_df = si.run(method='sobol')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# si.plot_sobol(si_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4: Model calibration\n",
    "\n",
    "To refine our emulator, we need real-world observations to compare against. These observations can come from experiments reported in the literature. \n",
    "\n",
    "In this example, we'll generate synthetic \"observations\" by running the simulator at the midpoint of each parameter range, treating these as our \"ground truth\" values for calibration. Note that in a real world example one can have multiple observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of how to define observed data with means and variances from a hypothetical experiment\n",
    "observations = {\n",
    "    'lv.P_i_min': (5.0, 0.1),   # Minimum of minimum LV pressure\n",
    "    'lv.P_i_max': (20.0, 0.1),   # Maximum of minimum LV pressure\n",
    "    'lv.P_i_mean': (10.0, 0.1),  # Mean of minimum LV pressure\n",
    "    'lv.P_i_range': (15.0, 0.5), # Range of minimum LV pressure\n",
    "    'lv.P_o_min': (1.0, 0.1),  # Minimum of maximum LV pressure\n",
    "    'lv.P_o_max': (13.0, 0.1),  # Maximum of maximum LV pressure\n",
    "    'lv.P_o_mean': (12.0, 0.1), # Mean of maximum LV pressure\n",
    "    'lv.P_o_range': (20.0, 0.5)  # Range of maximum LV pressure\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otherwise, use one forward pass of your simualtion to get the observed data\n",
    "# Calculate midpoint parameters\n",
    "midpoint_params = []\n",
    "for param_name, (min_val, max_val) in simulator.parameters_range.items():\n",
    "    midpoint_params.append((min_val + max_val) / 2.0)\n",
    "# Run the simulator with midpoint parameters\n",
    "midpoint_results = simulator.forward(torch.tensor(midpoint_params).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lv.P_i_min': (12.257057189941406, 0.12257057189941406),\n",
       " 'lv.P_i_max': (22.596208572387695, 0.22596208572387697),\n",
       " 'lv.P_i_mean': (20.69025421142578, 0.20690254211425782),\n",
       " 'lv.P_i_range': (10.339152336120605, 0.10339152336120605),\n",
       " 'lv.P_o_min': (12.257057189941406, 0.12257057189941406),\n",
       " 'lv.P_o_max': (22.596208572387695, 0.22596208572387697),\n",
       " 'lv.P_o_mean': (20.69025421142578, 0.20690254211425782),\n",
       " 'lv.P_o_range': (10.339152336120605, 0.10339152336120605)}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create observations dictionary\n",
    "observations = {\n",
    "    name: (val.item(), max(abs(val.item()) * 0.01, 0.01)) for\n",
    "    name, val in \n",
    "    zip(simulator.output_names, midpoint_results[0])}\n",
    "observations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### History Matching\n",
    " \n",
    "Once you have the final model, running history matching can improve your model. The Implausibility metric is calculated using the following relation for each set of parameter:\n",
    "\n",
    "$I_i(\\overline{x_0}) = \\frac{|z_i - \\mathbb{E}(f_i(\\overline{x_0}))|}{\\sqrt{\\text{Var}[z_i - \\mathbb{E}(f_i(\\overline{x_0}))]}}$\n",
    "Where if implosibility ($I_i$) exceeds a threshhold value, the points will be rulled out. \n",
    "The outcome of history matching are the NORY (Not Ruled Out Yet) and RO (Ruled Out) points.\n",
    "\n",
    "- create a dictionary of your observations, this should match the output names of your simulator \n",
    "- create the history matching object \n",
    "- run history matching \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoemulate.experimental.calibration.history_matching import HistoryMatchingWorkflow\n",
    "\n",
    "hmw = HistoryMatchingWorkflow(\n",
    "    simulator=simulator,\n",
    "    emulator=model,\n",
    "    observations=observations,\n",
    "    threshold=3.0,\n",
    "    train_x=x.float(),\n",
    "    train_y=y.float()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running simulations: 100%|██████████| 20.0/20.0 [00:20<00:00, 1.01s/sample]\n"
     ]
    }
   ],
   "source": [
    "# Run history matching\n",
    "test_parameters, impl_scores = hmw.run(n_simulations=20, n_test_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "527f0d5797a242719e87bc7355dbe47f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<h2>History Matching Dashboard</h2>')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2586bc84a334321b0d56d37daa66e20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='Plot Type:', options=('Parameter vs Implausibility', 'Pair…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from autoemulate.history_matching_dashboard import HistoryMatchingDashboard\n",
    "dashboard = HistoryMatchingDashboard(\n",
    "    samples=test_parameters,\n",
    "    impl_scores=impl_scores,\n",
    "    param_names=simulator.param_names,  \n",
    "    output_names=simulator.output_names, \n",
    "    )\n",
    "dashboard.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/alan-turing-institute/autoemulate/refs/heads/main/misc/vis_dashboard_pic_sample.png\" alt=\"Work Flow\" style=\"width:20%;\"/> \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11 - MCMC\n",
    "Once you have identified the important parameters through the Sensitivity analysis tool, the MCMC module can return the calibrated parameter values with uncertainty. \n",
    "The MCMC algorithm tries to find parameter values that match the predictions by the emulator to your `observations` whilst staying within the `parameters_range` (priors)\n",
    "and accounting for uncertainty.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Takes a pre-trained emulator (surrogate model)\n",
    "- Uses sensitivity analysis results to identify the most important parameters\n",
    "- Accepts observations (real data) to calibrate against\n",
    "- Optionally incorporates NROY (Not Ruled Out Yet) samples from prior history matching\n",
    "- Sets up parameter bounds for calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoemulate.mcmc import MCMCCalibrator\n",
    "# Define your observations (what you want to match)\n",
    "# Define observed data with means and variances\n",
    "\n",
    "\n",
    "# Run calibration\n",
    "calibrator = MCMCCalibrator(\n",
    "    emulator=gp_final,\n",
    "    sensitivity_results=si,\n",
    "    observations=observations,\n",
    "    parameter_bounds=parameters_range,\n",
    "    nroy_samples=nroy_samples,\n",
    "    nroy_indices=nroy_indices,\n",
    "    all_samples=all_samples,\n",
    "    top_n_params=3  # Calibrate top 5 most sensitive parameters\n",
    ")\n",
    "\n",
    "results = calibrator.run_mcmc(num_samples=100, warmup_steps=10)\n",
    "# Get calibrated parameter values\n",
    "calibrated_params = calibrator.get_calibrated_parameters()\n",
    "calibrated_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoemulate.mcmc_dashboard import MCMCVisualizationDashboard\n",
    "dashboard = MCMCVisualizationDashboard(calibrator)\n",
    "dashboard.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Footnote: Testing the dashboard\n",
    "\n",
    "Sometimes it is hard to know, if the results we are seeing is because the code is not working, or our simulation results are more interesting than we expected. Here is a little test dataset which tests the dashboard, so that you can see how the plots are supposed to look liek and what they shouldf show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test sample with KNOWN NROY regions\n",
    "test_samples = np.array([[x, y] for x in np.linspace(0,1,100) \n",
    "                               for y in np.linspace(0,1,100)])\n",
    "test_scores = (abs(test_samples[:, 0]-0.5)+abs(test_samples[:, 1]-0.5)).reshape(-1, 1)\n",
    "\n",
    "# Should show a clear diagonal pattern\n",
    "test_dash = HistoryMatchingDashboard(\n",
    "    samples=test_samples,\n",
    "    impl_scores=test_scores,\n",
    "    param_names=[\"p1\", \"p2\"],\n",
    "    output_names=[\"out1\"],\n",
    "    threshold=0.7  # ~50% of points should be NROY\n",
    ")\n",
    "#test_dash.display()"
   ]
  }
 ],
 "metadata": {
  "jupyter": {
   "tags": [
    "skip-execution"
   ]
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
