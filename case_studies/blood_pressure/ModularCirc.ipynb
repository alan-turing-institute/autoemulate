{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patient calibration workflow using AutoEmulate\n",
    "\n",
    "## Introduction\n",
    "\n",
    "\n",
    "## The Nagavi model\n",
    "\n",
    "<!-- <b>In this workflow we demonstrate the integration of a Cardiovascular simulator, Naghavi Model from ModularCirc in an end-to-end AutoEmulate workflow.</b>  -->\n",
    "\n",
    "The Nagavi lumped parameter model is a mathematical model of the human cardiovascular system, designed to simulate the dynamics of blood flow and pressure throughout the heart and circulatory system using lumped parameter modeling. \n",
    "A **lumped parameter model** simplifies the cardiovascular system by dividing it into compartments (or \"lumps\") such as:\n",
    "\n",
    "- Heart chambers (left and right atria and ventricles)\n",
    "- Major blood vessels (aorta, vena cava, pulmonary arteries and veins)\n",
    "- Systemic and pulmonary circulations\n",
    "\n",
    "Each compartment is modeled using analogies to electrical circuits:\n",
    "\n",
    "- Pressure ↔ Voltage\n",
    "- Flow ↔ Current\n",
    "- Resistance ↔ Vascular resistance (R\\)\n",
    "- Compliance ↔ Vessel elasticity or capacitance (C\\)\n",
    "- Inertance ↔ Blood inertia (L)\n",
    "\n",
    "This approach allows simulation of the time-dependent relationships between pressure, volume, and flow rate across the entire cardiovascular system using ordinary differential equations (ODEs).\n",
    "\n",
    "The Nagavi lumped parameter model is a mathematical model of the human cardiovascular system, designed to simulate the dynamics of blood flow and pressure throughout the heart and circulatory system using lumped parameter modeling. \n",
    "A **lumped parameter model** simplifies the cardiovascular system by dividing it into compartments (or \"lumps\") such as:\n",
    "\n",
    "## Patient calibration workflow\n",
    "\n",
    "In this tutorial, we present a three-stage workflow for calibrating the Nagavi model to patient-specific clinical data using AutoEmulate. The process has the following stages:\n",
    "\n",
    "- First we perform a global sensitivity analysis, which identifies the most influential parameters affecting model outputs and reduces the dimensionality of the calibration problem. \n",
    "- Next, we apply history matching, a sequential uncertainty quantification technique that uses emulators to efficiently rule out implausible regions of the parameter space based on observed patient data. This results in a restricted, plausible region—known as the NROY (Not Ruled Out Yet) space—where parameters are consistent with the clinical measurements within acceptable uncertainty bounds. \n",
    "- Finally, we perform Bayesian inference within this NROY region to estimate the full posterior distribution of the remaining parameters, capturing the most likely values and their associated uncertainty. \n",
    "\n",
    "### Global sensitivity analysis\n",
    "\n",
    "The Nagavi model has 16 parameters which makes individual patient calibration challenging. To address this we use a emulator-based global sensitivity analysis to quantify the influence each parameter on features derived from left ventricle artery pressure. This approach reduces the parameters that will be used in model personalization from 16 to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m ensurepip --upgrade\n",
    "!{sys.executable} -m pip install --upgrade pip\n",
    "!{sys.executable} -m pip install numpy==2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up simulator and generate data\n",
    "\n",
    "For this tutorial we use `ModularCirc` a package that providse a framework for building 0D models and simulating cardiovascular flow and mechanics. The `NaghaviSimulator` simulates pressure traces, we then choose to output summary statistics for each of the simulated traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cardiac_simulator import NaghaviSimulator\n",
    "\n",
    "simulator = NaghaviSimulator(\n",
    "    output_variables=['lv.P'],  # We simulate the left ventricle pressure, flow and volume traces\n",
    "    n_cycles=300, \n",
    "    dt=0.001,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simulator comes with predefined input parameters ranges. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator.parameters_range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can sample from those using Latin Hypercube Sampling to generate data to train the emulator with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_samples = 1024\n",
    "x = simulator.sample_inputs(N_samples,random_seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the simulator to generate predictions for the sampled parameters. Alternatively, for convenience. we can load already simulated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "N_samples = 1024\n",
    "save = True\n",
    "\n",
    "if not os.path.exists(f'simulator_results_{N_samples}.csv'):\n",
    "    # Run batch simulations with the samples generated in Cell 1\n",
    "    y, x = simulator.forward_batch_skip_failures(x)\n",
    "    \n",
    "    # Convert results to DataFrame for analysis\n",
    "    results_df = pd.DataFrame(y)\n",
    "    inputs_df = pd.DataFrame(x)\n",
    "    \n",
    "    if save:\n",
    "        # Save the results to a CSV file\n",
    "        results_df.to_csv(f'simulator_results_{N_samples}.csv', index=False)\n",
    "        inputs_df.to_csv(f'simulator_inputs_{N_samples}.csv', index=False)\n",
    "\n",
    "else:\n",
    "    # Read the results from the CSV file\n",
    "    results_df = pd.read_csv(f'simulator_results_{N_samples}.csv')\n",
    "    inputs_df = pd.read_csv(f'simulator_inputs_{N_samples}.csv')\n",
    "\n",
    "    y = torch.tensor(results_df.to_numpy())\n",
    "    x = torch.tensor(inputs_df.to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the output summary variables we've simulated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator.output_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train emulator with AutoEmulate\n",
    " \n",
    "To perform sensitivity analysis efficiently, we first need to construct an emulator—a fast, surrogate model that approximates the output of the full simulator. The simulated inputs and outputs from the cell above are  used to train the emulator, in this case we choose to use neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoemulate.core.compare import AutoEmulate\n",
    "\n",
    "from autoemulate.emulators.nn.mlp import MLP\n",
    "\n",
    "ae = AutoEmulate(\n",
    "    x, \n",
    "    y, \n",
    "    models=[MLP],  \n",
    "    model_tuning=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae.summarise()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the best performing emulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ae.best_result().model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Sensitivity Analysis \n",
    "\n",
    "The emulator trained above can predict model outputs rapidly across the entire parameter space, allowing us to estimate global sensitivity measures like Sobol’ indices or Morris elementary effects without repeatedly calling the full simulator. This approach enables scalable and accurate sensitivity analysis, especially in high-dimensional or computationally intensive settings.\n",
    "\n",
    "Here we use AutoEmulate to perform sensitivity analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoemulate.core.sensitivity_analysis import SensitivityAnalysis\n",
    "\n",
    "# Define the problem dictionary for Sobol sensitivity analysis\n",
    "problem = {\n",
    "    'num_vars': simulator.in_dim,\n",
    "    'names': simulator.param_names,\n",
    "    'bounds': simulator.param_bounds\n",
    "}\n",
    "\n",
    "si = SensitivityAnalysis(model, problem=problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "si_df = si.run(method='sobol')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "si.plot_sobol(si_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# si.plot_sa_heatmap(si_df, index='ST', cmap='coolwarm', normalize=True, figsize=(10, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can select the top 5 parameters that have the biggest influcence on the pressure wave summary statistics extracted from the Nagavi Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_parameters_sa = si.top_n_sobol_params(si_df,top_n=3)\n",
    "top_parameters_sa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters that are found to be less influential are fixed to a mid point value within its range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_range = {}\n",
    "for param_name, (min_val, max_val) in simulator.parameters_range.items():\n",
    "    if param_name not in top_parameters_sa:\n",
    "        print(f\"Fixing parameter {param_name} to a value within its range ({min_val}, {max_val})\")\n",
    "        midpoint_value = (max_val + min_val) / 2.0\n",
    "        updated_range[param_name] = (midpoint_value,midpoint_value)\n",
    "    else:\n",
    "        updated_range[param_name] = simulator.parameters_range[param_name]# Fix to a value\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Updated parameters range with fixed values for non-sensitive parameters:\")\n",
    "print(updated_range)\n",
    "simulator.parameters_range = updated_range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patient level calibration\n",
    "\n",
    "To refine our emulator, we need real-world observations to compare against. These observations can come from experiments reported in the literature. \n",
    "\n",
    "In this example, we'll generate synthetic \"observations\" by running the simulator at the midpoint of each parameter range, treating these as our \"ground truth\" values for calibration. Note that in a real world example one can have multiple observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate midpoint parameters\n",
    "midpoint_params_patient = []\n",
    "patient_true_values = {}\n",
    "for param_name in simulator.parameters_range:\n",
    "    # Calculate the midpoint of the parameter range\n",
    "    min_val, max_val = simulator.parameters_range[param_name]\n",
    "    midpoint_params_patient.append((max_val + min_val) / 2.0)\n",
    "    patient_true_values[param_name] = midpoint_params_patient[-1]\n",
    "\n",
    "# Run the simulator with midpoint parameters\n",
    "midpoint_results = simulator.forward(torch.tensor(midpoint_params_patient).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create observations dictionary\n",
    "observations = {\n",
    "    name: (val.item(), max(abs(val.item()) * 0.01, 0.01)) for\n",
    "    name, val in \n",
    "    zip(simulator.output_names, midpoint_results[0])}\n",
    "observations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### History Matching\n",
    "\n",
    "Once the influential parameters have been selected with sensitivity analysis, we want to find which values of those parameters are consistent with the clinical data for a specific patient. Rather than directly estimating the parameters, history matching first focuses on excluding regions of the parameter space that are not plausible.\n",
    "\n",
    "AutoEmulate has the history matching workflow where we use the simulator and a fast emulator to generate model predictions for many parameter combinations.\n",
    "\n",
    "For each simulation, \n",
    "\n",
    "- Compare the model output f(θ) to the observed data $y_{obs}$.  \n",
    "- Compute an implausibility measure for each parameter set: $I_i(\\overline{x_0}) = \\frac{|z_i - \\mathbb{E}(f_i(\\overline{x_0}))|}{\\sqrt{\\text{Var}[z_i - \\mathbb{E}(f_i(\\overline{x_0}))]}}$\n",
    "- Rule out all θ such that I(θ)>threshold (e.g., 3).\n",
    "\n",
    "Repeat this in waves:\n",
    "\n",
    "- After each wave, retrain the emulator on the non-implausible region (NROY).\n",
    "- Stop when the NROY region changes little between waves (e.g., <10% of new points are excluded).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to train a Gaussian Process emulator as we need uncertainty quantification for History Matching. Let's start generated a new dataset only sampling the most sensitive parameters and use this to train the GP emulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = simulator.sample_inputs(N_samples,random_seed=42)\n",
    "y, x = simulator.forward_batch_skip_failures(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoemulate.emulators.gaussian_process.kernel import matern_3_2_kernel\n",
    "\n",
    "\n",
    "ae_hm = AutoEmulate(\n",
    "    x, \n",
    "    y, \n",
    "    models=[\"GaussianProcess\"],  \n",
    "    model_tuning=False,\n",
    "    model_params = {\n",
    "        'covar_module': matern_3_2_kernel,\n",
    "        'standardize_x': True,\n",
    "        'standardize_y': True\n",
    "        \n",
    "    }\n",
    ")\n",
    "\n",
    "res = ae_hm.best_result()\n",
    "gp_matern = res.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a HistoryMatchingWorkflow object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoemulate.calibration.history_matching import HistoryMatchingWorkflow\n",
    "\n",
    "hmw = HistoryMatchingWorkflow(\n",
    "    simulator=simulator,\n",
    "    result=res,\n",
    "    observations=observations,\n",
    "    threshold=3.0,\n",
    "    train_x=x.float(),\n",
    "    train_y=y.float()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run waves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results\n",
    "history_matching_results = hmw.run_waves(n_waves=6, n_simulations=N_samples, n_test_samples=2000,max_retries=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This figure shows the implausibility scores for each parameter combination, allowing us to visualize which regions of the parameter space are plausible (i.e., not ruled out) based on the observed data. The NROY region is highlighted, showing the parameters that remain after history matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings # To suppress some warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"seaborn\")\n",
    "\n",
    "sa_parameter_idx = [simulator.get_parameter_idx(param) for param in top_parameters_sa]\n",
    "for i, (test_parameters, impl_scores) in enumerate(hmw.wave_results):\n",
    "    \n",
    "    df = pd.DataFrame(test_parameters[:, sa_parameter_idx], columns=top_parameters_sa)\n",
    "    df[\"Implausibility\"] = impl_scores.mean(axis=1)\n",
    "    df = df[df[\"Implausibility\"] < hmw.threshold]  \n",
    "    \n",
    "    pairplot = sns.pairplot(df, hue=\"Implausibility\", palette=\"viridis\",corner=True)\n",
    "    # make the pairplot axes the same across wave plots\n",
    "    # use initial parameter limits from the simulator\n",
    "    for ax in pairplot.axes.flatten():\n",
    "        if ax is not None:\n",
    "            xlabel = ax.get_xlabel()\n",
    "            ylabel = ax.get_ylabel()\n",
    "            if xlabel in simulator.parameters_range:\n",
    "                ax.set_xlim(simulator.parameters_range[xlabel])\n",
    "            if ylabel in simulator.parameters_range:\n",
    "                ax.set_ylim(simulator.parameters_range[ylabel])\n",
    "    pairplot.figure.suptitle(f\"Wave {i+1}\", fontsize=16)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the last wave results\n",
    "test_parameters, impl_scores = hmw.wave_results[-1]\n",
    "nroy_points = hmw.get_nroy(impl_scores,test_parameters) # Implausibility < 3.0\n",
    "# Get exact min/max bounds for the parameters from the NROY points\n",
    "params_post_hm = hmw.generate_param_bounds(nroy_x=nroy_points,param_names=simulator.param_names, buffer_ratio=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_name, bounds in params_post_hm.items():\n",
    "    \n",
    "    print (f\"Pre HM parameter bounds for {param_name}: {simulator.parameters_range[param_name]}\")\n",
    "    print (f\"Post HM parameter bounds for {param_name}: {bounds}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian calibration\n",
    "With the reduced and plausible parameter space from history matching, we now perform Bayesian inference to estimate the posterior distribution of parameters given patient data. We apply the following steps:\n",
    "\n",
    "- Define a prior over parameters using the NROY region from history matching.\n",
    "\n",
    "- Define a likelihood function that compares model predictions to patient data, including observation and model error.\n",
    "\n",
    "- Use a Bayesian method (MCMC) to sample from the posterior.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoemulate.calibration.bayes import BayesianCalibration\n",
    "\n",
    "model_post_hm = hmw.emulator  # Use the emulator from history matching\n",
    "\n",
    "bc = BayesianCalibration(\n",
    "    emulator=model_post_hm,\n",
    "    parameter_range=params_post_hm,\n",
    "    observations = {k: torch.tensor(v[0]) for k,v in observations.items()},\n",
    "    observation_noise={k: v[1] for k,v in observations.items()},\n",
    "    calibration_params = top_parameters_sa\n",
    ")\n",
    "\n",
    "mcmc = bc.run_mcmc(warmup_steps=10, num_samples=10000, sampler='nuts')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check if the posterior samples are consistent with the true values of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "idata = bc.to_arviz(mcmc)\n",
    "\n",
    "# add patient observations as a ref_val: list of floats in the order of top_parameters_sa\n",
    "# {param: float(val) for (param, val) in patient_true_values.items() if param in top_parameters_sa}\n",
    "ref_val = [float(patient_true_values[param]) for param in top_parameters_sa]\n",
    "\n",
    "az.plot_posterior(\n",
    "    idata, \n",
    "    var_names=top_parameters_sa, \n",
    "    kind='hist', \n",
    "    figsize=(10, 6), \n",
    "    ref_val=ref_val\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "jupyter": {
   "tags": [
    "skip-execution"
   ]
  },
  "kernelspec": {
   "display_name": "autoemulate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
