{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Learning with AutoEmulate\n",
    "\n",
    "In this notebook, we'll introduce **active learning (AL)** using AutoEmulate, demonstrating how emulators and simulators work together to efficiently build accurate predictive models.\n",
    "\n",
    "**Why Active Learning?**\n",
    "\n",
    "High-fidelity simulators produce accurate predictions but are computationally expensive. Emulators offer fast approximations but can be unreliable, especially in unexplored regions. Active learning intelligently selects informative simulator evaluations to maximize emulator improvement with minimal computational cost.\n",
    "\n",
    "**Key Components**\n",
    "\n",
    "- **Simulator:** High-fidelity, reliable, expensive to run.\n",
    "- **Emulator:** Fast, inexpensive surrogate model, less reliable in unseen parts of input space, but trainable.\n",
    "\n",
    "**Active Learning Policy**\n",
    "\n",
    "The **policy** $\\mathbf{u}$ determines which input points are evaluated by the simulator. At each iteration, based on:\n",
    "\n",
    "- **Labeled Set ($\\mathcal{D}$):** Evaluated points from the simulator.\n",
    "- **Query Set ($\\mathcal{Q}$):** Potential points for evaluation.\n",
    "\n",
    "The policy either:\n",
    "- Selects a new point $\\mathbf{x}$ for simulator evaluation, updating:\n",
    "  $$\n",
    "  \\mathcal{D}_{i+1} = \\mathcal{D}_i \\cup \\{(\\mathbf{x}, \\mathbf{y})\\}, \\quad \\mathbf{x} = \\mathbf{u}(\\mathcal{D}_i, \\mathcal{Q}_i).\n",
    "  $$\n",
    "- Or stops querying, leaving $\\mathcal{D}$ unchanged:\n",
    "  $$\n",
    "  \\mathcal{D}_{i+1} = \\mathcal{D}_i, \\quad \\mathbf{u}(\\mathcal{D}_i, \\mathcal{Q}_i) = \\emptyset.\n",
    "  $$\n",
    "\n",
    "**Active Learning Scenarios**\n",
    "\n",
    "Active learning typically includes three scenarios:\n",
    "1. **Stream-based AL:** Inputs arrive sequentially; the learner chooses whether to evaluate them.\n",
    "2. **Pool-based AL:** A fixed pool of inputs; the learner chooses which to evaluate.\n",
    "3. **Membership-based AL:** Learner generates inputs from the entire continuous input space.\n",
    "\n",
    "**This Notebook: Stream-Based AL**\n",
    "\n",
    "We focus on **stream-based AL**, where:\n",
    "\n",
    "1. A user queries the emulator at specific inputs.\n",
    "2. If emulator predictions are uncertain, the learner queries the simulator.\n",
    "3. The emulator is retrained on simulator evaluations triggered by uncertainty.\n",
    "4. The learner manages this process.\n",
    "\n",
    "Let's implement stream-based active learning—starting with necessary imports below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch, numpy as np, matplotlib.pyplot as plt, pandas as pd\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict, Tuple, Iterable\n",
    "import warnings\n",
    "\n",
    "\n",
    "# Import core classes from the source code.\n",
    "from autoemulate.experimental.learners import stream\n",
    "from autoemulate.experimental.simulations.base import Simulator\n",
    "from autoemulate.experimental.emulators.gaussian_process.exact import GaussianProcessExact\n",
    "from autoemulate.experimental.types import GaussianLike\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "show_progress = False #if os.environ.get(\"JUPYTER_BOOK\") == \"1\" else True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AutoEmulate provides several experimental implementations of stream-based active learners. These learners differ in how they assess informativeness—whether based on input space, output space, or adaptive thresholds.\n",
    "\n",
    "You can visualize the available options using:\n",
    "\n",
    "```python\n",
    "stream.Stream.plot_hierarchy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream.Stream.plot_hierarchy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active Learning Components\n",
    "\n",
    "To set up active learning in AutoEmulate, you typically need:\n",
    "\n",
    "1. A **simulator** – provides ground truth data (expensive but accurate).  \n",
    "2. An **emulator** – a fast surrogate that approximates the simulator.  \n",
    "3. Some **initial training data** – a small set of labeled input-output pairs to start with.\n",
    "\n",
    "### Defining Simulators\n",
    "\n",
    "Below, we define a simple one-dimensional simulator (`Sin`) where outputs are generated as $y = \\sin(x)$.\n",
    "\n",
    "We also import a 2D simulator (`Projectile`) that maps initial velocity and drag coefficient of a projectile to its final distance and impact velocity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple sine simulator.\n",
    "class Sin(Simulator):\n",
    "    def __init__(self, param_ranges={\"x\": (0., 50.)}, output_names = [\"y\"]):\n",
    "        super().__init__(param_ranges, output_names)\n",
    "    def _forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.sin(X)\n",
    "    \n",
    "# Import Projectile simulator which returns distance travelled by projectile and impact velocity.\n",
    "from autoemulate.experimental.simulations.projectile import ProjectileMultioutput as Projectile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the Sin simulator just to get a feel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator = Sin()\n",
    "X = simulator.sample_inputs(4)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = simulator.forward_batch(X)\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = simulator.sample_inputs(1000)\n",
    "Y = simulator.forward_batch(X)\n",
    "plt.scatter(X, Y, marker='.', c='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Emulator\n",
    "\n",
    "To use an emulator in AutoEmulate, you just need to create an instance of an emulator class. The core operations are:\n",
    "\n",
    "1. **Training** – Fit the emulator on a batch of input-output pairs.\n",
    "2. **Prediction** – Generate output predictions (with uncertainty estimates) for new inputs.\n",
    "\n",
    "In this example, we use a simple **Gaussian Process** model (`GaussianProcessExact`) provided by the AutoEmulate package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a feel for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train emulator\n",
    "simulator = Sin()\n",
    "x_train = simulator.sample_inputs(25).sort(dim=0).values\n",
    "y_train = simulator.forward_batch(x_train)\n",
    "\n",
    "emulator = GaussianProcessExact(x_train, y_train)\n",
    "emulator.fit(x_train, y_train)\n",
    "\n",
    "# Test emulator\n",
    "X_test = simulator.sample_inputs(1000).sort(dim=0).values\n",
    "dist = emulator.predict(X_test)\n",
    "assert isinstance(dist, GaussianLike)\n",
    "Y_true = simulator.forward_batch(X_test)\n",
    "\n",
    "# Plot\n",
    "x_train, y_train, X_test, Y_mean, Y_cov, Y_true = (\n",
    "    x_train.flatten(), y_train.flatten(), X_test.flatten(), \n",
    "    dist.mean.detach().numpy().flatten(), torch.sqrt(dist.variance).detach().numpy().flatten(), Y_true.flatten()\n",
    ")\n",
    "plt.plot(X_test, Y_true, label='Simulator', alpha=0.5, c='k')\n",
    "plt.plot(X_test, Y_mean, label='Emulator')\n",
    "plt.fill_between(X_test, Y_mean - Y_cov, Y_mean + Y_cov, alpha=0.2, label='Confidence')\n",
    "plt.scatter(x_train, y_train, label='Training', marker='x')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows how well the emulator approximates the simulator, along with confidence intervals from the GP model. Note how the uncertainty grows in regions far from the training data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the simplest forms of stream-based active learning is to just sample a queried input point with some probability. Inevitably, this results in a query rate of that probability, where the query rate is the proportion of accepted input points and total input points encountered. We can instantiate and train this learner as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learner components\n",
    "simulator = Sin()\n",
    "\n",
    "x_train = simulator.sample_inputs(5)\n",
    "y_train = simulator.forward_batch(x_train)\n",
    "emulator = GaussianProcessExact(x_train, y_train)\n",
    "\n",
    "# Learner itself!\n",
    "learner = stream.Random(\n",
    "    simulator=simulator,\n",
    "    emulator=emulator,\n",
    "    x_train=x_train,\n",
    "    y_train=y_train,\n",
    "    p_query=0.2,\n",
    "    show_progress=show_progress,\n",
    ")\n",
    "\n",
    "# Stream of 500 samples\n",
    "X_stream = simulator.sample_inputs(500)\n",
    "learner.fit_samples(X_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.metrics.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several metrics commonly recorded when running an active learner, each providing insight into different aspects of its performance:\n",
    "\n",
    "1. **`mse` (Mean Squared Error)**: Measures the average squared difference between the emulator's predicted outputs and the simulator's actual outputs before updating (fitting) the emulator. A lower MSE indicates that the emulator predictions closely match the simulator.\n",
    "\n",
    "2. **`r2` (Coefficient of Determination)**:  \n",
    "   $$\n",
    "   R^2 = 1 - \\frac{\\sum (y_{\\text{true}} - y_{\\text{pred}})^2}{\\sum (y_{\\text{true}} - \\bar{y})^2}\n",
    "   $$  \n",
    "   Here, $y_{\\text{true}}$ denotes the simulator outputs, $y_{\\text{pred}}$ denotes the emulator's predictions, and $\\bar{y}$ is the average of the simulator's outputs.\n",
    "\n",
    "   - An $R^2$ score of **1** indicates a perfect fit—emulator predictions exactly match simulator outputs.\n",
    "   - An $R^2$ score of **0** means the emulator is performing no better than a naive model that predicts the mean simulator output.\n",
    "   - An $R^2$ score **less than 0** suggests the emulator predictions are worse than simply using the mean output.\n",
    "\n",
    "3. **`rate` (Query Rate)**: The ratio of simulator queries made to the total number of input points encountered in the stream. Lower rates indicate a more efficient learner, as fewer expensive simulator queries are needed relative to the number of inputs seen.\n",
    "\n",
    "4. **`n_queries` (Number of Queries)**: The cumulative count of how many times the simulator has been queried so far. Monitoring this number helps track the overall computational cost incurred by the learner.\n",
    "\n",
    "5. **`logdet` (Log Determinant of Covariance)**: Represents uncertainty in the emulator’s predictions, calculated as the log determinant of the emulator’s covariance matrix. A decreasing (more negative) log determinant typically indicates increasing confidence in predictions as the emulator improves with additional simulator data.\n",
    "\n",
    "6. **`trace` (Trace of Covariance)**: Measures the sum of variances along each dimension of the emulator's predictions, given by the trace of the covariance matrix. As the emulator learns, this metric should decrease toward zero, reflecting growing certainty.\n",
    "\n",
    "7. **`max_eigval` (Maximum Eigenvalue of Covariance)**: Indicates the largest single uncertainty direction of the emulator’s predictions. A decreasing maximum eigenvalue approaching zero implies increased confidence in the emulator’s predictions along all directions in the output space.\n",
    "\n",
    "\n",
    "Let's plot these!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in learner.metrics.items():\n",
    "    plt.plot(v, c='k', alpha=0.8)\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel(k)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Along with the above running statistics, we also have summary statistics available from the active learner object.\n",
    "- **`mse_per_query`:**  \n",
    "  This represents the mean squared error (MSE) per simulator query. A lower value indicates the emulator is closely matching the simulator’s output on average each time it's queried, suggesting good predictive performance.\n",
    "\n",
    "- **`r2_per_query`:**  \n",
    "  The coefficient of determination ($R^2$) per simulator query measures how well emulator predictions explain the variability in the simulator outputs. An $R^2$ value close to 0 suggests that emulator predictions are currently only slightly better than simply predicting the mean.\n",
    "\n",
    "- **`trace_per_query`:**  \n",
    "  This metric  is the trace of the emulator’s covariance matrix per query, reflecting the average uncertainty across the emulator’s predicted dimensions. A small value means that, on average, the emulator is very certain about its predictions at the time of querying.\n",
    "\n",
    "- **`logdet_per_query`:**  \n",
    "  The log determinant of the covariance per query quantifies the overall uncertainty volume in the emulator’s predictions. A negative and decreasing value typically indicates increased confidence and improved stability in emulator predictions.\n",
    "\n",
    "- **`max_eigval_per_query`:**  \n",
    "  The maximum eigenvalue of the covariance matrix per query indicates uncertainty along the dimension with the highest variance. A small value signifies that even in the direction of greatest uncertainty, predictions remain relatively confident.\n",
    "\n",
    "- **`auc_mse`:**  \n",
    "  The Area Under the Curve (AUC) of the cumulative mean squared error summarizes the emulator’s predictive error over the entire learning process. A lower AUC-MSE generally indicates better overall emulator performance across queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A full experiment with all stream-based Learners (advanced)\n",
    "\n",
    "AutoEmulate includes a variety of **stream-based active learners**, each using a different strategy to decide whether to query the simulator.\n",
    "\n",
    "### 1. `Random`\n",
    "\n",
    "- **Strategy**: Queries the simulator at random.\n",
    "- **Control**: Uses a fixed probability (`p_query`).\n",
    "- **Use case**: Serves as a baseline for comparison.\n",
    "\n",
    "\n",
    "### 2. Threshold-Based Learners\n",
    "\n",
    "These learners query the simulator only when a particular metric exceeds a fixed threshold.\n",
    "\n",
    "- **`Distance`**  \n",
    "  Based on distance in the input space. Queries when a new input is sufficiently far from the training data.\n",
    "\n",
    "- **`A_Optimal`**, **`D_Optimal`**, **`E_Optimal`**  \n",
    "  Use information-theoretic metrics derived from the emulator:\n",
    "  - **`A_Optimal`**: Minimizes the trace of the inverse Fisher information matrix.\n",
    "  - **`D_Optimal`**: Maximizes the determinant (volume) of the Fisher information.\n",
    "  - **`E_Optimal`**: Maximizes the smallest eigenvalue (worst-case direction).\n",
    "\n",
    "> These methods use a **fixed threshold**, which may become suboptimal as the emulator's uncertainty naturally decreases over time.\n",
    "\n",
    "\n",
    "### 3. Adaptive Threshold Learners (PID-Controlled)\n",
    "\n",
    "To maintain a steady query rate over time, **adaptive learners** use a **PID controller** to adjust the threshold dynamically.\n",
    "\n",
    "This turns active learning into a control problem where:\n",
    "\n",
    "- The **threshold** is the control input.\n",
    "- The **query rate** is the measured system output.\n",
    "- The **target query rate** is the setpoint.\n",
    "\n",
    "The PID controller updates the threshold as follows:\n",
    "$$\n",
    "u_k = K_p e_k + K_i \\sum_{j=0}^k e_j + K_d (e_k - e_{k-1}), \\quad \\text{where} \\quad e_k = \\text{rate}_k - \\text{rate}_\\text{target}\n",
    "$$\n",
    "\n",
    "#### Adaptive Variants:\n",
    "- **`Adaptive_Distance`**: Applies PID control to input-space diversity.\n",
    "- **`Adaptive_A_Optimal`**: PID-controlled version of A-optimal design.\n",
    "- **`Adaptive_D_Optimal`**: PID-controlled version of D-optimal design.\n",
    "- **`Adaptive_E_Optimal`**: PID-controlled version of E-optimal design.\n",
    "\n",
    "These learners maintain a desired number of simulator queries over time by continuously adapting their thresholds.\n",
    "\n",
    "\n",
    "This suite of learners allows you to balance between:\n",
    "- **Exploration** (via distance-based criteria),\n",
    "- **Exploitation** (via uncertainty-based optimal design),\n",
    "- And **control over query budget** (via adaptive strategies).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Learners\n",
    "\n",
    "To facilitate benchmarking, we define a `learners` function that returns a collection of stream-based active learning strategies. Each learner is initialized with:\n",
    "\n",
    "- A **simulator** (e.g. `Sin`, `Projectile`)\n",
    "- An **emulator** (a Gaussian Process in our case)\n",
    "- An **initial training dataset** of input-output pairs\n",
    "\n",
    "This function supports two modes:\n",
    "- If `adaptive_only=False`, both **fixed-threshold** and **adaptive (PID-controlled)** learners are included.\n",
    "- If `adaptive_only=True`, only the adaptive learners are returned.\n",
    "\n",
    "The learners cover:\n",
    "- Baseline: `Random`\n",
    "- Threshold-based: `Distance`, `A_Optimal`, `D_Optimal`, `E_Optimal`\n",
    "- Adaptive PID-controlled variants: `Adaptive_Distance`, `Adaptive_A_Optimal`, `Adaptive_D_Optimal`, `Adaptive_E_Optimal`\n",
    "\n",
    "This modular setup allows us to easily loop over multiple learner types in the experiment and compare their performance on the same stream of inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learners(*, simulator: Simulator, n_initial_samples: int, adaptive_only: bool) -> Iterable:\n",
    "    x_train = simulator.sample_inputs(n_initial_samples)\n",
    "    y_train = simulator.forward_batch(x_train)\n",
    "    yield stream.Random(\n",
    "        simulator=simulator, emulator=GaussianProcessExact(x_train, y_train),\n",
    "        x_train=x_train, y_train=y_train,\n",
    "        p_query=0.25, show_progress=show_progress\n",
    "    )\n",
    "    if not adaptive_only:\n",
    "        yield stream.Distance(\n",
    "            simulator=simulator, emulator=GaussianProcessExact(x_train, y_train),\n",
    "            x_train=x_train, y_train=y_train,\n",
    "            threshold=0.5, show_progress=show_progress\n",
    "        )\n",
    "        yield stream.A_Optimal(\n",
    "            simulator=simulator, emulator=GaussianProcessExact(x_train, y_train),\n",
    "            x_train=x_train, y_train=y_train,\n",
    "            threshold=1.0, show_progress=show_progress\n",
    "        )\n",
    "        yield stream.D_Optimal(\n",
    "            simulator=simulator, emulator=GaussianProcessExact(x_train, y_train),\n",
    "            x_train=x_train, y_train=y_train,\n",
    "            threshold=-4.2, show_progress=show_progress\n",
    "        )\n",
    "        yield stream.E_Optimal(\n",
    "            simulator=simulator, emulator=GaussianProcessExact(x_train, y_train),\n",
    "            x_train=x_train, y_train=y_train,\n",
    "            threshold=1.0, show_progress=show_progress\n",
    "        )\n",
    "    yield stream.Adaptive_Distance(\n",
    "        simulator=simulator, emulator=GaussianProcessExact(x_train, y_train),\n",
    "        x_train=x_train, y_train=y_train,\n",
    "        threshold=0.5, Kp=1.0, Ki=1.0, Kd=1.0,\n",
    "        key=\"rate\", target=0.25,\n",
    "        min_threshold=0.0, # if isinstance(simulator, Sin) else None, \n",
    "        max_threshold=2.0 if isinstance(simulator, Sin) else None,\n",
    "        window_size=10, show_progress=show_progress\n",
    "    )\n",
    "    yield stream.Adaptive_A_Optimal(\n",
    "        simulator=simulator, emulator=GaussianProcessExact(x_train, y_train),\n",
    "        x_train=x_train, y_train=y_train,\n",
    "        threshold=1e-1, Kp=2.0, Ki=1.0, Kd=2.0,\n",
    "        key=\"rate\", target=0.25,\n",
    "        min_threshold=0.0, # if isinstance(simulator, Sin) else None, \n",
    "        max_threshold=1.0 if isinstance(simulator, Sin) else None,\n",
    "        window_size=10, show_progress=show_progress\n",
    "    )\n",
    "    yield stream.Adaptive_D_Optimal(\n",
    "        simulator=simulator, emulator=GaussianProcessExact(x_train, y_train),\n",
    "        x_train=x_train, y_train=y_train,\n",
    "        threshold=-4.0, Kp=2.0, Ki=1.0, Kd=2.0,\n",
    "        key=\"rate\", target=0.25,\n",
    "        min_threshold=None,\n",
    "        max_threshold=0 if isinstance(simulator, Sin) else None,\n",
    "        window_size=10, show_progress=show_progress\n",
    "    )\n",
    "    yield stream.Adaptive_E_Optimal(\n",
    "        simulator=simulator, emulator=GaussianProcessExact(x_train, y_train),\n",
    "        x_train=x_train, y_train=y_train,\n",
    "        threshold=0.75 if isinstance(simulator, Sin) else 1000, \n",
    "        Kp=2.0, Ki=1.0, Kd=2.0,\n",
    "        key=\"rate\", target=0.25,\n",
    "        min_threshold=0.0, # if isinstance(simulator, Sin) else None, \n",
    "        max_threshold=1.0 if isinstance(simulator, Sin) else None,\n",
    "        window_size=10, show_progress=show_progress\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Experiment\n",
    "\n",
    "The `run_experiment` function evaluates each learner on a fixed stream of input points, over multiple random seeds. For each trial:\n",
    "\n",
    "1. A stream of inputs is sampled from the simulator.\n",
    "2. Each learner is initialized with the same initial dataset.\n",
    "3. The learner processes the stream (either in batches or one sample at a time).\n",
    "4. Key performance metrics and summary statistics are recorded.\n",
    "\n",
    "This setup supports toggling:\n",
    "- Number of **initial training samples**\n",
    "- Number of **streamed inputs**\n",
    "- Whether to include only **adaptive learners**\n",
    "- Optional **batch size** for processing\n",
    "\n",
    "The results include:\n",
    "- `metrics`: detailed time-series metrics for each learner\n",
    "- `summary`: overall performance summaries for final comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(\n",
    "    *,\n",
    "    simulator: Simulator,\n",
    "    seeds: List[int],\n",
    "    n_initial_samples: int,\n",
    "    n_stream_samples: int,\n",
    "    adaptive_only: bool,\n",
    "    batch_size: int | None = None\n",
    ") -> Tuple[List[Dict], List[Dict]]:\n",
    "    metrics, summary = list(), list()\n",
    "    for seed in seeds:\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        X_stream = simulator.sample_inputs(n_stream_samples)\n",
    "        if show_progress:\n",
    "            tqdm.write(f\"Trial with seed {seed}\")\n",
    "        for learner in learners(\n",
    "            simulator=simulator, \n",
    "            n_initial_samples=n_initial_samples, \n",
    "            adaptive_only=adaptive_only\n",
    "        ):\n",
    "            if batch_size is not None:\n",
    "                learner.fit_batches(X_stream, batch_size)\n",
    "            else:\n",
    "                learner.fit_samples(X_stream)\n",
    "            metrics.append(dict(\n",
    "                name=learner.__class__.__name__,\n",
    "                **learner.metrics\n",
    "            ))\n",
    "            summary.append(dict(\n",
    "                name=learner.__class__.__name__,\n",
    "                **learner.summary\n",
    "            ))\n",
    "    return metrics, summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarizing Results\n",
    "\n",
    "After all learners have been evaluated, we use the `compute_statistics` function to produce a compact summary of performance across trials.\n",
    "\n",
    "```python\n",
    "compute_statistics(summary: List[Dict]) -> pd.DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_statistics(summary: List[Dict]) -> pd.DataFrame:\n",
    "    df = pd.DataFrame(summary).groupby('name').agg(['mean', 'std'])\n",
    "    df = df.sort_values(('mse_per_query', 'mean'), ascending=True).round(6)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Performance\n",
    "\n",
    "To better understand how learners perform over time, we use the `plot_metrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(metrics: List[Dict], smoothing_window=10):\n",
    "\n",
    "    # Compute the mean and variance curves for each class\n",
    "    def mean(s): return np.vstack(s).mean(axis=0).tolist()\n",
    "    def std(s):  return np.vstack(s).std(axis=0).tolist()\n",
    "    df = pd.DataFrame(metrics).groupby(\"name\").agg([mean, std])\n",
    "\n",
    "    # Smoothing to help visualisation\n",
    "    def moving_average(data, window):\n",
    "        pad_width = window // 2\n",
    "        padded_data = np.pad(data, (pad_width, window - pad_width - 1), mode='edge')\n",
    "        return np.convolve(padded_data, np.ones(window)/window, mode='valid')\n",
    "    \n",
    "    # Plot each metric\n",
    "    for metric in df.columns.get_level_values(0).unique():\n",
    "        for learner in df.index:\n",
    "            mean_values = moving_average(np.array(df.loc[learner, (metric, \"mean\")]), window=smoothing_window)\n",
    "            std_values  = moving_average(np.array(df.loc[learner, (metric, \"std\")]), window=smoothing_window)\n",
    "            iterations = np.arange(len(mean_values))\n",
    "            plt.plot(iterations, mean_values, label=learner if len(mean_values) != 1 else None)\n",
    "            plt.fill_between(iterations, mean_values - std_values, mean_values + std_values, alpha=0.2)\n",
    "        if metric == 'r2':\n",
    "            plt.ylim(-1, 1)\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel(metric)\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Active learning with the Sinusoid simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics, summary = run_experiment(\n",
    "    simulator=Sin(),\n",
    "    seeds=[0, 1],\n",
    "    n_initial_samples=5,\n",
    "    n_stream_samples=500,\n",
    "    adaptive_only=True,\n",
    "    batch_size=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_statistics(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see:\n",
    "1. The query rate approaches 0.25, as expected.\n",
    "2. The MSE approaches 0.\n",
    "3. The R2 score approache 1.\n",
    "4. The covariance matrix max eigenvalue and trace appraoch 0, and the log determinant approaches around -4.2.\n",
    "5. The number of queries and PID gains stabalise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Active learning with the Projectile simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we do the same thing, but for the more difficult projectile example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics, summary = run_experiment(\n",
    "    simulator=Projectile(),\n",
    "    seeds=[0, 1],\n",
    "    n_initial_samples=5,\n",
    "    n_stream_samples=500,\n",
    "    adaptive_only=True,\n",
    "    batch_size=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_statistics(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
