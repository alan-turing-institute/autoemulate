{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "A classic way of obtaining uncertainty quantification in regression tasks is ensembles – where we quantify uncertainty by looking at the distribution over outputs from an ensemble of emulators.\n",
    "\n",
    "In AutoEmulate, we try to be as flexible as possible and allow for any combination of emulators with different outputs. To do so, we assume, without loss of generality, that each emulator in the ensemble returns a Gaussian-like output\n",
    "$$\n",
    "\\mu_i \\in \\mathbb{R}^d, \\quad \\Sigma_i \\in \\mathbb{R}^{d \\times d}\n",
    "$$\n",
    "where if the emulator is determinstic we simply have $\\Sigma_i = 0$.\n",
    "\n",
    "With this, we can compute an ensemble mean and covariance\n",
    "$$\n",
    "\\mu_{\\mathrm{ens}}\n",
    "= \\frac{1}{M}\n",
    "  \\sum_{i=1}^{M}\n",
    "    \\mu_i,\n",
    "$$\n",
    "$$\n",
    "\\Sigma_{\\mathrm{ens}}\n",
    "= \\frac{1}{M}\n",
    "  \\sum_{i=1}^{M}\n",
    "    \\Sigma_i\n",
    "+\n",
    "  \\frac{1}{M - 1}\n",
    "  \\sum_{i=1}^{M}\n",
    "    \\bigl(\\mu_i - \\mu_{\\mathrm{ens}}\\bigr)\n",
    "    \\bigl(\\mu_i - \\mu_{\\mathrm{ens}}\\bigr)^{T}.\n",
    "$$\n",
    "\n",
    "We can then make an ensemble with, e.g., a set of deterministic NNs, a mixed set of NNs and GPs, only GPs, or even a set of Emulators – anything that abides the the Gaussian return type above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, matplotlib.pyplot as plt\n",
    "from autoemulate.emulators.nn.mlp import MLP \n",
    "from autoemulate.emulators.ensemble import Ensemble\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "Let's start with some simple 1D data\n",
    "$$\n",
    "x \\in \\mathbb{R}^1, \\quad y = \\sin(x) \\in \\mathbb{R}^1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "x_train = torch.rand(100, 1) * 10\n",
    "x_train = x_train[x_train.flatten().argsort(), :]\n",
    "y_train = torch.sin(x_train)\n",
    "plt.plot(x_train.flatten(), y_train.flatten(), '.', label='Training Data', alpha=0.5)\n",
    "\n",
    "# Testing\n",
    "x_test = torch.linspace(0.0, 15.0, steps=1000).reshape(-1, 1)\n",
    "y_test = torch.sin(x_test)\n",
    "plt.plot(x_test.flatten(), y_test.flatten(), '-', label='Testing Data', alpha=0.5)\n",
    "\n",
    "# Plot\n",
    "plt.xlabel('Input: x')\n",
    "plt.ylabel('Output: y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "Let's make a simple deterministic MLP emulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "emulator = MLP(x_train, y_train, layer_dims=[100, 100], lr=1e-2)\n",
    "emulator.epochs = 1000\n",
    "emulator.fit(x_train, y_train)\n",
    "y_test_hat = emulator.predict(x_test)\n",
    "plt.plot(x_test.flatten(), y_test_hat.flatten(), alpha=0.5, label='NN Output')\n",
    "plt.plot(x_train.flatten(), y_train.flatten(), '.', alpha=0.5, label='Training Data')\n",
    "plt.ylim(-1.2, 1.2)\n",
    "plt.xlabel('Input: x')\n",
    "plt.ylabel('Output: y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "As we see, the NN emulator output is pretty good over the areas where there is training data, but definitely off beyond that. We can only say this because we know what the output should look like beyond the training data. But, what if we did not know what it should look like – then it would be helpful if the NN emulator could provide some measure of uncertainty. This is exactly where ensembles can come in!\n",
    "\n",
    "Let's define several NN emulators initilised with different seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set of emulators\n",
    "emulators = []\n",
    "for i in range(4):\n",
    "    torch.manual_seed(i)\n",
    "    emulator = MLP(x_train, y_train, layer_dims=[100, 100], lr=1e-2)\n",
    "    emulator.epochs = 1000\n",
    "    emulators.append(emulator)\n",
    "\n",
    "# Ensemble of emulators\n",
    "emulator = Ensemble(emulators)\n",
    "emulator.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "y_test_hat = emulator.predict(x_test)\n",
    "y_test_hat_mean = y_test_hat.mean # (1000, 1)\n",
    "y_test_hat_var = y_test_hat.covariance_matrix # (1000, 1, 1)\n",
    "y_test_hat_std = torch.sqrt(y_test_hat_var.diagonal(dim1=-2, dim2=-1)) # (1000, 1)\n",
    "plt.plot(x_test.flatten(), y_test_hat_mean.flatten(), alpha=0.5, label='Mean')\n",
    "plt.fill_between(\n",
    "    x_test.flatten(), \n",
    "    (y_test_hat_mean - y_test_hat_std).flatten(),\n",
    "    (y_test_hat_mean + y_test_hat_std).flatten(),\n",
    "    alpha=0.25,\n",
    "    label='Standard Deviation'\n",
    ")\n",
    "plt.plot(x_train.flatten(), y_train.flatten(), '.', alpha=0.5, label='Training Data')\n",
    "plt.legend()\n",
    "plt.xlabel('Input: x')\n",
    "plt.ylabel('Output: y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "We can even try making an ensemble of ensembles – any arbitrary combination goes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 ensembles, each with 4 emulators\n",
    "emulators = []\n",
    "for i in range(2):\n",
    "    subemulators = []\n",
    "    for j in range(4):\n",
    "        torch.manual_seed(i + j)\n",
    "        emulator = MLP(x_train, y_train, layer_dims=[100, 100], lr=1e-2)\n",
    "        emulator.epochs = 1000\n",
    "        subemulators.append(emulator)\n",
    "    emulator = Ensemble(subemulators)\n",
    "    emulators.append(emulator)\n",
    "\n",
    "# Train\n",
    "emulator = Ensemble(emulators)\n",
    "emulator.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "y_test_hat = emulator.predict(x_test)\n",
    "y_test_hat_mean = y_test_hat.loc # (1000, 1)\n",
    "y_test_hat_var = y_test_hat.covariance_matrix # (1000, 1, 1)\n",
    "y_test_hat_std = torch.sqrt(y_test_hat_var.diagonal(dim1=-2, dim2=-1)) # (1000, 1)\n",
    "plt.plot(x_test.flatten(), y_test_hat_mean.flatten(), alpha=0.5, label='Mean')\n",
    "plt.fill_between(\n",
    "    x_test.flatten(), \n",
    "    (y_test_hat_mean - y_test_hat_std).flatten(),\n",
    "    (y_test_hat_mean + y_test_hat_std).flatten(),\n",
    "    alpha=0.25,\n",
    "    label='Standard Deviation'\n",
    ")\n",
    "plt.plot(x_train.flatten(), y_train.flatten(), '.', alpha=0.5, label='Training Data')\n",
    "plt.legend()\n",
    "plt.xlabel('Input: x')\n",
    "plt.ylabel('Output: y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Can we do active learning with this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "In active learning, we typically use the emulator's measure of uncertainty as a indication of where we should acquire more data. Lukcily, although the underyling emulators may not provide uncertainty quantification directly, building an ensemble out of them does.\n",
    "\n",
    "Below we give an example of stream-based active learning, where we are sequentially presented inputs, and we must decide whether or not we want to label them with an output (i.e. add them to the training set). This is important because obtaining an output label may require an expensive run of a simulator. We can drastically reduce the computational cost of building a good emulator – see below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoemulate.learners.stream import Adaptive_A_Optimal\n",
    "from autoemulate.learners.base import Simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sin(Simulator):\n",
    "    def __init__(self, param_ranges={\"x\": (0., 25.)}, output_names = [\"y\"]):\n",
    "        super().__init__(param_ranges, output_names)\n",
    "    def _forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.sin(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "emulators = []\n",
    "for i in range(4):\n",
    "    torch.manual_seed(i)\n",
    "    emulator = MLP(x_train, y_train, layer_dims=[100, 100], lr=1e-3, weight_init='xavier_normal')\n",
    "    emulator.epochs = 20\n",
    "    emulators.append(emulator)\n",
    "emulator = Ensemble(emulators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator=Sin()\n",
    "learner = Adaptive_A_Optimal(\n",
    "    simulator=simulator, emulator=emulator,\n",
    "    X_train=x_train, Y_train=y_train,\n",
    "    threshold=1e-1, Kp=2.0, Ki=1.0, Kd=1.0,\n",
    "    key=\"rate\", target=0.15,\n",
    "    min_threshold=0.0,\n",
    "    max_threshold=0.5,\n",
    "    window_size=10, show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_stream = simulator.sample_inputs(2000, 0)\n",
    "learner.fit_samples(x_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "As we see, the MSE asympotitcally approaches zero and then $R^2$ asympotically approaches 1 – all while the active learner tries to maintain a target query rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(learner.metrics['mse'], alpha=0.75, label='Steam MSE')\n",
    "plt.plot(learner.metrics['r2'], alpha=0.75, label='Stream R2')\n",
    "plt.plot(learner.metrics['rate'], alpha=0.75, label='Query Rate')\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel('Stream Iteration')\n",
    "plt.ylabel('Metric')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "x_test = torch.linspace(0.0, 25.0, steps=1000).reshape(-1, 1)\n",
    "y_test_hat = learner.emulator.predict(x_test)\n",
    "y_test_hat_mean = y_test_hat.loc # (1000, 1)\n",
    "y_test_hat_var = y_test_hat.covariance_matrix # (1000, 1, 1)\n",
    "y_test_hat_std = torch.sqrt(y_test_hat_var.diagonal(dim1=-2, dim2=-1)) # (1000, 1)\n",
    "plt.plot(x_test.flatten(), y_test_hat_mean.flatten(), alpha=0.5, label='Mean')\n",
    "plt.fill_between(\n",
    "    x_test.flatten(), \n",
    "    (y_test_hat_mean - y_test_hat_std).flatten(),\n",
    "    (y_test_hat_mean + y_test_hat_std).flatten(),\n",
    "    alpha=0.25,\n",
    "    label='Standard Deviation'\n",
    ")\n",
    "plt.plot(learner.X_train.flatten(), learner.Y_train.flatten(), '.', alpha=0.5, label='Query Data')\n",
    "plt.legend()\n",
    "plt.xlabel('Input: x')\n",
    "plt.ylabel('Output: y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_stream = x_stream.shape[0]\n",
    "n_query = learner.X_train.shape[0]\n",
    "p_query = n_query / n_stream\n",
    "print(f'We only needed to label {p_query*100:.2f}% of the streaming data!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Dropout Ensembles\n",
    "Where we get uncertainty quantification by running an NN multiple times with dropout turned on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoemulate.emulators.ensemble import DropoutEnsemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "emulator = MLP(x_train, y_train, layer_dims=[100, 100], lr=1e-2, dropout_prob=0.2)\n",
    "emulator.epochs = 1000\n",
    "emulator = DropoutEnsemble(emulator, n_samples=10)\n",
    "emulator.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "y_test_hat = emulator.predict(x_test)\n",
    "y_test_hat_mean = y_test_hat.loc # (1000, 1)\n",
    "y_test_hat_var = y_test_hat.covariance_matrix # (1000, 1, 1)\n",
    "y_test_hat_std = torch.sqrt(y_test_hat_var.diagonal(dim1=-2, dim2=-1)) # (1000, 1)\n",
    "plt.plot(x_test.flatten(), y_test_hat_mean.flatten(), alpha=0.5, label='Mean')\n",
    "plt.fill_between(\n",
    "    x_test.flatten(), \n",
    "    (y_test_hat_mean - y_test_hat_std).flatten(),\n",
    "    (y_test_hat_mean + y_test_hat_std).flatten(),\n",
    "    alpha=0.25,\n",
    "    label='Standard Deviation'\n",
    ")\n",
    "plt.plot(x_train.flatten(), y_train.flatten(), '.', alpha=0.5, label='Training Data')\n",
    "plt.legend()\n",
    "plt.xlabel('Input: x')\n",
    "plt.ylabel('Output: y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "emulator = MLP(x_train, y_train, layer_dims=[100, 100], lr=1e-3, dropout_prob=0.10)\n",
    "emulator.epochs = 10\n",
    "emulator = DropoutEnsemble(emulator, n_samples=20, jitter=1e-7)\n",
    "simulator=Sin()\n",
    "learner = Adaptive_A_Optimal(\n",
    "    simulator=simulator, emulator=emulator,\n",
    "    X_train=x_train, Y_train=y_train,\n",
    "    threshold=1e-1, Kp=2.0, Ki=1.0, Kd=1.0,\n",
    "    key=\"rate\", target=0.25,\n",
    "    min_threshold=0.0,\n",
    "    max_threshold=0.5,\n",
    "    window_size=10, show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_stream = simulator.sample_inputs(2000, 0)\n",
    "learner.fit_samples(x_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(learner.metrics['mse'], alpha=0.75, label='Steam MSE')\n",
    "plt.plot(learner.metrics['r2'], alpha=0.75, label='Stream R2')\n",
    "plt.plot(learner.metrics['rate'], alpha=0.75, label='Query Rate')\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel('Stream Iteration')\n",
    "plt.ylabel('Metric')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "x_test = torch.linspace(0.0, 25.0, steps=1000).reshape(-1, 1)\n",
    "y_test_hat = learner.emulator.predict(x_test)\n",
    "y_test_hat_mean = y_test_hat.loc # (1000, 1)\n",
    "y_test_hat_var = y_test_hat.covariance_matrix # (1000, 1, 1)\n",
    "y_test_hat_std = torch.sqrt(y_test_hat_var.diagonal(dim1=-2, dim2=-1)) # (1000, 1)\n",
    "plt.plot(x_test.flatten(), y_test_hat_mean.flatten(), alpha=0.5, label='Mean')\n",
    "plt.fill_between(\n",
    "    x_test.flatten(), \n",
    "    (y_test_hat_mean - y_test_hat_std).flatten(),\n",
    "    (y_test_hat_mean + y_test_hat_std).flatten(),\n",
    "    alpha=0.25,\n",
    "    label='Standard Deviation'\n",
    ")\n",
    "plt.plot(learner.X_train.flatten(), learner.Y_train.flatten(), '.', alpha=0.5, label='Query Data')\n",
    "plt.legend()\n",
    "plt.xlabel('Input: x')\n",
    "plt.ylabel('Output: y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_stream = x_stream.shape[0]\n",
    "n_query = learner.X_train.shape[0]\n",
    "p_query = n_query / n_stream\n",
    "print(f'We only needed to label {p_query*100:.2f}% of the streaming data!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
