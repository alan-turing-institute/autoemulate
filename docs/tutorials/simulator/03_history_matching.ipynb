{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoemulate.emulators import GaussianProcess\n",
    "from autoemulate.core.compare import AutoEmulate\n",
    "from autoemulate.simulations.epidemic import Epidemic\n",
    "from autoemulate.calibration.history_matching import HistoryMatchingWorkflow\n",
    "from autoemulate.calibration.history_matching_dashboard import HistoryMatchingDashboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# History Matching workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `HistoryMatching` calibration method can be a useful way to iteratively decide which simulations to run to generate data refine the emulator on. The `HistoryMatchingWorkflow` implements this iterative sample-predict-refit workflow. Each time it is run:\n",
    "- parameters are sampled from the not ruled out yet (NROY) space\n",
    "- an emulator is used in combination with `HistoryMatching` to score the implausability of the samples\n",
    "- simulations are run for a subset of the NROY samples\n",
    "- the emulator is refit given the newly simulated data\n",
    "\n",
    "\n",
    "In this tutorial, we demonstrate how to implement this simulator in the loop workflow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simulate data and train an emulator\n",
    "\n",
    "In this example, we'll use the `Epidemic` simulator, which returns the peak infection rate given two input parameters, `beta`(the transimission rate per day) and `gamma` (the recovery rate per day)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator = Epidemic(log_level=\"error\")\n",
    "x = simulator.sample_inputs(500, random_seed=random_seed)\n",
    "y = simulator.forward_batch(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this tutorial, we will restrict the model choice to `GaussianProcess`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae = AutoEmulate(\n",
    "    x,\n",
    "    y,\n",
    "    models=[GaussianProcess],\n",
    "    model_tuning=False,\n",
    "    log_level=\"error\",\n",
    "    random_seed=random_seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that the fitted emulator performs well on both the train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae.summarise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result = ae.best_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Calibrate\n",
    "\n",
    "To instantiate the `HistoryMatchingWorkflow` object, we need an observed mean and, optionally, variance for each simulator output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = {\"infection_rate\": (0.3, 0.05)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also pass the fitted emulator and the simulator to the `HistoryMatchingWorkflow` along with the observations. \n",
    "\n",
    "The user can also set the usually history matching parameters (e.g., implausability threshold). Optionally, the user can also pass the training data used to fit the emulator, which can then be used in refitting the emulator in combination with newly simulated data during the history matching workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmw = HistoryMatchingWorkflow(\n",
    "    simulator=simulator,\n",
    "    result=best_result,\n",
    "    observations=observations,\n",
    "    threshold=3.0,\n",
    "    random_seed=random_seed\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `run` method implements the iterative sample-predict-refit workflow:\n",
    "- sample `n_test_samples` to test from the not ruled out yet (NROY) space\n",
    "- use emulator to filter out implausible samples and update the NROY space\n",
    "- run `n_simulations` predictions for the sampled parameters using the simulator\n",
    "\n",
    "Optionally, the `run` method can also `refit_the_emulator` using only the newly simulated data or all the data available so far (see `refit_on_all_data` parameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_parameters, impl_scores = hmw.run(n_simulations=20, n_test_samples=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `HistoryMatchingWorkflow` object maintains and updates the internal state each time `run()` is called, including saving the newly simulated data. This means the full workflow can be run over a number of iterations by calling `run` repeatedly.\n",
    "\n",
    "Alternatively, you can use the `run_waves` method to run multiple waves of the history matching workflow in one go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualise results\n",
    "\n",
    "For visualising results, you can use the `HistoryMatchingDashboard` which implements a number of interactive plots. To initialize it just pass in outputs of the `run()` method and information about the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dashboard = HistoryMatchingDashboard(\n",
    "    samples=test_parameters,\n",
    "    impl_scores=impl_scores,\n",
    "    param_names=simulator.param_names,  \n",
    "    output_names=simulator.output_names, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots can be viewed using the `display()` method. Below you can see an example image of what the dashboard looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dashboard.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/alan-turing-institute/autoemulate/refs/heads/main/misc/vis_dashboard_pic_sample.png\" alt=\"Work Flow\" style=\"width:50%;\"/> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autoemulate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
