{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# probabilistic programming\n",
    "import pyro \n",
    "\n",
    "# MCMC plotting\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "from getdist.arviz_wrapper import arviz_to_mcsamples\n",
    "from getdist import plots\n",
    "\n",
    "# autoemulate imports\n",
    "from autoemulate.simulations.epidemic import Epidemic\n",
    "from autoemulate.core.compare import AutoEmulate\n",
    "from autoemulate.calibration.bayes import BayesianCalibration\n",
    "from autoemulate.emulators import GaussianProcess\n",
    "\n",
    "# suppress warnings in notebook for readability\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "\n",
    "# random seed for reproducibility\n",
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoemulate.data.utils import set_random_seed\n",
    "set_random_seed(random_seed)\n",
    "pyro.set_rng_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator = Epidemic(log_level=\"error\")\n",
    "x = simulator.sample_inputs(1000)\n",
    "y, _ = simulator.forward_batch(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transmission_rate = x[:, 0]\n",
    "recovery_rate = x[:, 1]\n",
    "\n",
    "plt.scatter(transmission_rate, recovery_rate, c=y, cmap='viridis')\n",
    "plt.xlabel('Transmission rate (beta)')\n",
    "plt.ylabel('Recovery rate (gamma)')\n",
    "plt.colorbar(label=\"Peak infection rate\")\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_beta = 0.3\n",
    "true_gamma = 0.15 \n",
    "\n",
    "# simulator expects inputs of shape [1, number of inputs]\n",
    "params = torch.tensor([true_beta, true_gamma]).view(1, -1)\n",
    "true_infection_rate = simulator.forward(params)\n",
    "assert isinstance(true_infection_rate, torch.Tensor)\n",
    "\n",
    "n_obs = 100\n",
    "stdev = 0.05\n",
    "noise = torch.normal(mean=0, std=stdev, size=(n_obs,))\n",
    "observed_infection_rates = true_infection_rate[0] + noise\n",
    "\n",
    "observations = {\"infection_rate\": observed_infection_rates}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run AutoEmulate to find the best GP model\n",
    "from autoemulate.emulators.gaussian_process.exact import GaussianProcessRBF\n",
    "\n",
    "\n",
    "ae = AutoEmulate(\n",
    "    x, \n",
    "    y, \n",
    "    models=[GaussianProcessRBF],\n",
    "    model_params={},\n",
    "    log_level=\"error\", \n",
    ")\n",
    "\n",
    "gp = ae.best_result().model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem set-up: identify an interval excursion set for $f(x)$\n",
    "\n",
    "The aim for the remainder of this notebook is to explore methods that are able to identify samples $x$ from the interval excursion set.\n",
    "\n",
    "Mathematically this is:\n",
    "$$\n",
    "x \\in \\mathbb{R}^n, \\quad a, b \\in \\mathbb{R}^m \\quad f: \\mathbb{R}^n \\mapsto \\mathbb{R}^m\\quad a < f(x) < b\n",
    "$$\n",
    "\n",
    "Solving this problem is more general than calculating:\n",
    "- the level set ($f(x) = c$)\n",
    "- superlevel set ($f(x) > c$)\n",
    "- sublevel set ($f(x) < c$)\n",
    "Howver, each can be formulated such that samples returned can approximate each of these types of level set for crafted values of $a, b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.distributions import constraints, Transform\n",
    "from torch.distributions.transforms import AffineTransform, SigmoidTransform\n",
    "\n",
    "class BoundedDomainTransform(Transform):\n",
    "    domain = constraints.real\n",
    "    codomain = constraints.interval(0.0, 1.0)  # will be rescaled\n",
    "    bijective = True\n",
    "\n",
    "    def __init__(self, domain_min, domain_max):\n",
    "        super().__init__()\n",
    "        self.domain_min = torch.as_tensor(domain_min)\n",
    "        self.domain_max = torch.as_tensor(domain_max)\n",
    "        self._sigmoid = SigmoidTransform()\n",
    "        self._affine = AffineTransform(\n",
    "            self.domain_min,\n",
    "            (self.domain_max - self.domain_min)\n",
    "        )\n",
    "\n",
    "    def _call(self, x):\n",
    "        u = self._sigmoid(x)\n",
    "        return self._affine(u)\n",
    "\n",
    "    def _inverse(self, y):\n",
    "        u = (y - self.domain_min) / (self.domain_max - self.domain_min)\n",
    "        return self._sigmoid.inv(u)\n",
    "\n",
    "    def log_abs_det_jacobian(self, x, y):\n",
    "        u = self._sigmoid(x)\n",
    "        return (\n",
    "            self._sigmoid.log_abs_det_jacobian(x, u)\n",
    "            + self._affine.log_abs_det_jacobian(u, y)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.special import ndtr\n",
    "from pyro.distributions import Normal, TransformedDistribution # type: ignore\n",
    "\n",
    "from autoemulate.core.types import TensorLike\n",
    "\n",
    "\n",
    "# Problem settings\n",
    "domain_min = torch.tensor([b[0] for b in simulator.param_bounds])\n",
    "domain_max = torch.tensor([b[1] for b in simulator.param_bounds])\n",
    "\n",
    "# y band settings\n",
    "y_band_low = torch.tensor([0.5])   # lower bound(s) per task\n",
    "y_band_high = torch.tensor([0.6])  # upper bound(s) per task\n",
    "d = len(simulator.param_bounds)\n",
    "\n",
    "MIN_VAR = 1e-12\n",
    "\n",
    "@torch.no_grad()\n",
    "def band_prob_from_mu_sigma(mu: torch.Tensor, var_or_cov: torch.Tensor, y1: torch.Tensor, y2: torch.Tensor, aggregate: str = \"geomean\"):\n",
    "    \"\"\"\n",
    "    Compute per-sample band probability across tasks given GP mean and variance/covariance.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mu : (N, T) or (T,)\n",
    "    var_or_cov : (N, T) variance per task OR (N, T, T) covariance across tasks\n",
    "    y1, y2 : (T,) lower/upper bounds per task\n",
    "    aggregate : 'geomean' | 'sumlog' | 'none'\n",
    "        - 'geomean': returns geometric mean across tasks (shape N,)\n",
    "        - 'sumlog': returns sum of log-probs across tasks (shape N,)\n",
    "        - 'none': returns per-task probabilities (shape N, T)\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - This helper derives per-task standard deviations as sqrt(diag(cov)) when a full covariance\n",
    "      is provided, or sqrt(variance) when per-task variances are provided.\n",
    "    \"\"\"\n",
    "    if mu.dim() == 1:\n",
    "        mu = mu.unsqueeze(0)\n",
    "\n",
    "    # Derive per-task std devs from variance / covariance\n",
    "    if var_or_cov.dim() == 3:\n",
    "        var_diag = torch.diagonal(var_or_cov, dim1=-2, dim2=-1).clamp_min(MIN_VAR)\n",
    "        sigma = var_diag.sqrt()\n",
    "    else:\n",
    "        # (N, T) variance or (T,) variance for single sample\n",
    "        if var_or_cov.dim() == 1:\n",
    "            var_or_cov = var_or_cov.unsqueeze(0)\n",
    "        sigma = var_or_cov.clamp_min(MIN_VAR).sqrt()\n",
    "\n",
    "    # Broadcast bounds to (N, T)\n",
    "    y1v = y1.view(1, -1)\n",
    "    y2v = y2.view(1, -1)\n",
    "\n",
    "    # Stable CDF differences\n",
    "    a = ((y1v - mu) / sigma).clamp(-30.0, 30.0)\n",
    "    b = ((y2v - mu) / sigma).clamp(-30.0, 30.0)\n",
    "    p_task = (ndtr(b.double()) - ndtr(a.double())).clamp_min(1e-12).to(mu.dtype)\n",
    "\n",
    "    if aggregate == \"none\":\n",
    "        return p_task\n",
    "    log_p = p_task.log()\n",
    "    if aggregate == \"geomean\":\n",
    "        return log_p.mean(dim=-1).exp()\n",
    "    if aggregate == \"sumlog\":\n",
    "        return log_p.sum(dim=-1)\n",
    "    raise ValueError(\"aggregate must be one of {'geomean', 'sumlog', 'none'}\")\n",
    "\n",
    "\n",
    "def band_logprob(mu: TensorLike, var: TensorLike, y1: TensorLike, y2: TensorLike, temp=1.0, softness: float | None=None, mix=1.0):\n",
    "    \"\"\"\n",
    "    Multi-task band log-probability with optional soft surrogate and mixing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mu: TensorLike\n",
    "        Predicted mean. Shape (N, T) or (T,)\n",
    "    var: TensorLike\n",
    "        Predicted variance per task (N, T) or covariance (N, T, T)\n",
    "    y1: TensorLike\n",
    "        lower bounds (T,)\n",
    "    y2: TensorLike\n",
    "        upper bounds (T,)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    TensorLike\n",
    "        Scalar per sample (sums over tasks) and then summed over batch here only\n",
    "        when inputs are 1-sample; callers can keep per-sample if needed by removing .sum().\n",
    "    \"\"\"\n",
    "    # Ensure batch: (N, T)\n",
    "    if mu.dim() == 1:\n",
    "        mu = mu.unsqueeze(0)\n",
    "\n",
    "    # Broadcast bounds to (N, T) for the soft surrogate below\n",
    "    y1v = y1.view(1, -1)\n",
    "    y2v = y2.view(1, -1)\n",
    "\n",
    "    # Exact likelihood across tasks via shared helper (sum of log-probs), with temperature\n",
    "    log_p_exact = temp * band_prob_from_mu_sigma(mu, var, y1, y2, aggregate=\"sumlog\")\n",
    "\n",
    "    if softness is None:\n",
    "        # Return per-sample log-prob; callers can sum if needed\n",
    "        return log_p_exact.sum()\n",
    "\n",
    "    # For the soft surrogate we need per-task std; reuse helper path to std\n",
    "    if var.dim() == 3:\n",
    "        var_diag = torch.diagonal(var, dim1=-2, dim2=-1).clamp_min(MIN_VAR)\n",
    "        sigma = var_diag.sqrt()\n",
    "    else:\n",
    "        if var.dim() == 1:\n",
    "            var = var.unsqueeze(0)\n",
    "        sigma = var.clamp_min(MIN_VAR).sqrt()\n",
    "\n",
    "    # Soft surrogate per task\n",
    "    lo = torch.sigmoid((mu - y1v) / (softness * sigma))\n",
    "    hi = torch.sigmoid((y2v - mu) / (softness * sigma))\n",
    "    p_soft = (lo * hi).clamp_min(1e-12)\n",
    "    log_p_soft = p_soft.log().sum(dim=-1)\n",
    "\n",
    "    # Mixture in log-space per sample\n",
    "    mix_t = torch.tensor(mix, dtype=mu.dtype, device=mu.device)\n",
    "    log_p_mix = torch.logaddexp(\n",
    "        torch.log1p(-mix_t) + log_p_soft,\n",
    "        torch.log(mix_t) + log_p_exact,\n",
    "    )\n",
    "    return log_p_mix.sum()\n",
    "\n",
    "\n",
    "def make_interval_band_model(temp=1.0, softness=None, mix=1.0):\n",
    "    def model():\n",
    "        base = Normal(0., 1.).expand([1, d]).to_event(2)\n",
    "        transform = BoundedDomainTransform(domain_min, domain_max)\n",
    "        x_star = pyro.sample(\"x_star\", TransformedDistribution(base, [transform]))\n",
    "        mu, var = gp.predict_mean_and_variance(x_star)\n",
    "        assert isinstance(var, TensorLike)\n",
    "        pyro.factor(\n",
    "            \"band_logp\",\n",
    "            band_logprob(\n",
    "                mu,\n",
    "                var,\n",
    "                y_band_low,\n",
    "                y_band_high,\n",
    "                temp=temp,\n",
    "                softness=softness,\n",
    "                mix=mix\n",
    "            ),\n",
    "        )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_samples(samples, num_samples):\n",
    "    # Ensure correct shape\n",
    "    samples = samples.reshape(num_samples, -1)\n",
    "    with torch.no_grad():\n",
    "        mu_s, var_s = gp.predict_mean_and_variance(samples)\n",
    "        assert isinstance(var_s, TensorLike)\n",
    "        # Use shared helper for band probability aggregation, passing var directly\n",
    "        p_band = band_prob_from_mu_sigma(mu_s, var_s, y_band_low, y_band_high, aggregate=\"geomean\")\n",
    "\n",
    "    _fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "    # Probability in band\n",
    "    sc1 = ax1.scatter(samples[:, 0].cpu(), samples[:, 1].cpu(), c=p_band.cpu(), cmap=\"viridis\", s=6, alpha=0.7)\n",
    "    ax1.set_title(\"Band Probability (agg across tasks)\")\n",
    "    ax1.set_xlabel(\"x1\"); ax1.set_ylabel(\"x2\")\n",
    "    plt.colorbar(sc1, ax=ax1, label=\"P[y in band]\")\n",
    "\n",
    "    # Predicted mean\n",
    "    mu_color = mu_s.mean(dim=-1) if mu_s.dim() > 1 else mu_s\n",
    "    sc2 = ax2.scatter(samples[:, 0].cpu(), samples[:, 1].cpu(), c=mu_color.cpu(), cmap=\"viridis\", s=6, alpha=0.7)\n",
    "    ax2.set_title(\"GP Mean (avg across tasks)\")\n",
    "    ax2.set_xlabel(\"x1\"); ax2.set_ylabel(\"x2\")\n",
    "    plt.colorbar(sc2, ax=ax2, label=\"GP Mean\")\n",
    "\n",
    "    plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NUTS (without annealing)\n",
    "\n",
    "Initially investigate whether posterior samples can be identified with NUTS without adapting density surface during warmup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.infer import NUTS, MCMC\n",
    "\n",
    "nuts = NUTS(make_interval_band_model(temp=1.0, softness=None, mix=1.0))\n",
    "mcmc_no_schedule = MCMC(nuts, num_samples=50, warmup_steps=50)\n",
    "mcmc_no_schedule.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot samples from MCMC without annealing schedule\n",
    "plot_samples(mcmc_no_schedule.get_samples()[\"x_star\"], mcmc_no_schedule.num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NUTS (with annealing)\n",
    "\n",
    "Initially investigate whether posterior samples can be identified with NUTS at a faster rate through including scheduling during warmup for temperature and softness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add annealing schedule\n",
    "class AnnealScheduler:\n",
    "    def __init__(\n",
    "            self,\n",
    "            warmup=500,\n",
    "            temp_init=0.2,\n",
    "            temp_final=1.0,\n",
    "            soft_init=0.5,\n",
    "            soft_final=0.05,\n",
    "            mix_init=0.0,\n",
    "            mix_final=1.0\n",
    "    ):\n",
    "        self.warmup = warmup\n",
    "        self.temp_init = temp_init\n",
    "        self.temp_final = temp_final\n",
    "        self.soft_init = soft_init\n",
    "        self.soft_final = soft_final\n",
    "        self.mix_init = mix_init\n",
    "        self.mix_final = mix_final\n",
    "        self.counter = 0\n",
    "\n",
    "    def __call__(self):\n",
    "        frac = min(1.0, self.counter / self.warmup)\n",
    "        self.counter += 1\n",
    "        temp = self.temp_init + (self.temp_final - self.temp_init) * frac\n",
    "        softness = self.soft_init + (self.soft_final - self.soft_init) * frac\n",
    "        mix = self.mix_init + (self.mix_final - self.mix_init) * frac\n",
    "        return temp, softness, mix\n",
    "\n",
    "scheduler = AnnealScheduler(\n",
    "    warmup=500,\n",
    "    temp_init=0.01, temp_final=1.0,\n",
    "    soft_init=0.5, soft_final=0.05,\n",
    "    mix_init=0.0, mix_final=1.0\n",
    ")\n",
    "\n",
    "def tempered_model():\n",
    "    temp, softness, mix = scheduler()\n",
    "    return make_interval_band_model(temp=temp, softness=softness, mix=mix)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuts = NUTS(tempered_model)\n",
    "mcmc_with_annealing = MCMC(nuts, num_samples=100, warmup_steps=100)\n",
    "mcmc_with_annealing.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_samples(mcmc_with_annealing.get_samples()[\"x_star\"], mcmc_with_annealing.num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elliptical slice sampling\n",
    "\n",
    "Exploring alternative MCMC sampling approaches that are faster to fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal ESS for Gaussian prior with nonlinear likelihood\n",
    "# Reference: Murray et al. (2010)\n",
    "def elliptical_slice(log_likelihood, prior_sample, cur_state, max_trials=100) -> TensorLike:\n",
    "    nu = prior_sample\n",
    "    theta = torch.rand(()) * 2 * torch.pi\n",
    "    theta_min = theta - 2 * torch.pi\n",
    "    theta_max = theta\n",
    "    ll_cur = log_likelihood(cur_state)\n",
    "    ll_threshold = ll_cur + torch.log(torch.rand(())).item()\n",
    "    for _ in range(max_trials):\n",
    "        proposal = cur_state * torch.cos(theta) + nu * torch.sin(theta)\n",
    "        if log_likelihood(proposal) >= ll_threshold:\n",
    "            return proposal\n",
    "        if theta < 0:\n",
    "            theta_min = theta\n",
    "        else:\n",
    "            theta_max = theta\n",
    "        theta = torch.rand(()) * (theta_max - theta_min) + theta_min\n",
    "    return proposal\n",
    "\n",
    "@torch.no_grad()\n",
    "def ess_band_samples(n_draws=2000, burn=200, thin=2, seed=0):\n",
    "    torch.manual_seed(seed)\n",
    "    # Use ESS in the whitened space and transform to domain\n",
    "    base_normal = torch.distributions.Normal(0., 1.).expand([d])\n",
    "    z = base_normal.sample()  # (d,)\n",
    "    transform = BoundedDomainTransform(domain_min, domain_max)\n",
    "\n",
    "    def ll(z_vec):\n",
    "        x = transform(z_vec.unsqueeze(0))  # (1, d)\n",
    "        assert isinstance(x, TensorLike)\n",
    "        mu, var = gp.predict_mean_and_variance(x)\n",
    "        assert isinstance(var, TensorLike)\n",
    "        # Multi-task band log-prob per-sample (returns scalar)\n",
    "        return band_logprob(mu, var, y_band_low, y_band_high)\n",
    "\n",
    "    samples = []\n",
    "    total = burn + n_draws * thin\n",
    "    for t in range(total):\n",
    "        nu = base_normal.sample()\n",
    "        z = elliptical_slice(ll, nu, z)\n",
    "        if t >= burn and ((t - burn) % thin == 0):\n",
    "            samples.append(transform(z.unsqueeze(0))[0])\n",
    "    return torch.stack(samples, dim=0)\n",
    "\n",
    "ess_samps = ess_band_samples(n_draws=2000, burn=200, thin=2, seed=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_samples(ess_samps, ess_samps.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "@torch.no_grad()\n",
    "def ess_diag(samples, max_lag=100):\n",
    "    \"\"\"Simple univariate ESS estimate per dim via initial positive sequence.\"\"\"\n",
    "    x = samples.cpu().numpy()  # (T, d)\n",
    "    T, D = x.shape\n",
    "    ess_dims = []\n",
    "    for j in range(D):\n",
    "        s = x[:, j]\n",
    "        s = (s - s.mean()) / (s.std() + 1e-12)\n",
    "        rho_sum = 0.0\n",
    "        for lag in range(1, min(max_lag, T-1)):\n",
    "            c = np.corrcoef(s[:-lag], s[lag:])[0,1]\n",
    "            if np.isnan(c):\n",
    "                break\n",
    "            if c <= 0:\n",
    "                break\n",
    "            rho_sum += 2*c\n",
    "        ess_j = T / (1.0 + rho_sum)\n",
    "        ess_dims.append(ess_j)\n",
    "    return np.array(ess_dims)\n",
    "\n",
    "ess_per_dim = ess_diag(ess_samps)\n",
    "print('ESS per dimension (ESS ~ larger is better):', ess_per_dim)\n",
    "print('Min ESS:', ess_per_dim.min(), 'Median ESS:', np.median(ess_per_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance sampling\n",
    "\n",
    "An alternative to MCMC is to use importance sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.special import ndtr\n",
    "\n",
    "@torch.no_grad()\n",
    "def importance_resample(n_candidates=20000, n_keep=2000, m=1, seed=0):\n",
    "    torch.manual_seed(seed)\n",
    "    transform = BoundedDomainTransform(domain_min, domain_max)\n",
    "    # Propose from standard normal in R^{m x d}\n",
    "    z = torch.randn(\n",
    "        n_candidates, m, d,\n",
    "        device=domain_min.device,\n",
    "        dtype=domain_min.dtype,\n",
    "    )\n",
    "    # Map into bounded domain\n",
    "    x_cand = transform(z)[:, 0, :]  # (n_candidates, d)\n",
    "\n",
    "    # Weight by band probability under GP (multi-task aware)\n",
    "    mu, var = gp.predict_mean_and_variance(x_cand)\n",
    "    assert isinstance(var, TensorLike)\n",
    "    w = band_prob_from_mu_sigma(mu, var, y_band_low, y_band_high, aggregate=\"geomean\")\n",
    "    w = (w / w.max()).clamp_min(1e-12)\n",
    "\n",
    "    # Resample\n",
    "    idx = torch.multinomial(w, num_samples=n_keep, replacement=True)\n",
    "    return x_cand[idx]\n",
    "\n",
    "# Draw and plot\n",
    "imp_samples = importance_resample(n_candidates=50000, n_keep=5000, seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_samples(imp_samples, imp_samples.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Monte Carlo (SMC) with adaptive tempering\n",
    "\n",
    "SMC is a further alternative to importance sampling that might be expected to scale to higher dimensions slightly better.\n",
    "\n",
    "Temper the band likelihood from 0 to 1, adaptively controlling steps to hit a target Effective Sample Size (ESS). We resample when ESS falls below the threshold. This converges to the exact target at temperature 1 without gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def smc_band(\n",
    "    n_particles=4000,\n",
    "    ess_target_frac=0.7,\n",
    "    max_steps=60,\n",
    "    move_steps=2,\n",
    "    rw_step=0.3,\n",
    "    seed=0,\n",
    "):\n",
    "    \"\"\"\n",
    "    SMC with adaptive tempering from beta=0 to 1 for the band posterior.\n",
    "    Includes vectorized random-walk Metropolis rejuvenation in the whitened space.\n",
    "    Returns: x_final, weights, beta_schedule, ess_history, unique_count\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    transform = BoundedDomainTransform(domain_min, domain_max)\n",
    "    device = domain_min.device\n",
    "    dtype = domain_min.dtype\n",
    "\n",
    "    # Work in whitened space z ~ N(0, I_d)\n",
    "    z = torch.randn(n_particles, d, device=device, dtype=dtype)\n",
    "\n",
    "    def compute_ll(z_batch: torch.Tensor) -> torch.Tensor:\n",
    "        x = transform(z_batch)  # (N, d)\n",
    "        assert isinstance(x, TensorLike)\n",
    "        mu, var = gp.predict_mean_and_variance(x)\n",
    "        assert isinstance(var, TensorLike)\n",
    "        return band_prob_from_mu_sigma(mu, var, y_band_low, y_band_high, aggregate=\"sumlog\")  # (N,)\n",
    "\n",
    "    # Initial log-likelihoods\n",
    "    ll = compute_ll(z)\n",
    "\n",
    "    # Initialize weights at beta=0\n",
    "    logw = torch.zeros(n_particles, device=device, dtype=dtype)\n",
    "    beta = 0.0\n",
    "    betas: list[float] = [beta]\n",
    "    ess_hist: list[float] = []\n",
    "\n",
    "    def ess_from_logw(lw: torch.Tensor) -> float:\n",
    "        w = (lw - lw.logsumexp(0)).exp()\n",
    "        return float(1.0 / (w.pow(2).sum() + 1e-12))\n",
    "\n",
    "    def systematic_resample(w: torch.Tensor) -> torch.Tensor:\n",
    "        N = w.numel()\n",
    "        u0 = torch.rand((), device=w.device, dtype=w.dtype) / N\n",
    "        cdf = torch.cumsum(w, dim=0)\n",
    "        thresholds = u0 + torch.arange(N, device=w.device, dtype=w.dtype) / N\n",
    "        idx = torch.searchsorted(cdf, thresholds)\n",
    "        return idx.clamp_max(N - 1).long()\n",
    "\n",
    "    def rejuvenate_rw(z: torch.Tensor, ll: torch.Tensor, beta: float) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Vectorized RW-MH in z-space with tempered target: prior + beta * ll\n",
    "        for _ in range(move_steps):\n",
    "            z_prop = z + rw_step * torch.randn_like(z)\n",
    "            ll_prop = compute_ll(z_prop)\n",
    "            dprior = -0.5 * (z_prop.pow(2).sum(dim=1) - z.pow(2).sum(dim=1))\n",
    "            dlike = beta * (ll_prop - ll)\n",
    "            log_alpha = dprior + dlike\n",
    "            accept = torch.log(torch.rand(z.shape[0], device=z.device, dtype=z.dtype)).le(log_alpha)\n",
    "            if accept.any():\n",
    "                z = torch.where(accept.unsqueeze(1), z_prop, z)\n",
    "                ll = torch.where(accept, ll_prop, ll)\n",
    "        return z, ll\n",
    "\n",
    "    # Adaptive tempering loop\n",
    "    for _ in range(max_steps):\n",
    "        target_ess = ess_target_frac * n_particles\n",
    "        # Find delta via binary search\n",
    "        low, high = 0.0, float(1.0 - beta)\n",
    "        for _ in range(25):\n",
    "            if high <= 1e-6:\n",
    "                break\n",
    "            delta = 0.5 * (low + high)\n",
    "            lw_try = logw + delta * ll\n",
    "            ess_try = ess_from_logw(lw_try)\n",
    "            if ess_try < target_ess:\n",
    "                high = delta\n",
    "            else:\n",
    "                low = delta\n",
    "        delta = low\n",
    "        if delta <= 1e-8 and beta < 1.0:\n",
    "            delta = min(1e-3, 1.0 - beta)\n",
    "        beta = float(min(1.0, beta + delta))\n",
    "        logw = logw + delta * ll\n",
    "\n",
    "        # Normalize and compute ESS\n",
    "        lw_norm = logw - logw.logsumexp(0)\n",
    "        w = lw_norm.exp()\n",
    "        ess = float(1.0 / (w.pow(2).sum() + 1e-12))\n",
    "        betas.append(beta)\n",
    "        ess_hist.append(ess)\n",
    "\n",
    "        need_resample = (ess < target_ess) or (beta >= 1.0 - 1e-6)\n",
    "        if need_resample:\n",
    "            idx = systematic_resample(w)\n",
    "            z = z[idx]\n",
    "            ll = ll[idx]\n",
    "            logw.zero_()\n",
    "            z, ll = rejuvenate_rw(z, ll, beta)\n",
    "\n",
    "        if beta >= 1.0 - 1e-6:\n",
    "            break\n",
    "\n",
    "    x_final = transform(z)\n",
    "    lw_norm = logw - logw.logsumexp(0)\n",
    "    w_final = lw_norm.exp()\n",
    "    unique_count = torch.unique(x_final, dim=0).shape[0]\n",
    "\n",
    "    return x_final, w_final, torch.tensor(betas), torch.tensor(ess_hist), int(unique_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run SMC and plot\n",
    "smc_particles, smc_w, smc_betas, smc_ess, smc_unique = smc_band(\n",
    "    n_particles=4000, ess_target_frac=0.6, move_steps=2, rw_step=0.25, seed=123\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_samples(smc_particles, smc_particles.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic plots\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.scatter(smc_particles[:,0].cpu(), smc_particles[:,1].cpu(), s=4, alpha=0.4, c='tab:orange')\n",
    "plt.title(f'SMC particles (final), unique={smc_unique}/{smc_particles.shape[0]}')\n",
    "plt.xlabel('x1'); plt.ylabel('x2'); plt.tight_layout()\n",
    "\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.plot(smc_betas.cpu().numpy(), '-o', ms=3)\n",
    "plt.ylabel('beta'); plt.xlabel('step'); plt.title('Temperatures')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.plot(smc_ess.cpu().numpy(), '-o', ms=3)\n",
    "plt.ylabel('ESS'); plt.xlabel('step'); plt.title('ESS over steps')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does initializing NUTS with SMC particles speed up inference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.infer import NUTS, MCMC\n",
    "\n",
    "nuts = NUTS(make_interval_band_model(temp=1.0, softness=None, mix=1.0))\n",
    "mcmc_smc = MCMC(nuts, num_samples=100, warmup_steps=20, initial_params={\"x_star\": smc_particles[1:2, :]})\n",
    "mcmc_smc.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_samples(mcmc_smc.get_samples()[\"x_star\"], mcmc_smc.num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### History matching with multi-task band likelihood\n",
    "\n",
    "This secion looks at using the current history matching workflow to generate samples from the excursion set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoemulate.calibration.history_matching import HistoryMatchingWorkflow\n",
    "import numpy as np\n",
    "\n",
    "lower = y_band_low.item()\n",
    "upper = y_band_high.item()\n",
    "midpoint = 0.5 * (lower + upper)\n",
    "difference = upper - lower\n",
    "observations = {\"infection_rate\": lower + (upper - lower)*torch.rand(100)}\n",
    "\n",
    "hm = HistoryMatchingWorkflow(\n",
    "    simulator=simulator,\n",
    "    result=ae.best_result(),\n",
    "    observations={\"infection_rate\": (midpoint, (difference / 4 * 2)**2)}, # 2 * sigma = 0.05\n",
    "    threshold=1.0, # implausibility threshold in sigma units\n",
    "    train_x=x,\n",
    "    train_y=y,\n",
    "    log_level=\"error\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get samples in NROY space\n",
    "x_new = simulator.sample_inputs(10000)\n",
    "mean, variance = gp.predict_mean_and_variance(x_new)\n",
    "assert isinstance(variance, torch.Tensor)\n",
    "implausibility = hm.calculate_implausibility(mean, variance)\n",
    "x_star_nroy = hm.get_nroy(implausibility, x_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_samples(x_star_nroy, x_star_nroy.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare with a BayesianCalibration approach\n",
    "\n",
    "This section looks at using the current `BayesianCalibration` approach with a Gaussian-noise observation probabilistic model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc = BayesianCalibration(\n",
    "    gp, \n",
    "    simulator.parameters_range, \n",
    "    observations, \n",
    "    observation_noise=0.1,\n",
    "    model_uncertainty=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run MCMC using the NUTS sampler. The `BayesianCalibration` class uses Pyro under the hood. Below we use `pyro.set_rng_seed` to ensure reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc_bc = bc.run_mcmc(\n",
    "    warmup_steps=250, \n",
    "    num_samples=500,\n",
    "    num_chains=2    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to required format for plotting\n",
    "x_post_bc = torch.hstack([\n",
    "    mcmc_bc.get_samples()[\"beta\"].reshape(-1, 1),\n",
    "    mcmc_bc.get_samples()[\"gamma\"].reshape(-1, 1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_samples(x_post_bc, x_post_bc.shape[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
