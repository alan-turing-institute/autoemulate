{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "if not hasattr(torch, \"get_default_device\"):\n",
    "    def _get_default_device():\n",
    "        # always use CPU by default\n",
    "        return torch.device(\"cpu\")\n",
    "    torch.get_default_device = _get_default_device\n",
    "\n",
    "# probabilistic programming\n",
    "import pyro \n",
    "\n",
    "# MCMC plotting\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "from getdist.arviz_wrapper import arviz_to_mcsamples\n",
    "from getdist import plots\n",
    "\n",
    "# autoemulate imports\n",
    "from autoemulate.simulations.epidemic import Epidemic\n",
    "from autoemulate.core.compare import AutoEmulate\n",
    "from autoemulate.calibration.bayes import BayesianCalibration\n",
    "from autoemulate.emulators import GaussianProcessRBF\n",
    "\n",
    "# suppress warnings in notebook for readability\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "\n",
    "# random seed for reproducibility\n",
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoemulate.data.utils import set_random_seed\n",
    "set_random_seed(random_seed)\n",
    "pyro.set_rng_seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian calibration\n",
    "\n",
    "Bayesian calibration is a method for estimating which input parameters were most likely to produce observed data. An advantage over other calibration methods is that it returns a probability distribution over the input parameters rather than just point estimates.\n",
    "\n",
    "Performing Bayesian calibration requires:\n",
    "- a simulator or an emulator trained to approximate the simulator\n",
    "- observations associated with the simulator/emulator output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simulate data\n",
    "\n",
    "In this example, we'll use the `Epidemic` simulator, which returns the peak infection rate given two input parameters, `beta`(the transimission rate per day) and `gamma` (the recovery rate per day)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator = Epidemic(log_level=\"error\")\n",
    "x = simulator.sample_inputs(1000)\n",
    "y, _ = simulator.forward_batch(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we plot the simulated data. The peak infection rate is higher when the transmission rate increases and the recovery rate decreases and the two parameters are correlated with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transmission_rate = x[:, 0]\n",
    "recovery_rate = x[:, 1]\n",
    "\n",
    "plt.scatter(transmission_rate, recovery_rate, c=y, cmap='viridis')\n",
    "plt.xlabel('Transmission rate (beta)')\n",
    "plt.ylabel('Recovery rate (gamma)')\n",
    "plt.colorbar(label=\"Peak infection rate\")\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calibration requires at least one or multiple observations. These can come from running experiments or from the literature.\n",
    "\n",
    "Below we pick the initial parameter values and simulate the output. We then add noise to generate 100 \"observations\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_beta = 0.3\n",
    "true_gamma = 0.15 \n",
    "\n",
    "# simulator expects inputs of shape [1, number of inputs]\n",
    "params = torch.tensor([true_beta, true_gamma]).view(1, -1)\n",
    "true_infection_rate = simulator.forward(params)\n",
    "\n",
    "n_obs = 100\n",
    "stdev = 0.05\n",
    "noise = torch.normal(mean=0, std=stdev, size=(n_obs,))\n",
    "observed_infection_rates = true_infection_rate[0] + noise\n",
    "\n",
    "observations = {\"infection_rate\": observed_infection_rates}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use these observations to infer which input parameters were most likely to have produced them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Calibrate with simulator\n",
    "\n",
    "In this example, we have a fast simulator with only two input parameters, so we can use the simulator for calibration. The below code shows how to do this directly with Pyro. We can then compare this approach with using an emulator for calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro.distributions as dist\n",
    "from pyro.infer import MCMC\n",
    "from pyro.infer.mcmc import RandomWalkKernel\n",
    "\n",
    "# define the probabilistic model\n",
    "def model():\n",
    "    # uniform priors on parameters range\n",
    "    beta = pyro.sample(\"beta\", dist.Uniform(0.1, 0.5))\n",
    "    gamma = pyro.sample(\"gamma\", dist.Uniform(0.01, 0.2))\n",
    "    \n",
    "    mean = simulator.forward(torch.tensor([[beta, gamma]]))\n",
    "\n",
    "    with pyro.plate(f\"data\", n_obs):\n",
    "        pyro.sample(\n",
    "            \"infection_rate\",\n",
    "            dist.Normal(mean, stdev),\n",
    "            obs=observations[\"infection_rate\"],\n",
    "        )\n",
    "\n",
    "# run Bayesian inference with MCMC\n",
    "\n",
    "\n",
    "kernel = RandomWalkKernel(model, init_step_size=2.5)\n",
    "mcmc_sim = MCMC(\n",
    "    kernel,\n",
    "    warmup_steps=500,\n",
    "    num_samples=5000,\n",
    "    num_chains=1\n",
    ")\n",
    "mcmc_sim.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we plot the posterior samples of the input parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_samples = mcmc_sim.get_samples()\n",
    "    \n",
    "plt.scatter(sim_samples['beta'], sim_samples['gamma'], alpha=0.5)\n",
    "plt.xlabel('Transmission rate (beta)')\n",
    "plt.ylabel('Recovery rate (gamma)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Calibrate with emulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more complex simulators, it is recommended to first train an emulator to approximate the simulator and then use the emulator for calibration. This is because calibration typically requires thousands of evaluations of the simulator, which can be computationally expensive.\n",
    "\n",
    "`AutoEmulate` provides the `BayesCalibrator` class to perform Bayesian calibration with an emulator.\n",
    "\n",
    "First we need to train an emulator. For the purposes of this tutorial, we will restrict the emulator choice to `GaussianProcess` with default hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae = AutoEmulate(\n",
    "    x, \n",
    "    y, \n",
    "    models=[GaussianProcessRBF], \n",
    "    # use default parameters\n",
    "    model_params={},\n",
    "    log_level=\"error\",\n",
    "    device=\"cpu\",  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that the fitted emulator performs well on both the train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae.summarise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = ae.best_result().model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `BayesianCalibration` object takes as input the trained emulator, the simulator parameter ranges and the \"observed\" data simulated above. \n",
    "\n",
    "The underlying probabilistic model is the same one used on the simulator example above. It assumes the observations are drawn from a Gaussian distribution with the mean predicted by the emulator. The user also has to specify the `observation_noise` which is the variance of the Gaussian likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc = BayesianCalibration(\n",
    "    gp, \n",
    "    simulator.parameters_range, \n",
    "    observations, \n",
    "    # specify noise as variance\n",
    "    observation_noise=stdev**2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run MCMC using the NUTS sampler. The `BayesianCalibration` class uses Pyro under the hood. Below we use `pyro.set_rng_seed` to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc_emu = bc.run_mcmc(\n",
    "    warmup_steps=250, \n",
    "    num_samples=1000,\n",
    "    num_chains=2    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above returns the Pyro `MCMC` object which has a number of useful methods associated with it. One can access all the posterior samples using `mcmc.get_samples()` or just the summary statistics using `mcmc.summary()`. This shows that the posterior mean estimates of the input parameters are close to the true values used to generate the observations."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "mcmc_emu.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Plotting with Arviz\n",
    "\n",
    "The `BayesianCalibrator.to_arviz` method converts the `mcmc` object so that it is compatible with the Arviz plotting library. Using Arviz makes it very easy to produce all the standard plots of the calibration results as well as MCMC diagnostics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az_data = bc.to_arviz(mcmc_emu, posterior_predictive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main plot of interest is the posterior distribution over the parameters given the observations. Below we plot the pairwise joint distribution and can see that the two parameters are correlated as expected. The results look very similar to the results obtained using the simulator directly above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = az.plot_pair(az_data, kind='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior predictive samples can be plotted alongside the observed data. This shows that the calibration results capture the observed data well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = az.plot_ppc(az_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the MCMC behaviour, the samples from the posterior distribution can be viewed as a trace (right-hand plots) with 1D KDEs for each chain for each variable (left-hand plots)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = az.plot_trace(az_data, figsize=(20, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Plotting with GetDist\n",
    "\n",
    "The `BayesianCalibration.to_getdist` static method converts an `mcmc` object so that it is compatible with the `getdist` plotting library. Alternatively, one can use the `arviz_to_mcsamples` function from GetDist to convert the Arviz data object to a GetDist `MCSamples` object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert simulator calibration samples\n",
    "sim_data = BayesianCalibration.to_getdist(mcmc_sim, label=\"Simulator\")\n",
    "\n",
    "# convert emulator calibration samples\n",
    "emu_data = arviz_to_mcsamples(az_data, dataset_label=\"Emulator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we compare the posterior distributions obtained using the simulator and the emulator. Both distributions capture the true parameter values (indicated by the dashed lines). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_data.smooth_scale_1D = 0.8\n",
    "emu_data.smooth_scale_1D = 0.8\n",
    "\n",
    "g = plots.get_subplot_plotter()\n",
    "g.triangle_plot( \n",
    "    [sim_data, emu_data], \n",
    "    filled=True,\n",
    "    markers={\"beta\": true_beta, \"gamma\": true_gamma},\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# g.fig.savefig(\"bayes_calibration_getdist.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from scipy.integrate import solve_ivp\n",
    "\n",
    "from autoemulate.core.types import NumpyLike, TensorLike\n",
    "from autoemulate.simulations.base import Simulator\n",
    "\n",
    "class SEIRSimulator(Simulator):\n",
    "    \"\"\"\n",
    "    Simulator of infectious disease spread using the SEIR model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        parameters_range=None,\n",
    "        output_names=None,\n",
    "        log_level: str = \"progress_bar\",\n",
    "    ):\n",
    "        if parameters_range is None:\n",
    "            parameters_range = {\n",
    "                \"beta\": (0.1, 0.5),\n",
    "                \"gamma\": (0.01, 0.2),\n",
    "                \"sigma\": (0.05, 0.3)  # incubation rate\n",
    "            }\n",
    "        if output_names is None:\n",
    "            output_names = [\"infection_rate\"]\n",
    "        super().__init__(parameters_range, output_names, log_level)\n",
    "\n",
    "    def _forward(self, x: TensorLike) -> TensorLike:\n",
    "        \"\"\"\n",
    "        Simulate the epidemic using the SEIR model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: TensorLike\n",
    "            input parameter values to simulate [beta, gamma, sigma]:\n",
    "            - `beta`: transmission rate per day\n",
    "            - `gamma`: recovery rate per day\n",
    "            - `sigma`: incubation rate per day\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        TensorLike\n",
    "            Peak infection rate.\n",
    "        \"\"\"\n",
    "        assert x.shape[0] == 1, (\n",
    "            f\"Simulator._forward expects a single input, got {x.shape[0]}\"\n",
    "        )\n",
    "        y = simulate_seir_epidemic(x.cpu().numpy()[0])\n",
    "        return torch.tensor([y], dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "def simulate_seir_epidemic(x: NumpyLike, N: int = 1000, I0: int = 1, E0: int = 0) -> float:\n",
    "    \"\"\"\n",
    "    Simulate an epidemic using the SEIR model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: NumpyLike\n",
    "        The parameters of the SEIR model. [beta, gamma, sigma]\n",
    "    N: int\n",
    "        Total population\n",
    "    I0: int\n",
    "        Initial number of infected individuals\n",
    "    E0: int\n",
    "        Initial number of exposed individuals\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    peak_infection_rate: float\n",
    "        The peak infection rate as a fraction of the total population.\n",
    "    \"\"\"\n",
    "    assert len(x) == 3\n",
    "    beta, gamma, sigma = x\n",
    "\n",
    "    S0 = N - I0 - E0\n",
    "    R0 = 0\n",
    "    t_span = [0, 160]\n",
    "    y0 = [S0, E0, I0, R0]\n",
    "\n",
    "    def seir_model(t, y, N, beta, gamma, sigma):\n",
    "        S, E, I, R = y\n",
    "        dSdt = -beta * S * I / N\n",
    "        dEdt = beta * S * I / N - sigma * E\n",
    "        dIdt = sigma * E - gamma * I\n",
    "        dRdt = gamma * I\n",
    "        return [dSdt, dEdt, dIdt, dRdt]\n",
    "\n",
    "    t_eval = np.linspace(t_span[0], t_span[1], 160)\n",
    "    sol = solve_ivp(seir_model, t_span, y0, args=(N, beta, gamma, sigma), t_eval=t_eval)\n",
    "\n",
    "    S, E, I, R = sol.y\n",
    "    I_max = np.max(I)\n",
    "\n",
    "    return I_max / N  # peak infection rate same as Epidemic SIR model\n",
    "\n",
    "\n",
    "simulator = SEIRSimulator(log_level=\"error\")\n",
    "x = simulator.sample_inputs(1000)\n",
    "y, _ = simulator.forward_batch(x)\n",
    "\n",
    "# Plot\n",
    "transmission_rate = x[:, 0]\n",
    "recovery_rate = x[:, 1]\n",
    "exposure_rate = x[:, 2]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sc = plt.scatter(transmission_rate, recovery_rate, c=y, cmap='viridis')\n",
    "plt.xlabel('Transmission rate (beta)')\n",
    "plt.ylabel('Recovery rate (gamma)')\n",
    "plt.colorbar(sc, label=\"Peak infection rate\")\n",
    "plt.title(\"SEIR Model Simulation\")\n",
    "plt.show()\n",
    "\n",
    "# Step 2: Observed data\n",
    "true_beta = 0.3\n",
    "true_gamma = 0.15\n",
    "true_sigma = 0.2\n",
    "params = torch.tensor([true_beta, true_gamma, true_sigma]).view(1, -1)\n",
    "true_infection_rate = simulator.forward(params)\n",
    "\n",
    "n_obs = 100\n",
    "stdev = 0.05\n",
    "noise = torch.normal(mean=0, std=stdev, size=(n_obs,))\n",
    "observed_infection_rates = true_infection_rate[0] + noise\n",
    "observations = {\"infection_rate\": observed_infection_rates}\n",
    "\n",
    "# Step 3: Calibrate with simulator using Pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import MCMC\n",
    "from pyro.infer.mcmc import RandomWalkKernel\n",
    "\n",
    "def model():\n",
    "    beta = pyro.sample(\"beta\", dist.Uniform(0.1, 0.5))\n",
    "    gamma = pyro.sample(\"gamma\", dist.Uniform(0.01, 0.2))\n",
    "    sigma = pyro.sample(\"sigma\", dist.Uniform(0.05, 0.5))\n",
    "\n",
    "    mean = simulator.forward(torch.tensor([[beta, gamma, sigma]]))\n",
    "\n",
    "    with pyro.plate(\"data\", n_obs):\n",
    "        pyro.sample(\n",
    "            \"infection_rate\",\n",
    "            dist.Normal(mean, stdev),\n",
    "            obs=observations[\"infection_rate\"],\n",
    "        )\n",
    "\n",
    "kernel = RandomWalkKernel(model, init_step_size=2.5)\n",
    "mcmc_sim = MCMC(kernel, warmup_steps=1500, num_samples=15000, num_chains=1) #might have to increase warmup syeps and num of samples for convergence\n",
    "mcmc_sim.run()\n",
    "\n",
    "sim_samples = mcmc_sim.get_samples()\n",
    "plt.scatter(sim_samples['beta'], sim_samples['gamma'], alpha=0.5)\n",
    "plt.xlabel('Beta')\n",
    "plt.ylabel('Gamma')\n",
    "plt.title('Simulator Posterior Samples')\n",
    "plt.show()\n",
    "\n",
    "# Step 4: Train Emulator and Calibrate\n",
    "from autoemulate.core.compare import AutoEmulate\n",
    "from autoemulate.emulators import GaussianProcessRBF\n",
    "from autoemulate.calibration.bayes import BayesianCalibration\n",
    "\n",
    "ae = AutoEmulate(x, y, models=[GaussianProcessRBF], model_params={}, log_level=\"error\")\n",
    "ae.summarise()\n",
    "gp = ae.best_result().model\n",
    "\n",
    "bc = BayesianCalibration(\n",
    "    gp,\n",
    "    simulator.parameters_range,\n",
    "    observations,\n",
    "    observation_noise=stdev**2,\n",
    ")\n",
    "\n",
    "mcmc_emu = bc.run_mcmc(warmup_steps=250, num_samples=1000, num_chains=2)\n",
    "mcmc_emu.summary()\n",
    "\n",
    "# Step 5: Plotting\n",
    "az_data = bc.to_arviz(mcmc_emu, posterior_predictive=True)\n",
    "_ = az.plot_pair(az_data, kind='kde')\n",
    "_ = az.plot_ppc(az_data)\n",
    "_ = az.plot_trace(az_data, figsize=(20, 8))\n",
    "\n",
    "# Step 6: Compare Posterior Distributions\n",
    "sim_data = BayesianCalibration.to_getdist(mcmc_sim, label=\"Simulator\")\n",
    "emu_data = arviz_to_mcsamples(az_data, dataset_label=\"Emulator\")\n",
    "sim_data.smooth_scale_1D = 0.8\n",
    "emu_data.smooth_scale_1D = 0.8\n",
    "\n",
    "g = plots.get_subplot_plotter()\n",
    "g.triangle_plot(\n",
    "    [sim_data, emu_data],\n",
    "    filled=True,\n",
    "    markers={\"beta\": true_beta, \"gamma\": true_gamma, \"sigma\": true_sigma},\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from scipy.integrate import solve_ivp\n",
    "\n",
    "from autoemulate.core.types import NumpyLike, TensorLike\n",
    "from autoemulate.simulations.base import Simulator\n",
    "\n",
    "\n",
    "class SEIRTrajectorySimulator(Simulator):\n",
    "    \"\"\"\n",
    "    SEIR simulator that returns I(t)/N at a set of time points (trajectory).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        parameters_range=None,\n",
    "        t_grid=None,              # time points at which we observe I(t)\n",
    "        output_names=None,\n",
    "        log_level: str = \"progress_bar\",\n",
    "    ):\n",
    "        # Parameter ranges as before\n",
    "        if parameters_range is None:\n",
    "            parameters_range = {\n",
    "                \"beta\": (0.1, 0.5),\n",
    "                \"gamma\": (0.01, 0.2),\n",
    "                \"sigma\": (0.05, 0.3),  # incubation rate\n",
    "            }\n",
    "\n",
    "        # Choose 10 time points in [0, 160]\n",
    "        if t_grid is None:\n",
    "            self.t_grid = np.linspace(20.0, 140.0, 10)  # shape (T,)\n",
    "        else:\n",
    "            self.t_grid = np.array(t_grid, dtype=float)\n",
    "\n",
    "        # One output per time point: I(t_k)/N\n",
    "        if output_names is None:\n",
    "            output_names = [f\"I_t_{k}\" for k in range(len(self.t_grid))]\n",
    "\n",
    "        self.T = len(self.t_grid)\n",
    "        super().__init__(parameters_range, output_names, log_level)\n",
    "\n",
    "    def _forward(self, x: TensorLike) -> TensorLike:\n",
    "        \"\"\"\n",
    "        Simulate the SEIR model and return I(t)/N at self.t_grid.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: TensorLike\n",
    "            Input parameters [beta, gamma, sigma].\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        TensorLike\n",
    "            Shape [1, T] where T = len(self.t_grid), containing I(t_k)/N.\n",
    "        \"\"\"\n",
    "        assert x.shape[0] == 1, (\n",
    "            f\"Simulator._forward expects a single input, got {x.shape[0]}\"\n",
    "        )\n",
    "        theta = x.cpu().numpy()[0]  # [beta, gamma, sigma]\n",
    "        y_traj = simulate_seir_trajectory(theta, t_grid=self.t_grid)\n",
    "        # shape (T,) -> (1, T)\n",
    "        return torch.tensor(y_traj, dtype=torch.float32).view(1, -1)\n",
    "\n",
    "\n",
    "def simulate_seir_trajectory(\n",
    "    x: NumpyLike,\n",
    "    N: int = 1000,\n",
    "    I0: int = 1,\n",
    "    E0: int = 0,\n",
    "    t_grid: NumpyLike | None = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Simulate an epidemic using the SEIR model and return I(t)/N at t_grid.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: NumpyLike\n",
    "        Parameters [beta, gamma, sigma].\n",
    "    N: int\n",
    "        Total population.\n",
    "    I0: int\n",
    "        Initial number of infected individuals.\n",
    "    E0: int\n",
    "        Initial number of exposed individuals.\n",
    "    t_grid: NumpyLike\n",
    "        Times at which to return I(t)/N.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Array of shape (T,) with I(t_k)/N at each t_k in t_grid.\n",
    "    \"\"\"\n",
    "    assert len(x) == 3\n",
    "    beta, gamma, sigma = x\n",
    "\n",
    "    S0 = N - I0 - E0\n",
    "    R0 = 0.0\n",
    "\n",
    "    # If no t_grid is given, default to 10 points (for direct calls)\n",
    "    if t_grid is None:\n",
    "        t_grid = np.linspace(20.0, 140.0, 10)\n",
    "    t_grid = np.array(t_grid, dtype=float)\n",
    "\n",
    "    t_span = [t_grid.min(), t_grid.max()]\n",
    "    y0 = [S0, E0, I0, R0]\n",
    "\n",
    "    def seir_model(t, y, N, beta, gamma, sigma):\n",
    "        S, E, I, R = y\n",
    "        dSdt = -beta * S * I / N\n",
    "        dEdt = beta * S * I / N - sigma * E\n",
    "        dIdt = sigma * E - gamma * I\n",
    "        dRdt = gamma * I\n",
    "        return [dSdt, dEdt, dIdt, dRdt]\n",
    "\n",
    "    # Solve ODE at the desired times\n",
    "    sol = solve_ivp(\n",
    "        seir_model,\n",
    "        t_span,\n",
    "        y0,\n",
    "        args=(N, beta, gamma, sigma),\n",
    "        t_eval=t_grid,\n",
    "    )\n",
    "\n",
    "    S, E, I, R = sol.y\n",
    "    I_frac = I / N  # I(t)/N\n",
    "    return I_frac  # shape (T,)\n",
    "\n",
    "# Instantiate trajectory-based SEIR simulator\n",
    "simulator = SEIRTrajectorySimulator(log_level=\"error\")\n",
    "t_grid = simulator.t_grid   # 10 time points\n",
    "T = len(t_grid)\n",
    "\n",
    "# Sample training inputs and corresponding trajectories\n",
    "x = simulator.sample_inputs(1000)      # shape (1000, 3)\n",
    "y, _ = simulator.forward_batch(x)      # shape (1000, T)\n",
    "\n",
    "print(\"x shape:\", x.shape)\n",
    "print(\"y shape:\", y.shape)  # (1000, 10)\n",
    "# True parameters (for synthetic \"observed\" trajectory)\n",
    "true_beta = 0.3\n",
    "true_gamma = 0.15\n",
    "true_sigma = 0.2\n",
    "\n",
    "params = torch.tensor([true_beta, true_gamma, true_sigma]).view(1, -1)\n",
    "true_traj = simulator.forward(params).squeeze(0)   # shape (T,)\n",
    "\n",
    "stdev = 0.05  # observation noise std per time point\n",
    "\n",
    "# Single observed trajectory: true I(t)/N + noise\n",
    "observed_traj = true_traj + torch.normal(\n",
    "    mean=torch.zeros_like(true_traj),\n",
    "    std=stdev,\n",
    ")\n",
    "\n",
    "# Build observations dict: one scalar observation per \"output\" (time point)\n",
    "observations = {}\n",
    "for name, value in zip(simulator.output_names, observed_traj):\n",
    "    # each is a 1D tensor of length 1\n",
    "    observations[name] = value.view(1)\n",
    "\n",
    "observations\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import MCMC\n",
    "from pyro.infer.mcmc import NUTS\n",
    "\n",
    "pyro.clear_param_store()\n",
    "pyro.set_rng_seed(random_seed)\n",
    "\n",
    "\n",
    "def seir_traj_model():\n",
    "    # Priors\n",
    "    beta = pyro.sample(\"beta\", dist.Uniform(0.1, 0.5))\n",
    "    gamma = pyro.sample(\"gamma\", dist.Uniform(0.01, 0.2))\n",
    "    sigma = pyro.sample(\"sigma\", dist.Uniform(0.05, 0.3))\n",
    "\n",
    "    # Simulator forward: trajectory at 10 time points\n",
    "    theta_vec = torch.tensor([[beta, gamma, sigma]])\n",
    "    mean_traj = simulator.forward(theta_vec).squeeze(0)   # shape (T,)\n",
    "\n",
    "    # Likelihood over time points (independent normals)\n",
    "    with pyro.plate(\"time\", T):\n",
    "        pyro.sample(\n",
    "            \"I_t\",                      # a single RV with shape (T,)\n",
    "            dist.Normal(mean_traj, stdev),\n",
    "            obs=observed_traj,\n",
    "        )\n",
    "\n",
    "\n",
    "# Run MCMC with NUTS on the simulator model\n",
    "nuts_kernel_sim = NUTS(seir_traj_model)\n",
    "mcmc_sim = MCMC(\n",
    "    nuts_kernel_sim,\n",
    "    warmup_steps=500,\n",
    "    num_samples=2000,\n",
    "    num_chains=1,\n",
    ")\n",
    "mcmc_sim.run()\n",
    "\n",
    "sim_samples = mcmc_sim.get_samples()\n",
    "plt.scatter(sim_samples['beta'], sim_samples['gamma'], alpha=0.5)\n",
    "plt.xlabel('beta')\n",
    "plt.ylabel('gamma')\n",
    "plt.title('Simulator posterior samples (SEIR, trajectory)')\n",
    "plt.show()\n",
    "from autoemulate.core.compare import AutoEmulate\n",
    "from autoemulate.emulators import GaussianProcessRBF\n",
    "from autoemulate.calibration.bayes import BayesianCalibration\n",
    "\n",
    "# Train multi-output GP emulator on (beta, gamma, sigma) -> I(t_1..t_10)/N\n",
    "ae = AutoEmulate(\n",
    "    x,\n",
    "    y,\n",
    "    models=[GaussianProcessRBF],\n",
    "    model_params={},      # defaults are fine here\n",
    "    log_level=\"error\",\n",
    ")\n",
    "ae.summarise()\n",
    "gp = ae.best_result().model\n",
    "\n",
    "# Bayesian calibration with emulator, using the trajectory\n",
    "bc = BayesianCalibration(\n",
    "    gp,\n",
    "    simulator.parameters_range,  # includes beta, gamma, sigma\n",
    "    observations,\n",
    "    observation_noise=stdev**2,  # scalar variance applied to each output\n",
    "    model_uncertainty=False,     # keep simple for now\n",
    ")\n",
    "\n",
    "# Run MCMC with NUTS using the emulator\n",
    "mcmc_emu = bc.run_mcmc(\n",
    "    warmup_steps=500,\n",
    "    num_samples=2000,\n",
    "    num_chains=2,\n",
    ")\n",
    "mcmc_emu.summary()\n",
    "# Convert emulator MCMC to ArviZ\n",
    "az_data = bc.to_arviz(mcmc_emu, posterior_predictive=True)\n",
    "\n",
    "# Joint posterior over parameters\n",
    "_ = az.plot_pair(az_data, kind='kde')\n",
    "plt.show()\n",
    "\n",
    "# Posterior predictive vs observed trajectory\n",
    "_ = az.plot_ppc(az_data)\n",
    "plt.show()\n",
    "\n",
    "# Trace plots\n",
    "_ = az.plot_trace(az_data, figsize=(20, 8))\n",
    "plt.show()\n",
    "\n",
    "# Compare simulator vs emulator posteriors (trajectory-based)\n",
    "from getdist.arviz_wrapper import arviz_to_mcsamples\n",
    "from getdist import plots\n",
    "\n",
    "sim_data = BayesianCalibration.to_getdist(mcmc_sim, label=\"Simulator\")\n",
    "emu_data = arviz_to_mcsamples(az_data, dataset_label=\"Emulator\")\n",
    "\n",
    "sim_data.smooth_scale_1D = 0.8\n",
    "emu_data.smooth_scale_1D = 0.8\n",
    "\n",
    "g = plots.get_subplot_plotter()\n",
    "g.triangle_plot(\n",
    "    [sim_data, emu_data],\n",
    "    filled=True,\n",
    "    markers={\"beta\": true_beta, \"gamma\": true_gamma, \"sigma\": true_sigma},\n",
    ")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
