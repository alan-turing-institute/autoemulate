{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "if not hasattr(torch, \"get_default_device\"):\n",
    "    def _get_default_device():\n",
    "        # always use CPU by default\n",
    "        return torch.device(\"cpu\")\n",
    "    torch.get_default_device = _get_default_device\n",
    "\n",
    "# probabilistic programming\n",
    "import pyro \n",
    "\n",
    "# MCMC plotting\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "from getdist.arviz_wrapper import arviz_to_mcsamples\n",
    "from getdist import plots\n",
    "\n",
    "# autoemulate imports\n",
    "from autoemulate.simulations.epidemic import Epidemic\n",
    "from autoemulate.core.compare import AutoEmulate\n",
    "from autoemulate.calibration.bayes import BayesianCalibration\n",
    "from autoemulate.emulators import GaussianProcessRBF\n",
    "\n",
    "# suppress warnings in notebook for readability\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "\n",
    "# random seed for reproducibility\n",
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoemulate.data.utils import set_random_seed\n",
    "set_random_seed(random_seed)\n",
    "pyro.set_rng_seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian calibration\n",
    "\n",
    "Bayesian calibration is a method for estimating which input parameters were most likely to produce observed data. An advantage over other calibration methods is that it returns a probability distribution over the input parameters rather than just point estimates.\n",
    "\n",
    "Performing Bayesian calibration requires:\n",
    "- a simulator or an emulator trained to approximate the simulator\n",
    "- observations associated with the simulator/emulator output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simulate data\n",
    "\n",
    "In this example, we'll use the `Epidemic` simulator, which returns the peak infection rate given two input parameters, `beta`(the transimission rate per day) and `gamma` (the recovery rate per day)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator = Epidemic(log_level=\"error\")\n",
    "x = simulator.sample_inputs(1000)\n",
    "y, _ = simulator.forward_batch(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we plot the simulated data. The peak infection rate is higher when the transmission rate increases and the recovery rate decreases and the two parameters are correlated with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transmission_rate = x[:, 0]\n",
    "recovery_rate = x[:, 1]\n",
    "\n",
    "plt.scatter(transmission_rate, recovery_rate, c=y, cmap='viridis')\n",
    "plt.xlabel('Transmission rate (beta)')\n",
    "plt.ylabel('Recovery rate (gamma)')\n",
    "plt.colorbar(label=\"Peak infection rate\")\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calibration requires at least one or multiple observations. These can come from running experiments or from the literature.\n",
    "\n",
    "Below we pick the initial parameter values and simulate the output. We then add noise to generate 100 \"observations\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_beta = 0.3\n",
    "true_gamma = 0.15 \n",
    "\n",
    "# simulator expects inputs of shape [1, number of inputs]\n",
    "params = torch.tensor([true_beta, true_gamma]).view(1, -1)\n",
    "true_infection_rate = simulator.forward(params)\n",
    "\n",
    "n_obs = 100\n",
    "stdev = 0.05\n",
    "noise = torch.normal(mean=0, std=stdev, size=(n_obs,))\n",
    "observed_infection_rates = true_infection_rate[0] + noise\n",
    "\n",
    "observations = {\"infection_rate\": observed_infection_rates}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use these observations to infer which input parameters were most likely to have produced them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Calibrate with simulator\n",
    "\n",
    "In this example, we have a fast simulator with only two input parameters, so we can use the simulator for calibration. The below code shows how to do this directly with Pyro. We can then compare this approach with using an emulator for calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro.distributions as dist\n",
    "from pyro.infer import MCMC\n",
    "from pyro.infer.mcmc import RandomWalkKernel\n",
    "\n",
    "# define the probabilistic model\n",
    "def model():\n",
    "    # uniform priors on parameters range\n",
    "    beta = pyro.sample(\"beta\", dist.Uniform(0.1, 0.5))\n",
    "    gamma = pyro.sample(\"gamma\", dist.Uniform(0.01, 0.2))\n",
    "    \n",
    "    mean = simulator.forward(torch.tensor([[beta, gamma]]))\n",
    "\n",
    "    with pyro.plate(f\"data\", n_obs):\n",
    "        pyro.sample(\n",
    "            \"infection_rate\",\n",
    "            dist.Normal(mean, stdev),\n",
    "            obs=observations[\"infection_rate\"],\n",
    "        )\n",
    "\n",
    "# run Bayesian inference with MCMC\n",
    "\n",
    "\n",
    "kernel = RandomWalkKernel(model, init_step_size=2.5)\n",
    "mcmc_sim = MCMC(\n",
    "    kernel,\n",
    "    warmup_steps=500,\n",
    "    num_samples=5000,\n",
    "    num_chains=1\n",
    ")\n",
    "mcmc_sim.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we plot the posterior samples of the input parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_samples = mcmc_sim.get_samples()\n",
    "    \n",
    "plt.scatter(sim_samples['beta'], sim_samples['gamma'], alpha=0.5)\n",
    "plt.xlabel('Transmission rate (beta)')\n",
    "plt.ylabel('Recovery rate (gamma)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Calibrate with emulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more complex simulators, it is recommended to first train an emulator to approximate the simulator and then use the emulator for calibration. This is because calibration typically requires thousands of evaluations of the simulator, which can be computationally expensive.\n",
    "\n",
    "`AutoEmulate` provides the `BayesCalibrator` class to perform Bayesian calibration with an emulator.\n",
    "\n",
    "First we need to train an emulator. For the purposes of this tutorial, we will restrict the emulator choice to `GaussianProcess` with default hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae = AutoEmulate(\n",
    "    x, \n",
    "    y, \n",
    "    models=[GaussianProcessRBF], \n",
    "    # use default parameters\n",
    "    model_params={},\n",
    "    log_level=\"error\",\n",
    "    device=\"cpu\",  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that the fitted emulator performs well on both the train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae.summarise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = ae.best_result().model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `BayesianCalibration` object takes as input the trained emulator, the simulator parameter ranges and the \"observed\" data simulated above. \n",
    "\n",
    "The underlying probabilistic model is the same one used on the simulator example above. It assumes the observations are drawn from a Gaussian distribution with the mean predicted by the emulator. The user also has to specify the `observation_noise` which is the variance of the Gaussian likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc = BayesianCalibration(\n",
    "    gp, \n",
    "    simulator.parameters_range, \n",
    "    observations, \n",
    "    # specify noise as variance\n",
    "    observation_noise=stdev**2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run MCMC using the NUTS sampler. The `BayesianCalibration` class uses Pyro under the hood. Below we use `pyro.set_rng_seed` to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc_emu = bc.run_mcmc(\n",
    "    warmup_steps=250, \n",
    "    num_samples=1000,\n",
    "    num_chains=2    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above returns the Pyro `MCMC` object which has a number of useful methods associated with it. One can access all the posterior samples using `mcmc.get_samples()` or just the summary statistics using `mcmc.summary()`. This shows that the posterior mean estimates of the input parameters are close to the true values used to generate the observations."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "mcmc_emu.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Plotting with Arviz\n",
    "\n",
    "The `BayesianCalibrator.to_arviz` method converts the `mcmc` object so that it is compatible with the Arviz plotting library. Using Arviz makes it very easy to produce all the standard plots of the calibration results as well as MCMC diagnostics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az_data = bc.to_arviz(mcmc_emu, posterior_predictive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main plot of interest is the posterior distribution over the parameters given the observations. Below we plot the pairwise joint distribution and can see that the two parameters are correlated as expected. The results look very similar to the results obtained using the simulator directly above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = az.plot_pair(az_data, kind='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior predictive samples can be plotted alongside the observed data. This shows that the calibration results capture the observed data well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = az.plot_ppc(az_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the MCMC behaviour, the samples from the posterior distribution can be viewed as a trace (right-hand plots) with 1D KDEs for each chain for each variable (left-hand plots)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = az.plot_trace(az_data, figsize=(20, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Plotting with GetDist\n",
    "\n",
    "The `BayesianCalibration.to_getdist` static method converts an `mcmc` object so that it is compatible with the `getdist` plotting library. Alternatively, one can use the `arviz_to_mcsamples` function from GetDist to convert the Arviz data object to a GetDist `MCSamples` object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert simulator calibration samples\n",
    "sim_data = BayesianCalibration.to_getdist(mcmc_sim, label=\"Simulator\")\n",
    "\n",
    "# convert emulator calibration samples\n",
    "emu_data = arviz_to_mcsamples(az_data, dataset_label=\"Emulator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we compare the posterior distributions obtained using the simulator and the emulator. Both distributions capture the true parameter values (indicated by the dashed lines). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_data.smooth_scale_1D = 0.8\n",
    "emu_data.smooth_scale_1D = 0.8\n",
    "\n",
    "g = plots.get_subplot_plotter()\n",
    "g.triangle_plot( \n",
    "    [sim_data, emu_data], \n",
    "    filled=True,\n",
    "    markers={\"beta\": true_beta, \"gamma\": true_gamma},\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# g.fig.savefig(\"bayes_calibration_getdist.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Extending to an SEIR epidemic model\n",
    "\n",
    "The Epidemic simulator used above implements a simple SIR model with two parameters (beta, gamma) and returns the peak infection rate as a summary statistic. In this final step, we extend the tutorial to a slightly richer SEIR model by adding an exposed compartment and an incubation rate.\n",
    "\n",
    "We first define a new SEIRSimulator that subclasses Simulator. The underlying SEIR dynamics are:\n",
    "\n",
    "● S(t): susceptible\n",
    "\n",
    "● E(t): exposed (infected but not yet infectious)\n",
    "\n",
    "● I(t): infectious\n",
    "\n",
    "● R(t): recovered\n",
    "\n",
    "with parameters:\n",
    "\n",
    "● beta: transmission rate per day\n",
    "\n",
    "● gamma: recovery rate per day\n",
    "\n",
    "● sigma: incubation rate per day\n",
    "\n",
    "The ODE system is integrated with solve_ivp, and we again return the peak infection fraction max_t I(t) / N so that the output is comparable to the original SIR example.\n",
    "\n",
    "As before, we:\n",
    "\n",
    "1. Simulate data.\n",
    "We use SEIRSimulator to draw a set of input parameters [beta, gamma, sigma], run the SEIR model, and visualise how the peak infection rate varies over the parameter space.\n",
    "\n",
    "2. Generate observations.\n",
    "We pick a “true” parameter triple (beta, gamma, sigma), run the SEIR simulator once to obtain a reference peak infection rate, and then add Gaussian noise to create multiple noisy observations, stored as observations = {\"infection_rate\": ...}.\n",
    "\n",
    "3. Calibrate with the simulator (Pyro).\n",
    "We specify Uniform priors over beta, gamma, and sigma, call the SEIR simulator inside a Pyro model, and run MCMC using a random walk kernel. This gives posterior samples for all three parameters directly from the simulator.\n",
    "\n",
    "4. Train an emulator and recalibrate.\n",
    "We then train a GaussianProcessRBF emulator on a design of SEIR simulations using AutoEmulate, and perform Bayesian calibration again using BayesianCalibration. This replaces repeated simulator calls with the GP surrogate, which is more scalable for expensive models.\n",
    "\n",
    "5. Compare posteriors and diagnostics.\n",
    "Finally, we convert the simulator– and emulator–based MCMC runs to ArviZ / GetDist objects, plot the joint posteriors, posterior predictive checks, and trace plots, and verify that the emulator reproduces the simulator posterior for the SEIR model while being much cheaper to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from scipy.integrate import solve_ivp\n",
    "\n",
    "from autoemulate.core.types import NumpyLike, TensorLike\n",
    "from autoemulate.simulations.base import Simulator\n",
    "from autoemulate.simulations.seir import SEIRSimulator\n",
    "\n",
    "\n",
    "\n",
    "simulator = SEIRSimulator(log_level=\"error\")\n",
    "x = simulator.sample_inputs(1000)\n",
    "y, _ = simulator.forward_batch(x)\n",
    "\n",
    "# Plot\n",
    "transmission_rate = x[:, 0]\n",
    "recovery_rate = x[:, 1]\n",
    "exposure_rate = x[:, 2]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sc = plt.scatter(transmission_rate, recovery_rate, c=y, cmap='viridis')\n",
    "plt.xlabel('Transmission rate (beta)')\n",
    "plt.ylabel('Recovery rate (gamma)')\n",
    "plt.colorbar(sc, label=\"Peak infection rate\")\n",
    "plt.title(\"SEIR Model Simulation\")\n",
    "plt.show()\n",
    "\n",
    "# Step 2: Observed data\n",
    "true_beta = 0.3\n",
    "true_gamma = 0.15\n",
    "true_sigma = 0.2\n",
    "params = torch.tensor([true_beta, true_gamma, true_sigma]).view(1, -1)\n",
    "true_infection_rate = simulator.forward(params)\n",
    "\n",
    "n_obs = 100\n",
    "stdev = 0.05\n",
    "noise = torch.normal(mean=0, std=stdev, size=(n_obs,))\n",
    "observed_infection_rates = true_infection_rate[0] + noise\n",
    "observations = {\"infection_rate\": observed_infection_rates}\n",
    "\n",
    "# Step 3: Calibrate with simulator using Pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import MCMC\n",
    "from pyro.infer.mcmc import RandomWalkKernel\n",
    "\n",
    "def model():\n",
    "    beta = pyro.sample(\"beta\", dist.Uniform(0.1, 0.5))\n",
    "    gamma = pyro.sample(\"gamma\", dist.Uniform(0.01, 0.2))\n",
    "    sigma = pyro.sample(\"sigma\", dist.Uniform(0.05, 0.5))\n",
    "\n",
    "    mean = simulator.forward(torch.tensor([[beta, gamma, sigma]]))\n",
    "\n",
    "    with pyro.plate(\"data\", n_obs):\n",
    "        pyro.sample(\n",
    "            \"infection_rate\",\n",
    "            dist.Normal(mean, stdev),\n",
    "            obs=observations[\"infection_rate\"],\n",
    "        )\n",
    "\n",
    "kernel = RandomWalkKernel(model, init_step_size=2.5)\n",
    "mcmc_sim = MCMC(kernel, warmup_steps=1500, num_samples=15000, num_chains=1) #might have to increase warmup syeps and num of samples for convergence\n",
    "mcmc_sim.run()\n",
    "\n",
    "sim_samples = mcmc_sim.get_samples()\n",
    "plt.scatter(sim_samples['beta'], sim_samples['gamma'], alpha=0.5)\n",
    "plt.xlabel('Beta')\n",
    "plt.ylabel('Gamma')\n",
    "plt.title('Simulator Posterior Samples')\n",
    "plt.show()\n",
    "\n",
    "# Step 4: Train Emulator and Calibrate\n",
    "from autoemulate.core.compare import AutoEmulate\n",
    "from autoemulate.emulators import GaussianProcessRBF\n",
    "from autoemulate.calibration.bayes import BayesianCalibration\n",
    "\n",
    "ae = AutoEmulate(x, y, models=[GaussianProcessRBF], model_params={}, log_level=\"error\")\n",
    "ae.summarise()\n",
    "gp = ae.best_result().model\n",
    "\n",
    "bc = BayesianCalibration(\n",
    "    gp,\n",
    "    simulator.parameters_range,\n",
    "    observations,\n",
    "    observation_noise=stdev**2,\n",
    ")\n",
    "\n",
    "mcmc_emu = bc.run_mcmc(warmup_steps=250, num_samples=1000, num_chains=2)\n",
    "mcmc_emu.summary()\n",
    "\n",
    "# Step 5: Plotting\n",
    "az_data = bc.to_arviz(mcmc_emu, posterior_predictive=True)\n",
    "_ = az.plot_pair(az_data, kind='kde')\n",
    "_ = az.plot_ppc(az_data)\n",
    "_ = az.plot_trace(az_data, figsize=(20, 8))\n",
    "\n",
    "# Step 6: Compare Posterior Distributions\n",
    "sim_data = BayesianCalibration.to_getdist(mcmc_sim, label=\"Simulator\")\n",
    "emu_data = arviz_to_mcsamples(az_data, dataset_label=\"Emulator\")\n",
    "sim_data.smooth_scale_1D = 0.8\n",
    "emu_data.smooth_scale_1D = 0.8\n",
    "\n",
    "g = plots.get_subplot_plotter()\n",
    "g.triangle_plot(\n",
    "    [sim_data, emu_data],\n",
    "    filled=True,\n",
    "    markers={\"beta\": true_beta, \"gamma\": true_gamma, \"sigma\": true_sigma},\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
