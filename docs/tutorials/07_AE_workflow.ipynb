{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrating a user-provided simulator in an end-to-end AutoEmulate workflow \n",
    "\n",
    "## Overview\n",
    "\n",
    "<b>In this workflow we demonstrate the integration of a Cardiovascular simulator, Naghavi Model from ModularCirc in the end-to-end AutoEmulate workflow.</b> \n",
    "\n",
    "Naghavi model is a 0D (zero-dimensional) computational model of the cardiovascular system, which is used to simulate blood flow and pressure dynamics in the heart and blood vessels.\n",
    "\n",
    "This demo includes:\n",
    "- Setting up parameter ranges \n",
    "- Creating samples\n",
    "- Running the simulator to generate training data for the emulator \n",
    "- Using AutoEmulate to find the best pre-processing technique and model tailored to the simulation data \n",
    "- Applying history matching to refine the model and enhance parameter ranges \n",
    "- Sensitivity Analysis \n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/alan-turing-institute/autoemulate/refs/heads/main/misc/workflow.png\" alt=\"Work Flow\" style=\"width:100%;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional dependency requirements\n",
    "\n",
    "<b>In this demonstration we are using the Naghavi Model Simulator from ModularCirc library. Therefore, the user needs to install the ModularCirc library in their existing AutoEmulate virtual environemnt as an additional dependency.</b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove_output"
    ]
   },
   "outputs": [],
   "source": [
    "# ! pip install git+https://github.com/alan-turing-institute/ModularCirc.git@dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 - Create a dictionary called `parameters_range` which contains the name of the simulator input parameters and their range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ao.r': (120.0, 360.0),\n",
       " 'ao.c': (0.15, 0.44999999999999996),\n",
       " 'art.r': (562.5, 1687.5),\n",
       " 'art.c': (1.5, 4.5),\n",
       " 'ven.r': (4.5, 13.5),\n",
       " 'ven.c': (66.65, 199.95000000000002),\n",
       " 'av.r': (3.0, 9.0),\n",
       " 'mv.r': (2.05, 6.1499999999999995),\n",
       " 'la.E_pas': (0.22, 0.66),\n",
       " 'la.E_act': (0.225, 0.675),\n",
       " 'la.v_ref': (5.0, 15.0),\n",
       " 'la.k_pas': (0.01665, 0.07500000000000001),\n",
       " 'lv.E_pas': (0.5, 1.5),\n",
       " 'lv.E_act': (1.5, 4.5),\n",
       " 'lv.v_ref': (5.0, 15.0),\n",
       " 'lv.k_pas': (0.00999, 0.045)}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autoemulate.simulations.naghavi_cardiac_ModularCirc import extract_parameter_ranges\n",
    "# Usage example:\n",
    "parameters_range = extract_parameter_ranges('../data/naghavi_model_parameters.json')\n",
    "parameters_range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 - Use  `LatinHypercube` method from AutoEmulate to generate initial samples using the parameters range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 16 Number of samples from each parameter: 100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ao.r</th>\n",
       "      <th>ao.c</th>\n",
       "      <th>art.r</th>\n",
       "      <th>art.c</th>\n",
       "      <th>ven.r</th>\n",
       "      <th>ven.c</th>\n",
       "      <th>av.r</th>\n",
       "      <th>mv.r</th>\n",
       "      <th>la.E_pas</th>\n",
       "      <th>la.E_act</th>\n",
       "      <th>la.v_ref</th>\n",
       "      <th>la.k_pas</th>\n",
       "      <th>lv.E_pas</th>\n",
       "      <th>lv.E_act</th>\n",
       "      <th>lv.v_ref</th>\n",
       "      <th>lv.k_pas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>181.183489</td>\n",
       "      <td>0.270318</td>\n",
       "      <td>1603.018317</td>\n",
       "      <td>2.959594</td>\n",
       "      <td>6.003299</td>\n",
       "      <td>195.369110</td>\n",
       "      <td>3.937637</td>\n",
       "      <td>3.884733</td>\n",
       "      <td>0.488983</td>\n",
       "      <td>0.513613</td>\n",
       "      <td>7.894707</td>\n",
       "      <td>0.071994</td>\n",
       "      <td>1.185785</td>\n",
       "      <td>4.021054</td>\n",
       "      <td>8.363500</td>\n",
       "      <td>0.028132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>176.848557</td>\n",
       "      <td>0.351545</td>\n",
       "      <td>909.233665</td>\n",
       "      <td>4.223447</td>\n",
       "      <td>12.895963</td>\n",
       "      <td>183.225533</td>\n",
       "      <td>8.706071</td>\n",
       "      <td>3.621647</td>\n",
       "      <td>0.302796</td>\n",
       "      <td>0.577803</td>\n",
       "      <td>13.784270</td>\n",
       "      <td>0.026378</td>\n",
       "      <td>1.460897</td>\n",
       "      <td>2.891056</td>\n",
       "      <td>12.684470</td>\n",
       "      <td>0.024778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>237.476807</td>\n",
       "      <td>0.167153</td>\n",
       "      <td>1628.006457</td>\n",
       "      <td>2.359281</td>\n",
       "      <td>5.509983</td>\n",
       "      <td>167.411090</td>\n",
       "      <td>5.175940</td>\n",
       "      <td>5.710833</td>\n",
       "      <td>0.578039</td>\n",
       "      <td>0.604609</td>\n",
       "      <td>9.962029</td>\n",
       "      <td>0.065479</td>\n",
       "      <td>1.492188</td>\n",
       "      <td>1.806558</td>\n",
       "      <td>14.004487</td>\n",
       "      <td>0.016258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>277.067184</td>\n",
       "      <td>0.282469</td>\n",
       "      <td>732.473802</td>\n",
       "      <td>3.231590</td>\n",
       "      <td>11.126694</td>\n",
       "      <td>135.807780</td>\n",
       "      <td>4.876715</td>\n",
       "      <td>4.021932</td>\n",
       "      <td>0.644373</td>\n",
       "      <td>0.325448</td>\n",
       "      <td>12.339279</td>\n",
       "      <td>0.058467</td>\n",
       "      <td>0.752356</td>\n",
       "      <td>1.883375</td>\n",
       "      <td>14.215886</td>\n",
       "      <td>0.022485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>272.561340</td>\n",
       "      <td>0.250207</td>\n",
       "      <td>1355.692548</td>\n",
       "      <td>1.860010</td>\n",
       "      <td>13.427768</td>\n",
       "      <td>114.692961</td>\n",
       "      <td>4.424005</td>\n",
       "      <td>2.191801</td>\n",
       "      <td>0.284561</td>\n",
       "      <td>0.282951</td>\n",
       "      <td>12.218897</td>\n",
       "      <td>0.072280</td>\n",
       "      <td>1.394213</td>\n",
       "      <td>2.948728</td>\n",
       "      <td>7.121979</td>\n",
       "      <td>0.032423</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ao.r      ao.c        art.r     art.c      ven.r       ven.c  \\\n",
       "0  181.183489  0.270318  1603.018317  2.959594   6.003299  195.369110   \n",
       "1  176.848557  0.351545   909.233665  4.223447  12.895963  183.225533   \n",
       "2  237.476807  0.167153  1628.006457  2.359281   5.509983  167.411090   \n",
       "3  277.067184  0.282469   732.473802  3.231590  11.126694  135.807780   \n",
       "4  272.561340  0.250207  1355.692548  1.860010  13.427768  114.692961   \n",
       "\n",
       "       av.r      mv.r  la.E_pas  la.E_act   la.v_ref  la.k_pas  lv.E_pas  \\\n",
       "0  3.937637  3.884733  0.488983  0.513613   7.894707  0.071994  1.185785   \n",
       "1  8.706071  3.621647  0.302796  0.577803  13.784270  0.026378  1.460897   \n",
       "2  5.175940  5.710833  0.578039  0.604609   9.962029  0.065479  1.492188   \n",
       "3  4.876715  4.021932  0.644373  0.325448  12.339279  0.058467  0.752356   \n",
       "4  4.424005  2.191801  0.284561  0.282951  12.218897  0.072280  1.394213   \n",
       "\n",
       "   lv.E_act   lv.v_ref  lv.k_pas  \n",
       "0  4.021054   8.363500  0.028132  \n",
       "1  2.891056  12.684470  0.024778  \n",
       "2  1.806558  14.004487  0.016258  \n",
       "3  1.883375  14.215886  0.022485  \n",
       "4  2.948728   7.121979  0.032423  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from autoemulate.experimental_design import LatinHypercube\n",
    "\n",
    "# Generate Latin Hypercube samples\n",
    "N_samples = 100\n",
    "lhd = LatinHypercube(list(parameters_range.values()))\n",
    "sample_array = lhd.sample(N_samples)\n",
    "sample_df = pd.DataFrame(sample_array, columns=parameters_range.keys())\n",
    "\n",
    "print(\"Number of parameters:\", sample_df.shape[1], \"Number of samples from each parameter:\", sample_df.shape[0])\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 - Wrap your Simulator in the AutoEmulate Simulator Base Class.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/alan-turing-institute/autoemulate/refs/heads/main/misc/base_simulator_guid.png\" alt=\"Work Flow\" style=\"width:30%;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoemulate.simulations.naghavi_cardiac_ModularCirc import NaghaviSimulator\n",
    "# Initialize simulator with specific outputs\n",
    "simulator = NaghaviSimulator(\n",
    "    parameters_range=parameters_range, \n",
    "    output_variables=['lv.P_i', 'lv.P_o'],  # Only the ones you're interested in\n",
    "    n_cycles=300, \n",
    "    dt=0.001,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 - Run the simulator using `run_batch_simulations` to obtain data for training AutoEmulate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff7b36e46e9044f896912a344d6055b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running simulations:   0%|          | 0/100 [00:00<?, ?sample/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = True\n",
    "save = False\n",
    "read = False\n",
    "if run:\n",
    "    # Run batch simulations with the samples generated in Cell 1\n",
    "    results = simulator.run_batch_simulations(sample_df)\n",
    "\n",
    "    # Convert results to DataFrame for analysis\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "if save:\n",
    "    # Save the results to a CSV file\n",
    "    results_df.to_csv('../data/simulator_results.csv', index=False)\n",
    "\n",
    "if read:\n",
    "    # Read the results from the CSV file\n",
    "    results_df = pd.read_csv('../data/simulator_results.csv')\n",
    "    results = results_df.to_numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Note that the first 4 steps can be replaced by having stored the output of your simulation in a file and then reading them in to a dataframe. However the purpose of this article is to demonstrate the use of a User-provided simulator in an end-to-end workflow.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "simulator.output_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Test your simulator with our test function to make sure it is compatible with AutoEmulate pipeline (Feature not provided yet).</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should be replaced with a test written specically to test the simulator written by the user\n",
    "# ! pytest ../../tests/test_base_simulator.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 - Setup AutoEmulate.\n",
    "- User should choose from the available target `pre-processing` methods the methods they would like to investigate.\n",
    "- User should choose from the available `models` the `models` they would like to investigate.\n",
    "- Setup AutoEmulate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from autoemulate.compare import AutoEmulate\n",
    "from autoemulate.plotting import _predict_with_optional_std\n",
    "\n",
    "\n",
    "preprocessing_methods = [{\"name\" : \"PCA\", \"params\" : {\"reduced_dim\": 2}}]\n",
    "em = AutoEmulate()\n",
    "em.setup(sample_df, results, models=[\"gp\"], scale_output = True, reduce_dim_output=True, preprocessing_methods=preprocessing_methods)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6 - Run compare to train AutoEmulate and extract the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = em.compare()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7 - Examine the summary of cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em.summarise_cv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8 - Extract the desired model, run evaluation and refit using the whole dataset.\n",
    "- You can use the `best_model` selected by AutoEmulate \n",
    "- or you can extract the model and pre-processing technique displayed in `em.summarise_cv()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = em.get_model('GaussianProcess')\n",
    "em.evaluate(gp)\n",
    "# for best model change the line above to:\n",
    "# em.evaluate(best_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_final = em.refit(gp)\n",
    "gp_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em.plot_eval(gp_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9 - Sensitivity Analysis \n",
    "Use AutoEmulate to perform sensitivity analysis. This will help identify the parameters that have higher impact on the outputs to narrow down the search space for performing model calibration. \n",
    "\n",
    "Sobol Interpretation:\n",
    "\n",
    "- $S_1$ values sum to â‰¤ 1.0 (exact fraction of variance explained)\n",
    "- $S_t - S_1$ = interaction effects involving that parameter\n",
    "- Large $S_t - S_1$ gap indicates strong interactions\n",
    "\n",
    "Morris Interpretation:\n",
    "\n",
    "- High $\\mu^*$, Low $\\sigma$: Important parameter with linear/monotonic effects\n",
    "- High $\\mu^*$, High $\\sigma$: Important parameter with non-linear effects or interactions\n",
    "- Low $\\mu^*$, High $\\sigma$: Parameter involved in interactions but not individually important\n",
    "- Low $\\mu^*$, Low $\\sigma$: Unimportant parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract parameter names and bounds from the dictionary\n",
    "parameter_names = list(parameters_range.keys())\n",
    "parameter_bounds = list(parameters_range.values())\n",
    "\n",
    "# Define the problem dictionary for Sobol sensitivity analysis\n",
    "problem = {\n",
    "    'num_vars': len(parameter_names),\n",
    "    'names': parameter_names,\n",
    "    'bounds': parameter_bounds\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "si = em.sensitivity_analysis(problem=problem, method='morris')\n",
    "si"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em.plot_sensitivity_analysis(si)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10 - History Matching\n",
    " \n",
    "Once you have the final model, running history matching can improve your model. The Implausibility metric is calculated using the following relation for each set of parameter:\n",
    "\n",
    "$I_i(\\overline{x_0}) = \\frac{|z_i - \\mathbb{E}(f_i(\\overline{x_0}))|}{\\sqrt{\\text{Var}[z_i - \\mathbb{E}(f_i(\\overline{x_0}))]}}$\n",
    "Where if implosibility ($I_i$) exceeds a threshhold value, the points will be rulled out. \n",
    "The outcome of history matching are the NORY (Not Ruled Out Yet) and RO (Ruled Out) points.\n",
    "\n",
    "- create a dictionary of your observations, this should match the output names of your simulator \n",
    "- create the history matching object \n",
    "- run history matching \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoemulate.history_matching import HistoryMatching\n",
    "\n",
    "# Define observed data with means and variances\n",
    "observations = {\n",
    "    'lv.P_i_min': (5.0, 0.1),   # Minimum of minimum LV pressure\n",
    "    'lv.P_i_max': (20.0, 0.1),   # Maximum of minimum LV pressure\n",
    "    'lv.P_i_mean': (10.0, 0.1),  # Mean of minimum LV pressure\n",
    "    'lv.P_i_range': (15.0, 0.5), # Range of minimum LV pressure\n",
    "    'lv.P_o_min': (1.0, 0.1),  # Minimum of maximum LV pressure\n",
    "    'lv.P_o_max': (13.0, 0.1),  # Maximum of maximum LV pressure\n",
    "    'lv.P_o_mean': (12.0, 0.1), # Mean of maximum LV pressure\n",
    "    'lv.P_o_range': (20.0, 0.5)  # Range of maximum LV pressure\n",
    "}\n",
    "\n",
    "# Create history matcher\n",
    "hm = HistoryMatching(\n",
    "    simulator=simulator,\n",
    "    observations=observations,\n",
    "    threshold=3.0\n",
    ")\n",
    "\n",
    "# Run history matching\n",
    "all_samples, all_impl_scores, emulator = hm.run(\n",
    "    n_waves=30,\n",
    "    n_samples_per_wave=100,\n",
    "    emulator_predict=True,\n",
    "    initial_emulator=gp_final,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em.plot_eval(emulator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10 - use the interactive dashboard to inspect the results of history matching \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoemulate.history_matching_dashboard import HistoryMatchingDashboard\n",
    "dashboard = HistoryMatchingDashboard(\n",
    "    samples=all_samples,\n",
    "    impl_scores=all_impl_scores,\n",
    "    param_names=simulator.param_names,  \n",
    "    output_names=simulator.output_names, \n",
    "    )\n",
    "dashboard.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\n",
    "\n",
    "\n",
    "\"https://raw.githubusercontent.com/alan-turing-institute/autoemulate/refs/heads/main/misc/vis_dashboard_pic_sample.png\" alt=\"Work Flow\" style=\"width:100%;\"/> \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Footnote: Testing the dashboard\n",
    "\n",
    "Sometimes it is hard to know, if the results we are seeing is because the code is not working, or our simulation results are more interesting than we expected. Here is a little test dataset which tests the dashboard, so that you can see how the plots are supposed to look liek and what they shouldf show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test sample with KNOWN NROY regions\n",
    "test_samples = np.array([[x, y] for x in np.linspace(0,1,100) \n",
    "                               for y in np.linspace(0,1,100)])\n",
    "test_scores = (abs(test_samples[:, 0]-0.5)+abs(test_samples[:, 1]-0.5)).reshape(-1, 1)\n",
    "\n",
    "# Should show a clear diagonal pattern\n",
    "test_dash = HistoryMatchingDashboard(\n",
    "    samples=test_samples,\n",
    "    impl_scores=test_scores,\n",
    "    param_names=[\"p1\", \"p2\"],\n",
    "    output_names=[\"out1\"],\n",
    "    threshold=0.7  # ~50% of points should be NROY\n",
    ")\n",
    "#test_dash.display()"
   ]
  }
 ],
 "metadata": {
  "jupyter": {
   "tags": [
    "skip-execution"
   ]
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
