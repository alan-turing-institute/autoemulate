{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrating a user-provided simulator in an end-to-end AutoEmulate workflow \n",
    "\n",
    "## Overview\n",
    "\n",
    "<b>In this workflow we demonstrate the integration of a Cardiovascular simulator, Naghavi Model from ModularCirc in the end-to-end AutoEmulate workflow.</b> \n",
    "\n",
    "Naghavi model is a 0D (zero-dimensional) computational model of the cardiovascular system, which is used to simulate blood flow and pressure dynamics in the heart and blood vessels.\n",
    "\n",
    "This demo includes:\n",
    "- Setting up parameter ranges \n",
    "- Creating samples\n",
    "- Running the simulator to generate training data for the emulator \n",
    "- Using AutoEmulate to find the best pre-processing technique and model tailored to the simulation data \n",
    "- Applying history matching to refine the model and enhance parameter ranges \n",
    "- Sensitivity Analysis \n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/alan-turing-institute/autoemulate/refs/heads/main/misc/workflow.png\" alt=\"Work Flow\" style=\"width:100%;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional dependency requirements\n",
    "\n",
    "<b>In this demonstration we are using the Naghavi Model Simulator from ModularCirc library. Therefore, the user needs to install the ModularCirc library in their existing AutoEmulate virtual environemnt as an additional dependency.</b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove_output"
    ]
   },
   "outputs": [],
   "source": [
    "#! pip install git+https://github.com/alan-turing-institute/ModularCirc.git@dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 - Create a dictionary called `parameters_range` which contains the name of the simulator input parameters and their range. In this case, we have an imported function to do this. \n",
    "\n",
    "The `parameter_range` dictionary is a string to tuple mapping where the string is the name of the parameter and the tuple is the range of the parameter. The range is defined as a tuple of two values, the minimum and maximum value of the parameter. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/edwin/anaconda3/envs/autoemulate/lib/python3.12/site-packages/pandera/engines/pandas_engine.py:67: UserWarning: Using typeguard < 3. Generic types like List[TYPE], Dict[TYPE, TYPE] will only validate the first element in the collection.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ao.r': (120.0, 360.0),\n",
       " 'ao.c': (0.15, 0.44999999999999996),\n",
       " 'art.r': (562.5, 1687.5),\n",
       " 'art.c': (1.5, 4.5),\n",
       " 'ven.r': (4.5, 13.5),\n",
       " 'ven.c': (66.65, 199.95000000000002),\n",
       " 'av.r': (3.0, 9.0),\n",
       " 'mv.r': (2.05, 6.1499999999999995),\n",
       " 'la.E_pas': (0.22, 0.66),\n",
       " 'la.E_act': (0.225, 0.675),\n",
       " 'la.v_ref': (5.0, 15.0),\n",
       " 'la.k_pas': (0.01665, 0.07500000000000001),\n",
       " 'lv.E_pas': (0.5, 1.5),\n",
       " 'lv.E_act': (1.5, 4.5),\n",
       " 'lv.v_ref': (5.0, 15.0),\n",
       " 'lv.k_pas': (0.00999, 0.045)}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autoemulate.simulations.naghavi_cardiac_ModularCirc import extract_parameter_ranges\n",
    "# Usage example:\n",
    "parameters_range = extract_parameter_ranges('../data/naghavi_model_parameters.json')\n",
    "parameters_range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 - Wrap your Simulator in the AutoEmulate Simulator Base Class.\n",
    "\n",
    "Autoemulate has a Simulator class that all simulators should inherit from. This class has one abstract method that the user must implement called _forward. This method is responsible for accepting some input parameters and outputting the results from a single simulation results. Both the input and output of this method should be an 2D array/Tensor where the dimensions are (n_samples, n_features).\n",
    "\n",
    "\n",
    "Please refer to the [Custom Simulators](https://autoemulate.readthedocs.io/en/latest/tutorials/08_Custom_Simulations.html) tutorial for more information on how to wrap your simulator in the AutoEmulate Simulator Base Class. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import X\n",
    "from sys import exception\n",
    "from autoemulate.experimental.simulations.base import Simulator\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from ModularCirc.Models.NaghaviModel import NaghaviModel\n",
    "from ModularCirc.Models.NaghaviModel import NaghaviModelParameters\n",
    "from ModularCirc.Solver import Solver\n",
    "\n",
    "from autoemulate.experimental.sensitivity_analysis import SensitivityAnalysis\n",
    "\n",
    "class NaghaviSimulator(Simulator):\n",
    "    def __init__(\n",
    "        self,\n",
    "        parameters_range,\n",
    "        output_names,\n",
    "        n_cycles: int = 40,\n",
    "        dt: float = 0.001,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the Naghavi simulator with specific parameters.\n",
    "        Some default parameter ranges can be found\n",
    "        autoemulate.simulations.naghavi_cardiac_ModularCirc.py\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        parameters_range : dict\n",
    "            Dictionary mapping parameter names to (min, max) tuples.\n",
    "        output_names : list\n",
    "            List of specific output names to track.\n",
    "        n_cycles : int\n",
    "            Number of simulation cycles.\n",
    "        dt : float\n",
    "            Time step size.\n",
    "        \"\"\"\n",
    "        super().__init__(parameters_range, output_names)\n",
    "\n",
    "        # Naghavi-specific attributes\n",
    "        self.n_cycles = n_cycles\n",
    "        self.dt = dt\n",
    "        self.time_setup = {\n",
    "            \"name\": \"HistoryMatching\",\n",
    "            \"ncycles\": n_cycles,\n",
    "            \"tcycle\": 1.0,\n",
    "            \"dt\": dt,\n",
    "            \"export_min\": 1,\n",
    "        }\n",
    "\n",
    "    def _forward(self, x):\n",
    "        \"\"\"\n",
    "        Run a single Naghavi model simulation and return output statistics.\n",
    "\n",
    "        Args:\n",
    "            x: TensorLike\n",
    "                Input parameters for the simulation\n",
    "\n",
    "        Returns:\n",
    "            Array of output statistics or None if simulation fails\n",
    "        \"\"\"\n",
    "        if x.shape[-1] != len(self._param_names):\n",
    "            raise ValueError(\n",
    "                f\"Input x must have the same shape as the number of parameters:\"\n",
    "                f\" {len(self._param_names)}\"\n",
    "            )\n",
    "\n",
    "        # Drop first dim\n",
    "        x = x.squeeze(0)\n",
    "\n",
    "        # Set parameter object\n",
    "        parobj = NaghaviModelParameters()\n",
    "\n",
    "        for i, param_name in enumerate(self._param_names):\n",
    "            if param_name == \"T\":\n",
    "                continue\n",
    "\n",
    "            obj, param = param_name.split(\".\")\n",
    "            value = x[i].numpy()\n",
    "            parobj._set_comp(obj, [obj], **{param: value})\n",
    "\n",
    "        # Set cycle time\n",
    "        t_cycle = (\n",
    "            x[self.get_parameter_idx(\"T\")].item() if \"T\" in self._param_names else 1.0\n",
    "        )\n",
    "\n",
    "        self.time_setup[\"tcycle\"] = t_cycle\n",
    "\n",
    "        # Run simulation\n",
    "        model = NaghaviModel(\n",
    "            time_setup_dict=self.time_setup, parobj=parobj, suppress_printing=True\n",
    "        )\n",
    "        solver = Solver(model=model)\n",
    "        solver.setup(suppress_output=True, optimize_secondary_sv=False, method=\"LSODA\")\n",
    "        solver.solve()\n",
    "\n",
    "        if not solver.converged:\n",
    "            err_msg = \"Solver did not converge\"\n",
    "            raise Exception(err_msg)\n",
    "\n",
    "        # Collect and process outputs\n",
    "        output_stats = []\n",
    "        output_names = []\n",
    "\n",
    "        for component_name, component_obj in model.components.items():\n",
    "            for attr_name in dir(component_obj):\n",
    "                if (\n",
    "                    not attr_name.startswith(\"_\")\n",
    "                    and attr_name != \"kwargs\"\n",
    "                    and not callable(getattr(component_obj, attr_name))\n",
    "                ):\n",
    "                    try:\n",
    "                        attr = getattr(component_obj, attr_name)\n",
    "                        if hasattr(attr, \"values\"):\n",
    "                            full_name = f\"{component_name}.{attr_name}\"\n",
    "\n",
    "                            # Check if we should track this variable\n",
    "                            if (\n",
    "                                not self.output_names\n",
    "                                or full_name in self.output_names\n",
    "                            ):\n",
    "                                values = np.array(attr.values)\n",
    "\n",
    "                                # Use the base class method to calculate statistics\n",
    "                                stats, stat_names = self._calculate_output_stats(\n",
    "                                    values, full_name\n",
    "                                )\n",
    "                                output_stats.extend(stats)\n",
    "                                output_names.extend(stat_names)\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                        continue\n",
    "\n",
    "        # Always update output names after the first simulation\n",
    "        if not self._has_sample_forward:\n",
    "            #self._output_names = output_names\n",
    "            self._has_sample_forward = True\n",
    "\n",
    "        # Convert output stats to a tensor\n",
    "        return torch.Tensor(output_stats).unsqueeze(0)\n",
    "\n",
    "    def _calculate_output_stats(\n",
    "        self, output_values: np.ndarray, base_name: str\n",
    "    ) -> tuple[np.ndarray, list[str]]:\n",
    "        \"\"\"\n",
    "        Calculate statistics for an output time series.\n",
    "\n",
    "        Args:\n",
    "            output_values: Array of time series values\n",
    "            base_name: Base name of the output variable\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (stats_array, stat_names)\n",
    "        \"\"\"\n",
    "        stats = np.array(\n",
    "            [\n",
    "                np.min(output_values),\n",
    "                np.max(output_values),\n",
    "                np.mean(output_values),\n",
    "                np.max(output_values) - np.min(output_values),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        stat_names = [\n",
    "            f\"{base_name}_min\",\n",
    "            f\"{base_name}_max\",\n",
    "            f\"{base_name}_mean\",\n",
    "            f\"{base_name}_range\",\n",
    "        ]\n",
    "\n",
    "        return stats, stat_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize simulator with specific outputs\n",
    "simulator = NaghaviSimulator(\n",
    "    parameters_range=parameters_range,\n",
    "    output_names=['lv.P_i', 'lv.P_o'],  # Only the ones you're interested in\n",
    "    n_cycles=300,\n",
    "    dt=0.001,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Generate input samples from given parameter ranges.\n",
    "\n",
    "The Simulator class has a built-in method for generating input samples from the given parameter ranges using Latin Hypercube Sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 16])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_samples = simulator.sample_inputs(\n",
    "    n_samples=20,\n",
    ")\n",
    "input_samples.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the shape of the input samples is (n_samples, n_parameters).\n",
    "\n",
    "There is a useful function called get_parameter_idx that will return the parameter index for a specific parameter name. For example, the code below shows how to extract from the input samples the value of the parameter la.E_pas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4081, 0.4756, 0.6338, 0.3171, 0.5651, 0.2833, 0.5821, 0.5271, 0.2391,\n",
       "        0.4874, 0.3396, 0.4263, 0.5345, 0.3521, 0.3958, 0.6419, 0.6044, 0.2576,\n",
       "        0.4436, 0.3080])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_idx = simulator.get_parameter_idx('la.E_pas')\n",
    "input_samples[:, param_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 - The simulator class can now be run either by a single time using the forward method or by using run_batch_simulations to obtain data for training AutoEmulate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single forward simulation output shape: torch.Size([1, 8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running simulations: 100%|██████████| 20/20 [00:33<00:00,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully completed 20/20 simulations (100.0%)\n",
      "Batch forward simulation output shape: torch.Size([20, 8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run a single simulation - NOTE the input sample must be a 2D array.\n",
    "single_output = simulator.forward(input_samples[0:1, :])\n",
    "print(f\"Single forward simulation output shape: {single_output.shape}\")\n",
    "\n",
    "batch_output = simulator.forward_batch(input_samples)\n",
    "print(f\"Batch forward simulation output shape: {batch_output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 16])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 8])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 - Run AutoEmulate.\n",
    "\n",
    "We will now run initialise the main AutoEmulate class. To initialise the class, we need to pass the input and output samples and the type of models we would like to train as emulators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from autoemulate.experimental.compare import AutoEmulate\n",
    "from autoemulate.experimental.emulators import ALL_EMULATORS\n",
    "\n",
    "em = AutoEmulate(x=input_samples, y=batch_output, models=[ALL_EMULATORS[0], ALL_EMULATORS[4], ALL_EMULATORS[5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6 - Run compare to train AutoEmulate and extract the best model.\n",
    "\n",
    "We can now run the compare method to train AutoEmulate and extract the best model. The compare method will return the best model based on the given input and output samples. Under the hood, the following steps are performed for each model type:\n",
    "\n",
    "1. Tune the model hyperparameters using the input and output samples.\n",
    "2. Train the model using the input and output samples with cross-validation to get a score for each model.\n",
    "3. Returns a dictionary with all models and their scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:10<00:00,  3.57s/it]\n",
      "100%|██████████| 3/3 [00:00<00:00, 10.01it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 83.58it/s]\n"
     ]
    }
   ],
   "source": [
    "model_scores = em.compare(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`model_scores` holds the performance scores as well as the parameters the model was tuned to optimise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'GaussianProcessExact': {'config': {'mean_module_fn': <function autoemulate.emulators.gaussian_process.poly_mean(n_features, n_outputs)>,\n",
       "   'covar_module_fn': <function autoemulate.emulators.gaussian_process.rbf(n_features, n_outputs)>,\n",
       "   'epochs': 1000,\n",
       "   'batch_size': 16,\n",
       "   'activation': torch.nn.modules.activation.ReLU,\n",
       "   'lr': 0.006551285568595509,\n",
       "   'preprocessor_cls': None,\n",
       "   'likelihood_cls': gpytorch.likelihoods.multitask_gaussian_likelihood.MultitaskGaussianLikelihood},\n",
       "  'r2_score': -5.600991225242614,\n",
       "  'rmse_score': 56197.28505657627},\n",
       " 'RandomForest': {'config': {'n_estimators': 287,\n",
       "   'min_samples_split': 4,\n",
       "   'min_samples_leaf': 6,\n",
       "   'max_features': 'log2',\n",
       "   'bootstrap': False,\n",
       "   'oob_score': False,\n",
       "   'max_depth': 20,\n",
       "   'max_samples': 0.9},\n",
       "  'r2_score': -138.89365921020507,\n",
       "  'rmse_score': 12.622238472530475},\n",
       " 'MLP': {'config': {'epochs': 200,\n",
       "   'layer_dims': [64, 32, 16],\n",
       "   'lr': 0.1,\n",
       "   'batch_size': 16,\n",
       "   'weight_init': 'default',\n",
       "   'scale': 0.1,\n",
       "   'bias_init': 'default',\n",
       "   'dropout_prob': 0.5},\n",
       "  'r2_score': -5039.851953125,\n",
       "  'rmse_score': 21.996197854532955}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7 - Examine the summary of cross-validation.\n",
    "\n",
    "We can now loop through the model scores and print the summary of cross-validation scores for each model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: GaussianProcessExact\n",
      "rmse_score: 56197.28505657627\n",
      "r2_score: -5.600991225242614\n",
      "Model: RandomForest\n",
      "rmse_score: 12.622238472530475\n",
      "r2_score: -138.89365921020507\n",
      "Model: MLP\n",
      "rmse_score: 21.996197854532955\n",
      "r2_score: -5039.851953125\n"
     ]
    }
   ],
   "source": [
    "for model_name, scores in model_scores.items():\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"rmse_score: {scores['rmse_score']}\")\n",
    "    print(f\"r2_score: {scores['r2_score']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8 - Sensitivity Analysis \n",
    "Use AutoEmulate to perform sensitivity analysis. This will help identify the parameters that have higher impact on the outputs to narrow down the search space for performing model calibration. \n",
    "\n",
    "Sobol Interpretation:\n",
    "\n",
    "- $S_1$ values sum to ≤ 1.0 (exact fraction of variance explained)\n",
    "- $S_t - S_1$ = interaction effects involving that parameter\n",
    "- Large $S_t - S_1$ gap indicates strong interactions\n",
    "\n",
    "Morris Interpretation:\n",
    "\n",
    "- High $\\mu^*$, Low $\\sigma$: Important parameter with linear/monotonic effects\n",
    "- High $\\mu^*$, High $\\sigma$: Important parameter with non-linear effects or interactions\n",
    "- Low $\\mu^*$, High $\\sigma$: Parameter involved in interactions but not individually important\n",
    "- Low $\\mu^*$, Low $\\sigma$: Unimportant parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract parameter names and bounds from the dictionary\n",
    "parameter_names = simulator.param_names\n",
    "parameter_bounds = simulator.param_bounds\n",
    "\n",
    "# Define the problem dictionary for Sobol sensitivity analysis\n",
    "problem = {\n",
    "    'num_vars': len(parameter_names),\n",
    "    'names': parameter_names,\n",
    "    'bounds': parameter_bounds\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm = em.models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "si = SensitivityAnalysis(emulator=gm, problem=problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Emulator.predict() missing 1 required positional argument: 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43msi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msobol\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/alan-turing-institute/autoemulate/autoemulate/experimental/sensitivity_analysis.py:203\u001b[39m, in \u001b[36mSensitivityAnalysis.run\u001b[39m\u001b[34m(self, method, n_samples, conf_level)\u001b[39m\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m    202\u001b[39m param_samples = \u001b[38;5;28mself\u001b[39m._sample(method, n_samples)\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m y = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m output_names = \u001b[38;5;28mself\u001b[39m._get_output_names(y.shape[\u001b[32m1\u001b[39m])\n\u001b[32m    206\u001b[39m results = {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/alan-turing-institute/autoemulate/autoemulate/experimental/sensitivity_analysis.py:134\u001b[39m, in \u001b[36mSensitivityAnalysis._predict\u001b[39m\u001b[34m(self, param_samples)\u001b[39m\n\u001b[32m    132\u001b[39m param_tensor = \u001b[38;5;28mself\u001b[39m._convert_to_tensors(param_samples)\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(param_tensor, TensorLike)\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m y_pred = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43memulator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[38;5;66;03m# handle types, convert to numpy\u001b[39;00m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(y_pred, TensorLike):\n",
      "\u001b[31mTypeError\u001b[39m: Emulator.predict() missing 1 required positional argument: 'x'"
     ]
    }
   ],
   "source": [
    "si.run(method='sobol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em.plot_sensitivity_analysis(si)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refining the Model with Real-World Observations\n",
    "\n",
    "To refine our emulator, we need real-world observations to compare against. These observations can come from:\n",
    "- Experimental values from literature\n",
    "- Simulation results from a known reliable parameter set\n",
    "\n",
    "In this example, we'll generate our observations by running the simulator at the midpoint of each parameter range, treating these as our \"ground truth\" values for calibration. Note that in a real world example one can have multiple observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# An example of how to define observed data with means and variances from a hypothetical experiment\n",
    "observations = {\n",
    "    'lv.P_i_min': (5.0, 0.1),   # Minimum of minimum LV pressure\n",
    "    'lv.P_i_max': (20.0, 0.1),   # Maximum of minimum LV pressure\n",
    "    'lv.P_i_mean': (10.0, 0.1),  # Mean of minimum LV pressure\n",
    "    'lv.P_i_range': (15.0, 0.5), # Range of minimum LV pressure\n",
    "    'lv.P_o_min': (1.0, 0.1),  # Minimum of maximum LV pressure\n",
    "    'lv.P_o_max': (13.0, 0.1),  # Maximum of maximum LV pressure\n",
    "    'lv.P_o_mean': (12.0, 0.1), # Mean of maximum LV pressure\n",
    "    'lv.P_o_range': (20.0, 0.5)  # Range of maximum LV pressure\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otherwise, use one forward pass of your simualtion to get the observed data\n",
    "# Calculate midpoint parameters\n",
    "midpoint_params = {}\n",
    "for param_name, (min_val, max_val) in parameters_range.items():\n",
    "    midpoint_params[param_name] = (min_val + max_val) / 2.0\n",
    "# Run the simulator with midpoint parameters\n",
    "midpoint_results = simulator.sample_forward(midpoint_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create observations dictionary\n",
    "observations = {}\n",
    "output_names = simulator.output_names\n",
    "observations = {name: (float(val), max(abs(val) * 0.01, 0.01)) for name, val in zip(output_names, midpoint_results)}\n",
    "observations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10 - History Matching\n",
    " \n",
    "Once you have the final model, running history matching can improve your model. The Implausibility metric is calculated using the following relation for each set of parameter:\n",
    "\n",
    "$I_i(\\overline{x_0}) = \\frac{|z_i - \\mathbb{E}(f_i(\\overline{x_0}))|}{\\sqrt{\\text{Var}[z_i - \\mathbb{E}(f_i(\\overline{x_0}))]}}$\n",
    "Where if implosibility ($I_i$) exceeds a threshhold value, the points will be rulled out. \n",
    "The outcome of history matching are the NORY (Not Ruled Out Yet) and RO (Ruled Out) points.\n",
    "\n",
    "- create a dictionary of your observations, this should match the output names of your simulator \n",
    "- create the history matching object \n",
    "- run history matching \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoemulate.history_matching import HistoryMatching\n",
    "\n",
    "# Create history matcher\n",
    "hm = HistoryMatching(\n",
    "    simulator=simulator,\n",
    "    observations=observations,\n",
    "    threshold=1.0\n",
    ")\n",
    "\n",
    "# Run history matching\n",
    "all_samples, all_impl_scores, emulator = hm.run(\n",
    "    n_waves=50,\n",
    "    n_samples_per_wave=100,\n",
    "    emulator_predict=True,\n",
    "    initial_emulator=gp_final,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Simple NROY extraction - just check the threshold!\n",
    "threshold = 1.0  # Same threshold used in history matching\n",
    "\n",
    "# Find samples where ALL outputs have implausibility <= threshold\n",
    "nroy_mask = np.all(all_impl_scores <= threshold, axis=1)\n",
    "nroy_indices = np.where(nroy_mask)[0]\n",
    "nroy_samples = all_samples[nroy_indices]\n",
    "\n",
    "print(f\"Total samples: {len(all_samples)}\")\n",
    "print(f\"NROY samples: {len(nroy_samples)}\")\n",
    "print(f\"NROY percentage: {len(nroy_samples)/len(all_samples)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoemulate.history_matching_dashboard import HistoryMatchingDashboard\n",
    "dashboard = HistoryMatchingDashboard(\n",
    "    samples=all_samples,\n",
    "    impl_scores=all_impl_scores,\n",
    "    param_names=simulator.param_names,\n",
    "    output_names=simulator.output_names,\n",
    "    )\n",
    "dashboard.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/alan-turing-institute/autoemulate/refs/heads/main/misc/vis_dashboard_pic_sample.png\" alt=\"Work Flow\" style=\"width:20%;\"/> \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11 - MCMC\n",
    "Once you have identified the important parameters through the Sensitivity analysis tool, the MCMC module can return the calibrated parameter values with uncertainty. \n",
    "The MCMC algorithm tries to find parameter values that match the predictions by the emulator to your `observations` whilst staying within the `parameters_range` (priors)\n",
    "and accounting for uncertainty.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Takes a pre-trained emulator (surrogate model)\n",
    "- Uses sensitivity analysis results to identify the most important parameters\n",
    "- Accepts observations (real data) to calibrate against\n",
    "- Optionally incorporates NROY (Not Ruled Out Yet) samples from prior history matching\n",
    "- Sets up parameter bounds for calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoemulate.mcmc import MCMCCalibrator\n",
    "# Define your observations (what you want to match)\n",
    "# Define observed data with means and variances\n",
    "\n",
    "\n",
    "# Run calibration\n",
    "calibrator = MCMCCalibrator(\n",
    "    emulator=gp_final,\n",
    "    sensitivity_results=si,\n",
    "    observations=observations,\n",
    "    parameter_bounds=parameters_range,\n",
    "    nroy_samples=nroy_samples,\n",
    "    nroy_indices=nroy_indices,\n",
    "    all_samples=all_samples,\n",
    "    top_n_params=3  # Calibrate top 5 most sensitive parameters\n",
    ")\n",
    "\n",
    "results = calibrator.run_mcmc(num_samples=100, warmup_steps=10)\n",
    "# Get calibrated parameter values\n",
    "calibrated_params = calibrator.get_calibrated_parameters()\n",
    "calibrated_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoemulate.mcmc_dashboard import MCMCVisualizationDashboard\n",
    "dashboard = MCMCVisualizationDashboard(calibrator)\n",
    "dashboard.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Footnote: Testing the dashboard\n",
    "\n",
    "Sometimes it is hard to know, if the results we are seeing is because the code is not working, or our simulation results are more interesting than we expected. Here is a little test dataset which tests the dashboard, so that you can see how the plots are supposed to look liek and what they shouldf show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test sample with KNOWN NROY regions\n",
    "test_samples = np.array([[x, y] for x in np.linspace(0,1,100)\n",
    "                               for y in np.linspace(0,1,100)])\n",
    "test_scores = (abs(test_samples[:, 0]-0.5)+abs(test_samples[:, 1]-0.5)).reshape(-1, 1)\n",
    "\n",
    "# Should show a clear diagonal pattern\n",
    "test_dash = HistoryMatchingDashboard(\n",
    "    samples=test_samples,\n",
    "    impl_scores=test_scores,\n",
    "    param_names=[\"p1\", \"p2\"],\n",
    "    output_names=[\"out1\"],\n",
    "    threshold=0.7  # ~50% of points should be NROY\n",
    ")\n",
    "#test_dash.display()"
   ]
  }
 ],
 "metadata": {
  "jupyter": {
   "tags": [
    "skip-execution"
   ]
  },
  "kernelspec": {
   "display_name": "autoemulate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
