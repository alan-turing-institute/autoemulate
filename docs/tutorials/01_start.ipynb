{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A deeper dive into `AutoEmulate`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why `AutoEmulate`\n",
    "\n",
    "Simulations of real-world physical, chemical or biological processes can be complex and computationally expensive, and will often need high-performance computing resources and a lot of time to run. This becomes a problem when simulations need to be run thousands or tens of thousands of times to do uncertainty quantification or sensitivity analysis. It's also a problem when a simulation needs to run fast enough to be useful in real-world or real-time applications, such as digital twins. \n",
    "\n",
    "Emulator models are drop-in replacements for complex simulations, and can be orders of magnitude faster. Any model could be an emulator in principle, from a simple linear regression to Gaussian Processes to Neural Networks. Evaluating all these models requires time and machine learning expertise. `AutoEmulate` is designed to automate the process of finding a good emulator model for a simulation. \n",
    "\n",
    "In the background, `AutoEmulate` does input processing, cross-validation, hyperparameter optimization and model selection. It's different from typical AutoML packages, as the choice of models and hyperparameter search spaces is optimised for typical emulation problems. Over time, we expect the package to further adapt to the emulation needs of the community."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical workflow will involve   \n",
    "1. generating data from a simulation  \n",
    "2. running `AutoEmulate`   \n",
    "3. summarising cross-validation results   \n",
    "4. refitting the emulator model on the full data \n",
    "5. saving and loading the emulator.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from autoemulate.experimental_design import LatinHypercube\n",
    "from autoemulate.simulations.epidemic import simulate_epidemic\n",
    "from autoemulate.compare import AutoEmulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Experimental Design (Sampling)\n",
    "\n",
    "Let's first generate a set of inputs/outputs.  We'll use a simple simulator for the spread of infectious diseases based on the SIR model (see [here](https://en.wikipedia.org/wiki/Compartmental_models_in_epidemiology) for details). For simplicity, we assume a population size of 1000 individuals and start with 1 infected person. The simulator takes two inputs, the transmission rate per day and the recovery rate per day and outputs the peak infection rate. \n",
    "\n",
    "We sample 200 sets of inputs `X` using a Latin Hypercube and run the simulator for those inputs to get a vector of outputs `y`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "beta = (0.1, 0.5) # lower and upper bounds for the transmission rate\n",
    "gamma = (0.01, 0.2) # lower and upper bounds for the recovery rate\n",
    "lhd = LatinHypercube([beta, gamma])\n",
    "X = lhd.sample(200)\n",
    "y = np.array([simulate_epidemic(x) for x in X])\n",
    "\n",
    "print(f\"shapes: input X: {X.shape}, output y: {y.shape}\\n\")\n",
    "print(f\"X: {np.round(X[:3], 2)}\\n\")\n",
    "print(f\"y: {np.round(y[:3], 2)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general shape of the input data is a matrix `X` and a matrix (or vector) `y` where each row represents one run of the simulation and each column is a different input parameter. So in the example above, `0.29` and `0.18` are the two input parameters `beta` and `gamma` for the first run of the simulation, and `0.09` (peak transmission rate) is the output.\n",
    "\n",
    "**Note**: The package currently only works with scalar inputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now plot the simulated data to see how the pattern looks like. As we might guess intuitively, the peak infection rate is higher when the transmission rate increases and the recovery rate decreases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transmission_rate = X[:, 0]\n",
    "recovery_rate = X[:, 1]\n",
    "\n",
    "plt.scatter(transmission_rate, recovery_rate, c=y, cmap='viridis')\n",
    "plt.xlabel('Transmission rate')\n",
    "plt.ylabel('Recovery rate')\n",
    "plt.colorbar(label=\"Peak infection rate\")\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Emulation\n",
    "A real world epidemic simulation will be computationally expensive and will take a long time to run. To evaluate the peak infection rate for thousands of different combinations of input parameters, it would be practical to to create a fast emulator model, which can then be used to predict the peak infection rate for new input parameters.\n",
    "\n",
    "The simplest way to test different emulators is to run `AutoEmulate` with default parameters, providing only inputs `X` and outputs `y`. What happens in the background is that the inputs will be **standardised** (`scale`=True), after which various models are fitted and evaluated using 5-fold **cross-validation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em = AutoEmulate()\n",
    "em.setup(X, y)\n",
    "best_model = em.compare()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Summarising cross-validation results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can summarise the cross-validation results to see that several models have a high $R^2$ and root mean squared error, suggesting a good fit. These metrics are the average metric on the test data across cv-folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em.summarise_cv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at each of the cv-folds for a specific model, like Gaussian Processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em.summarise_cv(model=\"GaussianProcess\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can plot the cv results using different plot styles. If the simulation has multiple inputs and outputs, the default is to plot the first input ('input_index=0') and the first output (`output_index=0`), but this can be changed. The default style is `Xy`, which plots predictions and data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em.plot_cv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To inspect specific models more closely, we can plot the predictions for each cv fold for a given model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em.plot_cv(model='GaussianProcess')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative plot is `actual_vs_predicted`, which plots the true values against the predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em.plot_cv(style=\"actual_vs_predicted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Always good to inspect the residuals too to spot any patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em.plot_cv(style=\"residual_vs_predicted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Evaluate the emulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking at the cv results, we can chose an emulator model and see how it performs on the test-set, which `AutoEmulate` automatically sets aside. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = em.get_model(\"GaussianProcess\")\n",
    "em.evaluate(gp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the test-set performance for chosen emulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em.plot_eval(gp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Refitting the model on the full dataset\n",
    "\n",
    "`AutoEmulate` splits the dataset into a training and holdout set. All cross-validation, parameter optimisation and model selection is done on the training set. After we selected a best emulator model, we can refit it on the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_final = em.refit(gp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now try to predict the peak infection rate for a new set of transmission and recovery rates. Because our emulator is much faster then the original simulation, we can now evaluate the peak infection rate for a much larger set of input parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "beta = (0.1, 0.5) # lower and upper bounds for the transmission rate\n",
    "gamma = (0.01, 0.2) # lower and upper bounds for the recovery rate\n",
    "lhd = LatinHypercube([beta, gamma])\n",
    "X_new = lhd.sample(1000)\n",
    "y_new = gp.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's do another plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transmission_rate = X_new[:, 0]\n",
    "recovery_rate = X_new[:, 1]\n",
    "\n",
    "plt.scatter(transmission_rate, recovery_rate, c=y_new, cmap='viridis')\n",
    "plt.xlabel('Transmission rate')\n",
    "plt.ylabel('Recovery rate')\n",
    "plt.colorbar(label=\"Peak infection rate\")\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Saving / Loading\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save and load the model using `em.save()` and `em.load()`. The model is saved using `joblib.dump`. Next to the model, there is also a `_meta.json` file which specifies the required dependencies and should be present when loading the model to check that the correct package versions are installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# em.save(gp_final, \"gp_final\")\n",
    "# gp_final_loaded = em.load(\"gp_final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we tried to chose default model parameters that work well in a wide range of scenarios, hyperparameter search will often find an emulator model with a better fit. Internally, `AutoEmulate` compares the performance of different models and hyperparameters using cross-validation on the training data, which can be computationally expensive and time-consuming for larger datasets. To speed it up, we can parallelise the process with `n_jobs`.\n",
    "\n",
    "For each model, we've pre-defined a search space for hyperparameters. When setting up `AutoEmulate` with `param_search=True`, we default to using random search with `param_search_iters = 20` iterations. This means that 20 hyperparameter combinations from the search space are sampled and evaluated. We plan to add other hyperparameter search methods in the future. \n",
    "\n",
    "Let's do a hyperparameter search for the Support Vector Machines and Random Forest models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em = AutoEmulate()\n",
    "em.setup(X, y, param_search=True, param_search_type=\"random\", param_search_iters=10, models=[\"SupportVectorMachines\", \"RandomForest\"], n_jobs=-2) # n_jobs=-2 uses all cores but one\n",
    "em.compare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em.summarise_cv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes**: \n",
    "* Some models, such as `GaussianProcess` can be slow when conducting hyperparameter search on larger datasets (say n > 1000). \n",
    "* Use the `models` argument to only run hyperparameter search on a subset of models to speed up the process.\n",
    "* When possible, use `n_jobs` to parallelise the hyperparameter search. With larger datasets, we recommend setting `param_search_iters` to a lower number, such as 5, to see how long it takes to run and then increase it if necessary.\n",
    "* all models can be specified with short names too, such as `rf` for `RandomForest`, `gp` for `GaussianProcess`, `svm` for `SupportVectorMachines`, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multioutput simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All models run with multi-output data as well. Some models naively support multiple outputs. For models that don't, `AutoEmulate` fits the model to each output separately under the hood. To see which models run separately for each output, we can check a model and see whether the pipeline includes a `MultiOutputRegressor` step. Note: all following metrics are averaged across outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoemulate.simulations.projectile import simulate_projectile_multioutput\n",
    "lhd = LatinHypercube([(-5., 1.), (0., 1000.)]) # (upper, lower) bounds for each parameter\n",
    "X = lhd.sample(100)\n",
    "y = np.array([simulate_projectile_multioutput(x) for x in X])\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em = AutoEmulate()\n",
    "em.setup(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we print a model, say SVM, we can see that it's wrapped in a `MultiOutputRegressor`, because it doesn't natively support multioutput data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(em.models[5]) # print the 5th model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom standardisation, cross-validation and dimension reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardisation\n",
    "`AutoEmulate` standardises inputs by default (`scale=True`) to have zero mean and unit variance. It uses `scaler=StandardScaler()` but [other normalisers](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing) can be used, or the inputs can be left unscaled (`scale=False`). In addition, some models, like Gaussian Processes also standardise outputs, which makes them work better. Checking the parameters of a model with `model.get_params()` will show whether the model standardises outputs.\n",
    "\n",
    "### Dimension reduction\n",
    "When there are lots of input variables, it can be useful to reduce the dimensionality. To do this, we can add a dimension reduction step to each model using `reduce_dim=True`. By default, this uses PCA from scikit-learn, but [other dimension reduction methods](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.decomposition) can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation\n",
    "\n",
    "The default cross-validation method is 5-fold cross-validation using `sklearn.model_selection.KFold`. The parameters can be changed or other cross-validation methods can be used, see [sklearn.model_selection splitter classes](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection) for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Let's say we wanted to change to a MinMaxScaler, to do PCA but retain components explaining more than 99%  of the variance and do KFold cross validation but with 3 splits and no shuffling. To do this, we can just import the respective classes from sklearn and pass them to `setup()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Example\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "mmscaler = MinMaxScaler()\n",
    "pca = PCA(0.99)\n",
    "kfold = KFold(n_splits=3, shuffle=False)\n",
    "\n",
    "em = AutoEmulate()\n",
    "em.setup(X, y, scale=True, scaler=mmscaler, cross_validator=kfold)\n",
    "best_model = em.compare()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Not all possible cross validators, scalers and decomposers have been tested and only a few make sense in the current version of `AutoEmulate`. If you encounter any issues, please open an [issue on GitHub](https://github.com/alan-turing-institute/autoemulate/issues/new/choose)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downstream tasks\n",
    "\n",
    "Once you have a trained emulator, it can be used for many downstream tasks like calibration or sensitivity analysis. `AutoEmulate` comes with inbuilt support for some of the most common of these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## History Matching\n",
    "\n",
    "In this section, we perform History Matching on the predictions from the best_emulator. This allows us to see which reagions of the parameter space are plausible. The Implausibility metric is calculated using the following relation for each set of parameter:\n",
    "\n",
    "$I_i(\\overline{x_0}) = \\frac{|z_i - \\mathbb{E}(f_i(\\overline{x_0}))|}{\\sqrt{\\text{Var}[z_i - \\mathbb{E}(f_i(\\overline{x_0}))]}}$\n",
    "\n",
    "Where if implosibility ($I_i$) exceeds a threshhold value, the points will be rulled out. \n",
    "The outcome of history matching are the NORY (Not Ruled Out Yet) and RO (Ruled Out) points.\n",
    "\n",
    "The NORY region can be used for resampling and re-training as a part of active-learning process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoemulate.history_matching import history_matching\n",
    "\n",
    "pred_mean, pred_std = best_model.predict(X, return_std=True)\n",
    "pred_var = np.square(pred_std)  # Convert std to variance\n",
    "predictions = (pred_mean, pred_var)  \n",
    "hist_match = history_matching(predictions=predictions, obs = [(1500, 53), (200, 80)], threshold=1.0 )\n",
    "\n",
    "print(f'The Not Rulled Out Points are {hist_match[\"NROY\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
