{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart\n",
    "\n",
    "`AutoEmulate`'s goal is to make it easy to create an emulator for your simulation.\n",
    "\n",
    "This tutorial's purpose is to walk you through the the basic functionality of the Python API using simple toy simulation as example.\n",
    "\n",
    "We'll demonstrate following steps:\n",
    "1. Getting input and output tensor data from our example simulation\n",
    "2. Creating, comparing and evaluating Emulators with `AutoEmulate`\n",
    "3. Using an `Emulator` model to predict outputs for new inputs\n",
    "4. Saving `Emulator` models (and associated metadata) to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports for the notebook\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy simulation\n",
    "\n",
    "Before we build an emulator with AutoEmulate, we need to get a set of input/output pairs from our simulation to use as training data.\n",
    "\n",
    "Below is a toy simulation for a projectile's motion with drag (see [here](https://mogp-emulator.readthedocs.io/en/latest/intro/tutorial.html) for details). The simulation includes:\n",
    "- Inputs: drag coefficient (log scale), velocity\n",
    "- Outputs: distance the projectile travelled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoemulate.simulations.projectile import Projectile\n",
    "\n",
    "projectile = Projectile(log_level=\"error\")\n",
    "n_samples = 500\n",
    "x = projectile.sample_inputs(n_samples).float()\n",
    "y, _ = projectile.forward_batch(x)\n",
    "y = y.float()\n",
    "\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "As you can see, our simulator inputs (`x`) and outputs (`y`) are PyTorch tensors.\n",
    "PyTorch tensors are a common data structure used in machine learning, and `AutoEmulate` is built to work with them.\n",
    "\n",
    "We can also visualize the simulation data before training emulators where the output of the simulator is depicted as the colour of each scatter point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(x[:, 0], x[:, 1], c=y[:, 0], cmap='viridis')\n",
    "plt.xlabel(projectile.param_names[0])\n",
    "plt.ylabel(projectile.param_names[1])\n",
    "plt.colorbar(label=projectile.output_names[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and compare Emulators\n",
    "\n",
    "With our simulator inputs and outputs, we can run a full machine learning pipeline, including data processing, model fitting, model selection and hyperparameter optimisation in just a few lines of code.\n",
    "\n",
    "First, let's import `AutoEmulate` and check the names of the available Emulator models. The columns indicate whether the emulator has a PyTorch backend, supports multioutput data and provides predictive uncertainty quantification. The list shows only the default set of emulators, but you can also see all available emulators by passing `default_only=False` to the function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoemulate import AutoEmulate\n",
    "\n",
    "AutoEmulate.list_emulators()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready run `AutoEmulate` to build and compare emulators.\n",
    "\n",
    "This will fit (including hyperparameter tuning) emulator models to the simulation input and output to the data, evaluating performance on witheld test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run AutoEmulate with default settings\n",
    "ae = AutoEmulate(x, y, log_level=\"error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information about the configuration options available, see the [AutoEmulate API docs](https://alan-turing-institute.github.io/autoemulate/reference/index.html).\n",
    "Here's a brief overview of some important options:\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>Model selection</summary>\n",
    "\n",
    "By default, `AutoEmulate` will use of all the listed emulator models, but you can also specify a subset to use if you already know which kinds of models are suitable for your data.\n",
    "\n",
    "Specify models used by AutoEmulate with the `models` argument, for example:\n",
    "```python\n",
    "models = [\"GaussianProcessExact\", \"RadialBasisFunctions\"]\n",
    "ae = AutoEmulate(x, y, models=models)\n",
    "```\n",
    "\n",
    "The user can also restrict the selection to just PyTorch models or probabilistis models by using the `only_pytorch` or `only_probabilistic` arguments, respectively. For example, to use only PyTorch models:\n",
    "\n",
    "```python\n",
    "ae = AutoEmulate(x, y, only_pytorch=True)\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>Logging</summary>\n",
    "\n",
    "When running `AutoEmulate`, you may also wish to enable logging to track the progress and performance of the emulator comparison. You can do this by setting the `log_level` argument when creating the `AutoEmulate` instance:\n",
    "```python\n",
    "ae = AutoEmulate(x, y, models=models, log_level=\"info\")\n",
    "```\n",
    "\n",
    "Try setting various log levels to see the difference. The options are \"progress_bar\", \"debug\", \"info\", \"warning\", \"error\", or \"critical\".\n",
    "\n",
    "</details>\n",
    "\n",
    "Now that we have run `AutoEmulate`, let's look at the summary for a comparison of emulator performance (r-squared and RMSE) on both the train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae.summarise()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing an Emulator\n",
    "\n",
    "From this list, we can choose an emulator based on the index from the summary dataframe, or quickly get the best performing one using the `best_result` function, which picks based on the `r2_test` metric by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = ae.best_result()\n",
    "print(\"Model with id: \", best.id, \" performed best: \", best.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best.model.untransformed_model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the configuration of the best model. These are the values of the model's hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can quickly visualise the performance of this Emulator with a plot of its predictions against the simulator outputs for the heldout test data. We also save the plot to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae.plot(best, fname=\"best_model_plot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also subset the data included in the plots by providing input and output ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae.plot(best, input_ranges={0: (0, 4), 1: (200, 500)}, output_ranges={0: (0, 10)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as plotting the data, we can directly plot the predicted mean and variance of the emulator for a pair of variables while holding the other variables constant at a given quantile. API to support plotting for a subset of the parameter and output range is also supported.\n",
    "\n",
    "The emulator predicted mean captures the simulated data plotted at the top of the tutorial well. The predicted variance is low where we have data, and increases away from the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae.plot_surface(best.model, projectile.parameters_range, quantile=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "\n",
    "We can use the emulator to make predictions using the `predict` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best.model.predict(x[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and loading emulators\n",
    "\n",
    "Emulators and their metadata (hyperparameter config and performance metrics) can be saved to disk and loaded again later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a directory to save Emulator models\n",
    "import os\n",
    "path = \"my_emulators\"\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the best result, the best performing emulator plus metadata, to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The use_timestamp paramater ensures a new result is saved each time the save method is called\n",
    "best_result_filepath = ae.save(best, path, use_timestamp=True)\n",
    "print(\"Model and metadata saved to: \", best_result_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now have a two files saved to disk, one with the emulator model and one with the metadata that has the same name and a `.csv` extension.\n",
    "\n",
    "You can later pass this filepath to the `load_model` method to use the model again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEmulate.load_model(best_result_filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(x[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autoemulate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
