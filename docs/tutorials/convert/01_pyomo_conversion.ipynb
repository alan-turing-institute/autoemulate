{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting Emulators to Pyomo Expressions\n",
    "\n",
    "This tutorial's purpose is to walk you through the process of converting your `AutoEmulate` models to Pyomo algebraic expressions for easy optimization workflows. Currently only `PolynomialRegression` and `MLP` are supported for this conversion.\n",
    "\n",
    "We'll demonstrate the following steps:\n",
    "1. Training a Neural Network (MLP) on a simple toy function using `AutoEmulate`\n",
    "2. Exporting the trained network into algebraic expressions compatible with Pyomo\n",
    "3. Validating the Pyomo expressions against PyTorch predictions\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "* `autoemulate`\n",
    "* `pyomo`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1612365eef0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# General imports for the notebook\n",
    "import warnings\n",
    "import numpy as np\n",
    "import torch\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy simulation\n",
    "\n",
    "Before we build an emulator with AutoEmulate, we need to get a set of input/output pairs from our simulation to use as training data.\n",
    "\n",
    "Below is a simple toy simulation that computes the product of two inputs: `y = x1 * x2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1000, 2]), torch.Size([1000]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate data: y = x1 * x2\n",
    "n_samples = 1000\n",
    "x1 = np.random.uniform(-100, 100, size=n_samples)\n",
    "x2 = np.random.uniform(-100, 100, size=n_samples)\n",
    "\n",
    "def F(x1, x2):\n",
    "    return x1 * x2\n",
    "\n",
    "x = np.column_stack((x1, x2))\n",
    "y = F(x1, x2)\n",
    "\n",
    "# Convert to tensors for AutoEmulate\n",
    "x = torch.tensor(x, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "As you can see, our simulator inputs (`x`) and outputs (`y`) are PyTorch tensors. PyTorch tensors are a common data structure used in machine learning, and `AutoEmulate` is built to work with them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Emulator\n",
    "\n",
    "For this tutorial, we'll focus on training an MLP (Multi-Layer Perceptron) neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO    2026-02-05 17:30:02,339 - autoemulate - Comparing ['MLP']\n",
      "INFO    2026-02-05 17:30:02,340 - autoemulate - Running Model: MLP: 1/1 (attempt 1/3)\n",
      "INFO    2026-02-05 17:36:00,161 - autoemulate - Finished running Model: MLP\n",
      "\n",
      "INFO    2026-02-05 17:36:00,163 - autoemulate - Using metric 'r2' to determine best result.\n"
     ]
    }
   ],
   "source": [
    "from autoemulate import AutoEmulate\n",
    "\n",
    "# Initialize AutoEmulate with MLP model only\n",
    "ae = AutoEmulate(x, y, log_level=\"info\", models=['MLP'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have run `AutoEmulate`, let's look at the summary for the emulator performance (r-squared and RMSE) on both the train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>x_transforms</th>\n",
       "      <th>y_transforms</th>\n",
       "      <th>params</th>\n",
       "      <th>r2_test</th>\n",
       "      <th>r2_test_std</th>\n",
       "      <th>rmse_test</th>\n",
       "      <th>rmse_test_std</th>\n",
       "      <th>r2_train</th>\n",
       "      <th>r2_train_std</th>\n",
       "      <th>rmse_train</th>\n",
       "      <th>rmse_train_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLP</td>\n",
       "      <td>[StandardizeTransform()]</td>\n",
       "      <td>[StandardizeTransform()]</td>\n",
       "      <td>{'epochs': 200, 'layer_dims': [32, 16], 'lr': ...</td>\n",
       "      <td>0.99976</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>54.460751</td>\n",
       "      <td>4.148852</td>\n",
       "      <td>0.99978</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>49.899025</td>\n",
       "      <td>1.874318</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model_name              x_transforms              y_transforms  \\\n",
       "0        MLP  [StandardizeTransform()]  [StandardizeTransform()]   \n",
       "\n",
       "                                              params  r2_test  r2_test_std  \\\n",
       "0  {'epochs': 200, 'layer_dims': [32, 16], 'lr': ...  0.99976     0.000034   \n",
       "\n",
       "   rmse_test  rmse_test_std  r2_train  r2_train_std  rmse_train  \\\n",
       "0  54.460751       4.148852   0.99978      0.000014   49.899025   \n",
       "\n",
       "   rmse_train_std  \n",
       "0        1.874318  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ae.summarise()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing an Emulator\n",
    "\n",
    "From this list, we can choose an emulator based on the index from the summary dataframe, or quickly get the best performing one using the `best_result` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO    2026-02-05 17:36:00,208 - autoemulate - Using metric 'r2' to determine best result.\n",
      "Model with id:  0  performed best:  MLP\n"
     ]
    }
   ],
   "source": [
    "best = ae.best_result()\n",
    "print(\"Model with id: \", best.id, \" performed best: \", best.model_name)\n",
    "\n",
    "# best = ae.get_models('MLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the configuration of the best model. These are the values of the model's hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epochs': 200, 'layer_dims': [32, 16], 'lr': 0.01, 'batch_size': 16, 'weight_init': 'default', 'scale': 0.1, 'bias_init': 'default', 'dropout_prob': None, 'scheduler_cls': None, 'scheduler_params': {}}\n"
     ]
    }
   ],
   "source": [
    "print(best.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting to Pyomo\n",
    "\n",
    "Now that we have a trained emulator, we can convert it to Pyomo algebraic expressions. This allows us to use the emulator in optimization problems with mathematical programming solvers.\n",
    "\n",
    "First, we need to set up a Pyomo model and define decision variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyomo.environ as pyo\n",
    "\n",
    "# Create a Pyomo concrete model\n",
    "pyo_model = pyo.ConcreteModel()\n",
    "\n",
    "# Pick a test point for initialization\n",
    "x_init = x[0]\n",
    "\n",
    "# Define decision variables\n",
    "# We use the real domain since our training data spans [-100, 100]\n",
    "pyo_model.x1 = pyo.Var(domain=pyo.Reals)\n",
    "pyo_model.x2 = pyo.Var(domain=pyo.Reals)\n",
    "\n",
    "# Initialize to a known point\n",
    "pyo_model.x1.set_value(x_init[0].item())\n",
    "pyo_model.x2.set_value(x_init[1].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Neural Network to Pyomo\n",
    "\n",
    "This is the core integration step. The `pyomofy` function converts the neural network's weights, biases, and activation functions into explicit algebraic expressions.\n",
    "\n",
    "It automatically handles:\n",
    "\n",
    "* **Input Standardization:** Scaling Pyomo variables to the statistical distribution the network expects\n",
    "* **Forward Pass:** Generating constraints for every layer\n",
    "* **Output Inverse Scaling:** Converting the network output back to real-world units\n",
    "* **ReLU Approximation:** If `nn.ReLU` was used during training, it's automatically approximated with Softplus (controlled by the `relu_beta` parameter). Increase `relu_beta` value if results mismatch significantly, or use a smooth activation function like `SiLU` during training:\n",
    "```\n",
    "from torch import nn\n",
    "\n",
    "# Train with smooth activation for better Pyomo conversion\n",
    "ae = AutoEmulate(\n",
    "    x, y, \n",
    "    models=['MLP'],\n",
    "    model_params={'activation_cls': nn.SiLU}  # Use SiLU instead of ReLU\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoemulate.convert.pyomo import pyomofy\n",
    "\n",
    "# Convert the best model to Pyomo expressions\n",
    "# Pass the Result object (or TransformedEmulator) and the list of Pyomo variables\n",
    "# Returns a list of expressions (one per output dimension)\n",
    "emulator_expressions = pyomofy(best, [pyo_model.x1, pyo_model.x2], relu_beta=100)\n",
    "\n",
    "# Since our output is 1D (the product), we take the first expression\n",
    "emulator_expr = emulator_expressions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Numerical Equivalence\n",
    "\n",
    "Before using the Pyomo expressions in optimization, we **must** verify that they yield the same values as the PyTorch model. This validation ensures the conversion was successful and the algebraic expressions accurately represent the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input point:        x1=-25.0920, x2=-62.9734\n",
      "PyTorch prediction: 1554.267089843750\n",
      "Pyomo prediction:   1554.234839389693\n",
      "\n",
      "Pyomo vs PyTorch:   0.032250454057\n"
     ]
    }
   ],
   "source": [
    "# 1. Evaluate Pyomo expression at the initialization point\n",
    "pyomo_val = pyo.value(emulator_expr)\n",
    "\n",
    "# 2. Evaluate PyTorch model at the same point\n",
    "torch_input = x[0].reshape(1, -1)\n",
    "torch_val = best.model.predict(torch_input).item()\n",
    "\n",
    "# Print comparison\n",
    "print(f\"Input point:        x1={x_init[0]:.4f}, x2={x_init[1]:.4f}\")\n",
    "print(f\"PyTorch prediction: {torch_val:.12f}\")\n",
    "print(f\"Pyomo prediction:   {pyomo_val:.12f}\")\n",
    "print(f\"\\nPyomo vs PyTorch:   {abs(pyomo_val - torch_val):.12f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Pyomo Expression\n",
    "\n",
    "Now that we've validated the conversion, the Pyomo expression can be used in optimization problems. Here's a simple example of how you might set up an objective function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Define an objective to maximize the emulator output\n",
    "pyo_model.obj = pyo.Objective(expr=emulator_expr, sense=pyo.maximize)\n",
    "\n",
    "# You could also use as constraints, for example:\n",
    "# pyo_model.constraint1 = pyo.Constraint(expr=emulator_expr >= 1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autoemulate (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
